{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khmar\\ana\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n",
      "You have TensorFlow version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# read in some helpful libraries\n",
    "import nltk                       # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd               # pandas dataframe\n",
    "import re                         # regular expression\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing        # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "from nltk.corpus import stopwords  \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "print('import done')\n",
    "\n",
    "# This code was tested with TensorFlow v1.4\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  I had ordered a data cable, got a very well fi...  NOTISSUE\n",
      "1                                   Love This Phone.  NOTISSUE\n",
      "2                I get a very well finished product.  NOTISSUE\n",
      "3                            I could not be happier.  NOTISSUE\n",
      "4  I was looking for this headset for a long time...  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = 'C:/Users/khmar/Desktop/ISSUE/dataset/CSV/data_ameliorate/data.csv'\n",
    "df = pd.read_csv(DATA_FILE,delimiter=';',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts= df['text']\n",
    "tags = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOTISSUE    2030\n",
       "ISSUE       2025\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that we have a balanced dataset\n",
    "# Note: data was randomly shuffled in our BigQuery query\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max = 1000\n",
    "# preprocess\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(tags)\n",
    "tok = Tokenizer(num_words=num_max)\n",
    "tok.fit_on_texts(texts)\n",
    "mat_texts = tok.texts_to_matrix(texts,mode='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tok save\n",
    "import pickle\n",
    "tok_file = 'CNN_token_normal_embeddings_without_text_preprocessing_DATA.sav'\n",
    "pickle.dump(tok, open(tok_file, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn preproces\n",
    "max_len = 1000\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 644,097\n",
      "Trainable params: 644,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "Train on 2838 samples, validate on 1217 samples\n",
      "Epoch 1/2\n",
      "2838/2838 [==============================] - 3s 1ms/step - loss: 0.6035 - acc: 0.6660 - binary_accuracy: 0.6660 - val_loss: 0.4214 - val_acc: 0.8168 - val_binary_accuracy: 0.8168\n",
      "Epoch 2/2\n",
      "2838/2838 [==============================] - 1s 517us/step - loss: 0.3568 - acc: 0.8411 - binary_accuracy: 0.8411 - val_loss: 0.3354 - val_acc: 0.8562 - val_binary_accuracy: 0.8562\n"
     ]
    }
   ],
   "source": [
    "# try a simple model first\n",
    "\n",
    "def get_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(num_max,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',metrics.binary_accuracy])\n",
    "    print('compile done')\n",
    "    return model\n",
    "\n",
    "def check_model(model,x,y):\n",
    "    model.fit(x,y,batch_size=32,epochs=2,verbose=1,validation_split=0.3)\n",
    "\n",
    "m = get_simple_model()\n",
    "check_model(m,mat_texts,tags)\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 0s 122us/step\n",
      "loss: 753.99%\n",
      "acc: 51.85%\n",
      "binary_accuracy: 51.85%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(\"CNN_model1_normal_embeddings_DATA_with_text_processing.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' bad service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' mediocre service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['great service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn preproces\n",
    "max_len = 1000\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 20)          20000     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 998, 64)           3904      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 40,801\n",
      "Trainable params: 40,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 2838 samples, validate on 1217 samples\n",
      "Epoch 1/2\n",
      "2838/2838 [==============================] - 13s 5ms/step - loss: 0.6846 - acc: 0.5613 - binary_accuracy: 0.5613 - val_loss: 0.6495 - val_acc: 0.7149 - val_binary_accuracy: 0.7149\n",
      "Epoch 2/2\n",
      "2838/2838 [==============================] - 9s 3ms/step - loss: 0.5467 - acc: 0.7498 - binary_accuracy: 0.7498 - val_loss: 0.4387 - val_acc: 0.7929 - val_binary_accuracy: 0.7929\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v1():   \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m1 = get_cnn_model_v1()\n",
    "check_model(m1,cnn_texts_mat,tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 1s 607us/step\n",
      "loss: 11.65%\n",
      "acc: 96.71%\n",
      "binary_accuracy: 96.71%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m1.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m1.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m1.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m1.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.save(\"CNN_model1_normal_embeddings_DATA_with_text_processing.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01772978]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' bad service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m1.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94345254]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['good service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m1.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28707814]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['help me please'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m1.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 50)          50000     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1000, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 998, 64)           9664      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 76,561\n",
      "Trainable params: 76,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2838 samples, validate on 1217 samples\n",
      "Epoch 1/2\n",
      "2838/2838 [==============================] - 17s 6ms/step - loss: 0.6865 - acc: 0.5553 - binary_accuracy: 0.5553 - val_loss: 0.6481 - val_acc: 0.7354 - val_binary_accuracy: 0.7354\n",
      "Epoch 2/2\n",
      "2838/2838 [==============================] - 15s 5ms/step - loss: 0.5101 - acc: 0.7791 - binary_accuracy: 0.7791 - val_loss: 0.3860 - val_acc: 0.8250 - val_binary_accuracy: 0.8250\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v2(): # added embed   \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        50, #!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m2 = get_cnn_model_v2()\n",
    "check_model(m2,cnn_texts_mat,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 1s 1ms/step\n",
      "loss: 5.74%\n",
      "acc: 98.44%\n",
      "binary_accuracy: 98.44%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m2.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m2.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m2.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m2.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.save(\"CNN_model2_normal_embeddings_DATA_with_text_processing.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26738784]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['help me'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m2.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03977815]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' bad service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m2.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1000, 20)          20000     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1000, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 998, 256)          15616     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 101,665\n",
      "Trainable params: 101,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2838 samples, validate on 1217 samples\n",
      "Epoch 1/2\n",
      "2838/2838 [==============================] - 32s 11ms/step - loss: 0.6872 - acc: 0.5233 - binary_accuracy: 0.5233 - val_loss: 0.6585 - val_acc: 0.6894 - val_binary_accuracy: 0.6894\n",
      "Epoch 2/2\n",
      "2838/2838 [==============================] - 30s 11ms/step - loss: 0.5158 - acc: 0.7685 - binary_accuracy: 0.7685 - val_loss: 0.3832 - val_acc: 0.8291 - val_binary_accuracy: 0.8291\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v3():    # added filter\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(256, #!!!!!!!!!!!!!!!!!!!\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m3 = get_cnn_model_v3()\n",
    "check_model(m3,cnn_texts_mat,tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 2s 2ms/step\n",
      "loss: 35.37%\n",
      "acc: 85.78%\n",
      "binary_accuracy: 85.78%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m3.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m3.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m3.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m3.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.save(\"CNN_model3_normal_embeddings_DATA_with_text_processing.sav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39144567]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['help me'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "m2.save('model2.h5')\n",
    "\n",
    "# Recreate the exact same model purely from the file\n",
    "#new_model = keras.models.load_model('path_to_my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
