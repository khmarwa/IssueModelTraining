{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khmar\\ana\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n",
      "You have TensorFlow version 1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# read in some helpful libraries\n",
    "import nltk                       # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd               # pandas dataframe\n",
    "import re                         # regular expression\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing        # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "from nltk.corpus import stopwords  \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "print('import done')\n",
    "\n",
    "# This code was tested with TensorFlow v1.4\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  I had ordered a data cable, got a very well fi...  NOTISSUE\n",
      "1                                   Love This Phone.  NOTISSUE\n",
      "2                I get a very well finished product.  NOTISSUE\n",
      "3                            I could not be happier.  NOTISSUE\n",
      "4  I was looking for this headset for a long time...  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = 'C:/Users/khmar/Desktop/ISSUE/dataset/CSV/data_ameliorate/data.csv'\n",
    "df = pd.read_csv(DATA_FILE,delimiter=';',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS ={\n",
    " 'a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'across',\n",
    " 'after',\n",
    " 'afterwards',\n",
    " 'again',\n",
    " 'against',\n",
    " 'all',\n",
    " 'almost',\n",
    " 'alone',\n",
    " 'along',\n",
    " 'already',\n",
    " 'also',\n",
    " 'although',\n",
    " 'always',\n",
    " 'am',\n",
    " 'among',\n",
    " 'amongst',\n",
    " 'amount',\n",
    " 'an',\n",
    " 'and',\n",
    " 'another',\n",
    " 'any',\n",
    " 'anyhow',\n",
    " 'anyone',\n",
    " 'anything',\n",
    " 'anyway',\n",
    " 'anywhere',\n",
    " 'are',\n",
    " 'around',\n",
    " 'as',\n",
    " 'at',\n",
    " 'back',\n",
    " 'be',\n",
    " 'became',\n",
    " 'because',\n",
    " 'become',\n",
    " 'becomes',\n",
    " 'becoming',\n",
    " 'been',\n",
    " 'before',\n",
    " 'beforehand',\n",
    " 'behind',\n",
    " 'being',\n",
    " 'below',\n",
    " 'beside',\n",
    " 'besides',\n",
    " 'between',\n",
    " 'beyond',\n",
    " 'both',\n",
    " 'bottom',\n",
    " 'but',\n",
    " 'by',\n",
    " 'ca',\n",
    " 'call',\n",
    " 'can',\n",
    " 'cannot',\n",
    " 'could',\n",
    " 'did',\n",
    " 'do',\n",
    " 'does',\n",
    " 'doing',\n",
    " 'done',\n",
    " 'down',\n",
    " 'due',\n",
    " 'during',\n",
    " 'each',\n",
    " 'eight',\n",
    " 'either',\n",
    " 'eleven',\n",
    " 'else',\n",
    " 'elsewhere',\n",
    " 'empty',\n",
    " 'enough',\n",
    " 'even',\n",
    " 'ever',\n",
    " 'every',\n",
    " 'everyone',\n",
    " 'everything',\n",
    " 'everywhere',\n",
    " 'except',\n",
    " 'few',\n",
    " 'fifteen',\n",
    " 'fifty',\n",
    " 'first',\n",
    " 'five',\n",
    " 'for',\n",
    " 'former',\n",
    " 'formerly',\n",
    " 'forty',\n",
    " 'four',\n",
    " 'from',\n",
    " 'front',\n",
    " 'full',\n",
    " 'further',\n",
    " 'get',\n",
    " 'give',\n",
    " 'go',\n",
    " 'had',\n",
    " 'has',\n",
    " 'have',\n",
    " 'he',\n",
    " 'hence',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hereafter',\n",
    " 'hereby',\n",
    " 'herein',\n",
    " 'hereupon',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'however',\n",
    " 'hundred',\n",
    " 'i',\n",
    " 'if',\n",
    " 'in',\n",
    " 'indeed',\n",
    " 'into',\n",
    " 'is',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'just',\n",
    " 'keep',\n",
    " 'last',\n",
    " 'latter',\n",
    " 'latterly',\n",
    " 'least',\n",
    " 'less',\n",
    " 'made',\n",
    " 'make',\n",
    " 'many',\n",
    " 'may',\n",
    " 'me',\n",
    " 'meanwhile',\n",
    " 'might',\n",
    " 'mine',\n",
    " 'more',\n",
    " 'moreover',\n",
    " 'most',\n",
    " 'mostly',\n",
    " 'move',\n",
    " 'much',\n",
    " 'must',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'name',\n",
    " 'namely',\n",
    " 'neither',\n",
    " 'never',\n",
    " 'nevertheless',\n",
    " 'next',\n",
    " 'nine',\n",
    " 'nobody',\n",
    " 'none',\n",
    " 'noone',\n",
    " 'nor',\n",
    " 'nothing',\n",
    " 'now',\n",
    " 'nowhere',\n",
    " 'of',\n",
    " 'off',\n",
    " 'often',\n",
    " 'on',\n",
    " 'once',\n",
    " 'one',\n",
    " 'only',\n",
    " 'onto',\n",
    " 'or',\n",
    " 'other',\n",
    " 'others',\n",
    " 'otherwise',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 'part',\n",
    " 'per',\n",
    " 'perhaps',\n",
    " 'please',\n",
    " 'put',\n",
    " 'quite',\n",
    " 'rather',\n",
    " 're',\n",
    " 'really',\n",
    " 'regarding',\n",
    " 'same',\n",
    " 'say',\n",
    " 'see',\n",
    " 'seem',\n",
    " 'seemed',\n",
    " 'seeming',\n",
    " 'seems',\n",
    " 'serious',\n",
    " 'several',\n",
    " 'she',\n",
    " 'should',\n",
    " 'show',\n",
    " 'side',\n",
    " 'since',\n",
    " 'six',\n",
    " 'sixty',\n",
    " 'so',\n",
    " 'some',\n",
    " 'somehow',\n",
    " 'someone',\n",
    " 'something',\n",
    " 'sometime',\n",
    " 'sometimes',\n",
    " 'somewhere',\n",
    " 'still',\n",
    " 'such',\n",
    " 'take',\n",
    " 'ten',\n",
    " 'than',\n",
    " 'that',\n",
    " 'the',\n",
    " 'their',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'thence',\n",
    " 'there',\n",
    " 'thereafter',\n",
    " 'thereby',\n",
    " 'therefore',\n",
    " 'therein',\n",
    " 'thereupon',\n",
    " 'these',\n",
    " 'they',\n",
    " 'third',\n",
    " 'this',\n",
    " 'those',\n",
    " 'though',\n",
    " 'three',\n",
    " 'through',\n",
    " 'throughout',\n",
    " 'thru',\n",
    " 'thus',\n",
    " 'to',\n",
    " 'together',\n",
    " 'too',\n",
    " 'top',\n",
    " 'toward',\n",
    " 'towards',\n",
    " 'twelve',\n",
    " 'twenty',\n",
    " 'two',\n",
    " 'under',\n",
    " 'unless',\n",
    " 'until',\n",
    " 'up',\n",
    " 'upon',\n",
    " 'us',\n",
    " 'used',\n",
    " 'using',\n",
    " 'various',\n",
    " 'very',\n",
    " 'via',\n",
    " 'was',\n",
    " 'we',\n",
    " 'well',\n",
    " 'were',\n",
    " 'what',\n",
    " 'whatever',\n",
    " 'when',\n",
    " 'whence',\n",
    " 'whenever',\n",
    " 'where',\n",
    " 'whereafter',\n",
    " 'whereas',\n",
    " 'whereby',\n",
    " 'wherein',\n",
    " 'whereupon',\n",
    " 'wherever',\n",
    " 'whether',\n",
    " 'which',\n",
    " 'while',\n",
    " 'whither',\n",
    " 'who',\n",
    " 'whoever',\n",
    " 'whole',\n",
    " 'whom',\n",
    " 'whose',\n",
    " 'why',\n",
    " 'will',\n",
    " 'with',\n",
    " 'within',\n",
    " 'without',\n",
    " 'would',\n",
    " 'yet',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khmar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def transformText(text):\n",
    "#stops = set(stopwords.words(\"english\"))\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Removing all the stopwords\n",
    "    text = [word for word in text.split() if word not in STOP_WORDS]\n",
    "\n",
    "    ##Fixing Word Lengthening\n",
    "    #pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    #pattern.sub(r\"\\1\\1\", text)\n",
    "    #print(text)\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(text)\n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "    # Strip all the numerics\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    ##Lemmatisation\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    \n",
    "    # Stemming\n",
    "    text=gensim.parsing.preprocessing.stem_text(text)\n",
    "    #Spellchecker \n",
    "    #correcteur\n",
    "    # find those words that may be misspelled\n",
    "    spell = SpellChecker()\n",
    "    misspelled = text.split()\n",
    "    for i in range(len(misspelled)):\n",
    "    # Get the one `most likely` answer\n",
    "        word = spell.correction(misspelled[i])\n",
    "        misspelled[i]=word\n",
    "        text = \" \".join(misspelled)\n",
    "\n",
    "    # Reduce words to their root form\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text.split()]\n",
    "        \n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    #text=word_tokenize(text)\n",
    "    for word in text:\n",
    "        word=lemmatizer.lemmatize(word,pos='v')\n",
    "        word=lemmatizer.lemmatize(word,pos='n')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(transformText)\n",
    "texts= df['text']\n",
    "tags = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order data cabl got well finish work product \n",
      "\n",
      "love phone \n",
      "\n",
      "get well finish product\n"
     ]
    }
   ],
   "source": [
    "## Print a couple of rows after the preprocessing of the data is done\n",
    "\n",
    "print (df['text'][0] , '\\n')\n",
    "print (df['text'][1] , '\\n')\n",
    "print (df['text'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOTISSUE    2030\n",
       "ISSUE       2025\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that we have a balanced dataset\n",
    "# Note: data was randomly shuffled in our BigQuery query\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max = 1000\n",
    "# preprocess\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(tags)\n",
    "tok = Tokenizer(num_words=num_max)\n",
    "tok.fit_on_texts(texts)\n",
    "mat_texts = tok.texts_to_matrix(texts,mode='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn preproces\n",
    "max_len = 1000\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 644,097\n",
      "Trainable params: 644,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "Train on 3244 samples, validate on 811 samples\n",
      "Epoch 1/10\n",
      "3244/3244 [==============================] - 3s 881us/step - loss: 0.5824 - acc: 0.6890 - binary_accuracy: 0.6890 - val_loss: 0.3373 - val_acc: 0.8668 - val_binary_accuracy: 0.8668\n",
      "Epoch 2/10\n",
      "3244/3244 [==============================] - 2s 463us/step - loss: 0.3435 - acc: 0.8499 - binary_accuracy: 0.8499 - val_loss: 0.2310 - val_acc: 0.9285 - val_binary_accuracy: 0.9285\n",
      "Epoch 3/10\n",
      "3244/3244 [==============================] - 1s 447us/step - loss: 0.2138 - acc: 0.9174 - binary_accuracy: 0.9174 - val_loss: 0.2008 - val_acc: 0.9457 - val_binary_accuracy: 0.9457\n",
      "Epoch 4/10\n",
      "3244/3244 [==============================] - 1s 447us/step - loss: 0.1360 - acc: 0.9488 - binary_accuracy: 0.9488 - val_loss: 0.1864 - val_acc: 0.9457 - val_binary_accuracy: 0.9457\n",
      "Epoch 5/10\n",
      "3244/3244 [==============================] - 1s 447us/step - loss: 0.0943 - acc: 0.9636 - binary_accuracy: 0.9636 - val_loss: 0.1878 - val_acc: 0.9531 - val_binary_accuracy: 0.9531\n",
      "Epoch 6/10\n",
      "3244/3244 [==============================] - 2s 485us/step - loss: 0.0662 - acc: 0.9760 - binary_accuracy: 0.9760 - val_loss: 0.2051 - val_acc: 0.9457 - val_binary_accuracy: 0.9457\n",
      "Epoch 7/10\n",
      "3244/3244 [==============================] - 2s 511us/step - loss: 0.0551 - acc: 0.9790 - binary_accuracy: 0.9790 - val_loss: 0.2264 - val_acc: 0.9519 - val_binary_accuracy: 0.9519\n",
      "Epoch 8/10\n",
      "3244/3244 [==============================] - 2s 502us/step - loss: 0.0515 - acc: 0.9787 - binary_accuracy: 0.9787 - val_loss: 0.2249 - val_acc: 0.9494 - val_binary_accuracy: 0.9494\n",
      "Epoch 9/10\n",
      "3244/3244 [==============================] - 2s 495us/step - loss: 0.0425 - acc: 0.9812 - binary_accuracy: 0.9812 - val_loss: 0.2481 - val_acc: 0.9494 - val_binary_accuracy: 0.9494\n",
      "Epoch 10/10\n",
      "3244/3244 [==============================] - 1s 453us/step - loss: 0.0448 - acc: 0.9797 - binary_accuracy: 0.9797 - val_loss: 0.2647 - val_acc: 0.9457 - val_binary_accuracy: 0.9457\n"
     ]
    }
   ],
   "source": [
    "# try a simple model first\n",
    "\n",
    "def get_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(num_max,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',metrics.binary_accuracy])\n",
    "    print('compile done')\n",
    "    return model\n",
    "\n",
    "def check_model(model,x,y):\n",
    "    model.fit(x,y,batch_size=32,epochs=10,verbose=1,validation_split=0.3)\n",
    "\n",
    "m = get_simple_model()\n",
    "check_model(m,mat_texts,tags)\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 1s 1ms/step\n",
      "loss: 7.82%\n",
      "acc: 97.62%\n",
      "binary_accuracy: 97.62%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m2.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0066704]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' bad service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45153654]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' mediocre service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99777055]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['great service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn preproces\n",
    "max_len = 1000\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 20)          20000     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 998, 64)           3904      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 40,801\n",
      "Trainable params: 40,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 3244 samples, validate on 811 samples\n",
      "Epoch 1/10\n",
      "3244/3244 [==============================] - 12s 4ms/step - loss: 0.6838 - acc: 0.5555 - binary_accuracy: 0.5555 - val_loss: 0.6132 - val_acc: 0.7719 - val_binary_accuracy: 0.7719\n",
      "Epoch 2/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.4725 - acc: 0.7802 - binary_accuracy: 0.7802 - val_loss: 0.3398 - val_acc: 0.8656 - val_binary_accuracy: 0.8656\n",
      "Epoch 3/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.3575 - acc: 0.8474 - binary_accuracy: 0.8474 - val_loss: 0.2933 - val_acc: 0.8878 - val_binary_accuracy: 0.8878 -\n",
      "Epoch 4/10\n",
      "3244/3244 [==============================] - 9s 3ms/step - loss: 0.3048 - acc: 0.8711 - binary_accuracy: 0.8711 - val_loss: 0.2738 - val_acc: 0.9149 - val_binary_accuracy: 0.9149\n",
      "Epoch 5/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.2691 - acc: 0.8859 - binary_accuracy: 0.8859 - val_loss: 0.2585 - val_acc: 0.9112 - val_binary_accuracy: 0.9112\n",
      "Epoch 6/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.2358 - acc: 0.9054 - binary_accuracy: 0.9054 - val_loss: 0.2460 - val_acc: 0.9199 - val_binary_accuracy: 0.9199\n",
      "Epoch 7/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.2103 - acc: 0.9115 - binary_accuracy: 0.9115 - val_loss: 0.2342 - val_acc: 0.9273 - val_binary_accuracy: 0.9273\n",
      "Epoch 8/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.1828 - acc: 0.9245 - binary_accuracy: 0.9245 - val_loss: 0.2313 - val_acc: 0.9285 - val_binary_accuracy: 0.9285\n",
      "Epoch 9/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.1635 - acc: 0.9362 - binary_accuracy: 0.9362 - val_loss: 0.2378 - val_acc: 0.9297 - val_binary_accuracy: 0.9297cc: 0.9375 - binary_accuracy:\n",
      "Epoch 10/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.1485 - acc: 0.9414 - binary_accuracy: 0.9414 - val_loss: 0.2424 - val_acc: 0.9383 - val_binary_accuracy: 0.9383\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v1():   \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m1 = get_cnn_model_v1()\n",
    "check_model(m1,cnn_texts_mat,tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 1s 1ms/step\n",
      "loss: 8.84%\n",
      "acc: 96.80%\n",
      "binary_accuracy: 96.80%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m2.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m1.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m1.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m1.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0066704]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' bad service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.83600277]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['good service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7769191]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['help me please'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 50)          50000     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1000, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 998, 64)           9664      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 76,561\n",
      "Trainable params: 76,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3244 samples, validate on 811 samples\n",
      "Epoch 1/10\n",
      "3244/3244 [==============================] - 18s 5ms/step - loss: 0.6824 - acc: 0.5672 - binary_accuracy: 0.5672 - val_loss: 0.5863 - val_acc: 0.8150 - val_binary_accuracy: 0.8150\n",
      "Epoch 2/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.4627 - acc: 0.7901 - binary_accuracy: 0.7901 - val_loss: 0.3221 - val_acc: 0.8755 - val_binary_accuracy: 0.8755\n",
      "Epoch 3/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.3281 - acc: 0.8573 - binary_accuracy: 0.8573 - val_loss: 0.2670 - val_acc: 0.8964 - val_binary_accuracy: 0.8964\n",
      "Epoch 4/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.2665 - acc: 0.8872 - binary_accuracy: 0.8872 - val_loss: 0.2391 - val_acc: 0.9137 - val_binary_accuracy: 0.9137\n",
      "Epoch 5/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.2100 - acc: 0.9171 - binary_accuracy: 0.9171 - val_loss: 0.2271 - val_acc: 0.9137 - val_binary_accuracy: 0.9137\n",
      "Epoch 6/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.1628 - acc: 0.9353 - binary_accuracy: 0.9353 - val_loss: 0.2272 - val_acc: 0.9322 - val_binary_accuracy: 0.9322\n",
      "Epoch 7/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.1305 - acc: 0.9448 - binary_accuracy: 0.9448 - val_loss: 0.2300 - val_acc: 0.9322 - val_binary_accuracy: 0.9322\n",
      "Epoch 8/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.1109 - acc: 0.9519 - binary_accuracy: 0.9519 - val_loss: 0.2393 - val_acc: 0.9383 - val_binary_accuracy: 0.9383\n",
      "Epoch 9/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.0885 - acc: 0.9612 - binary_accuracy: 0.9612 - val_loss: 0.2511 - val_acc: 0.9396 - val_binary_accuracy: 0.9396\n",
      "Epoch 10/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.0831 - acc: 0.9664 - binary_accuracy: 0.9664 - val_loss: 0.2557 - val_acc: 0.9420 - val_binary_accuracy: 0.9420\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v2(): # added embed   \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        50, #!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m2 = get_cnn_model_v2()\n",
    "check_model(m2,cnn_texts_mat,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 1s 1ms/step\n",
      "loss: 10.43%\n",
      "acc: 97.12%\n",
      "binary_accuracy: 97.12%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m2.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m2.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m2.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m2.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.22539295]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['help me'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m2.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0066704]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' bad service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1000, 20)          20000     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 1000, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 998, 256)          15616     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 101,665\n",
      "Trainable params: 101,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3244 samples, validate on 811 samples\n",
      "Epoch 1/10\n",
      "3244/3244 [==============================] - 34s 10ms/step - loss: 0.6712 - acc: 0.5820 - binary_accuracy: 0.5820 - val_loss: 0.4994 - val_acc: 0.7768 - val_binary_accuracy: 0.7768\n",
      "Epoch 2/10\n",
      "3244/3244 [==============================] - 32s 10ms/step - loss: 0.4662 - acc: 0.7879 - binary_accuracy: 0.7879 - val_loss: 0.3414 - val_acc: 0.8656 - val_binary_accuracy: 0.8656\n",
      "Epoch 3/10\n",
      "3244/3244 [==============================] - 35s 11ms/step - loss: 0.3564 - acc: 0.8520 - binary_accuracy: 0.8520 - val_loss: 0.2947 - val_acc: 0.8989 - val_binary_accuracy: 0.8989\n",
      "Epoch 4/10\n",
      "3244/3244 [==============================] - 35s 11ms/step - loss: 0.2962 - acc: 0.8730 - binary_accuracy: 0.8730 - val_loss: 0.2590 - val_acc: 0.9112 - val_binary_accuracy: 0.9112\n",
      "Epoch 5/10\n",
      "3244/3244 [==============================] - 31s 10ms/step - loss: 0.2678 - acc: 0.8930 - binary_accuracy: 0.8930 - val_loss: 0.2435 - val_acc: 0.9149 - val_binary_accuracy: 0.9149\n",
      "Epoch 6/10\n",
      "3244/3244 [==============================] - 32s 10ms/step - loss: 0.2383 - acc: 0.9057 - binary_accuracy: 0.9057 - val_loss: 0.2292 - val_acc: 0.9285 - val_binary_accuracy: 0.9285\n",
      "Epoch 7/10\n",
      "3244/3244 [==============================] - 32s 10ms/step - loss: 0.2131 - acc: 0.9168 - binary_accuracy: 0.9168 - val_loss: 0.2325 - val_acc: 0.9309 - val_binary_accuracy: 0.9309\n",
      "Epoch 8/10\n",
      "3244/3244 [==============================] - 31s 9ms/step - loss: 0.1804 - acc: 0.9279 - binary_accuracy: 0.9279 - val_loss: 0.2262 - val_acc: 0.9346 - val_binary_accuracy: 0.9346\n",
      "Epoch 9/10\n",
      "3244/3244 [==============================] - 32s 10ms/step - loss: 0.1653 - acc: 0.9328 - binary_accuracy: 0.9328 - val_loss: 0.2103 - val_acc: 0.9383 - val_binary_accuracy: 0.9383\n",
      "Epoch 10/10\n",
      "3244/3244 [==============================] - 32s 10ms/step - loss: 0.1432 - acc: 0.9445 - binary_accuracy: 0.9445 - val_loss: 0.2209 - val_acc: 0.9445 - val_binary_accuracy: 0.9445\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v3():    # added filter\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(256, #!!!!!!!!!!!!!!!!!!!\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m3 = get_cnn_model_v3()\n",
    "check_model(m3,cnn_texts_mat,tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 2s 1ms/step\n",
      "loss: 7.25%\n",
      "acc: 97.37%\n",
      "binary_accuracy: 97.37%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m2.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m3.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m3.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m3.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7769191]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['help me'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
