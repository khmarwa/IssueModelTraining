{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khmar\\ana\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n",
      "You have TensorFlow version 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# read in some helpful libraries\n",
    "import nltk                       # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd               # pandas dataframe\n",
    "import re                         # regular expression\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing        # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import numpy as np\n",
    "import itertools\n",
    "import os\n",
    "from nltk.corpus import stopwords  \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "from keras import metrics\n",
    "print('import done')\n",
    "\n",
    "# This code was tested with TensorFlow v1.4\n",
    "print(\"You have TensorFlow version\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  I had ordered a data cable, got a very well fi...  NOTISSUE\n",
      "1                                   Love This Phone.  NOTISSUE\n",
      "2                I get a very well finished product.  NOTISSUE\n",
      "3                            I could not be happier.  NOTISSUE\n",
      "4  I was looking for this headset for a long time...  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = 'C:/Users/khmar/Desktop/ISSUE/dataset/CSV/data_ameliorate/data.csv'\n",
    "df = pd.read_csv(DATA_FILE,delimiter=';',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts= df['text']\n",
    "tags = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOTISSUE    2030\n",
       "ISSUE       2025\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm that we have a balanced dataset\n",
    "# Note: data was randomly shuffled in our BigQuery query\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_max = 1000\n",
    "# preprocess\n",
    "le = LabelEncoder()\n",
    "tags = le.fit_transform(tags)\n",
    "tok = Tokenizer(num_words=num_max)\n",
    "tok.fit_on_texts(texts)\n",
    "mat_texts = tok.texts_to_matrix(texts,mode='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn preproces\n",
    "max_len = 1000\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 644,097\n",
      "Trainable params: 644,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "compile done\n",
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3244 samples, validate on 811 samples\n",
      "Epoch 1/10\n",
      "3244/3244 [==============================] - 4s 1ms/step - loss: 0.5806 - acc: 0.7053 - binary_accuracy: 0.7053 - val_loss: 0.3243 - val_acc: 0.8841 - val_binary_accuracy: 0.8841\n",
      "Epoch 2/10\n",
      "3244/3244 [==============================] - 2s 531us/step - loss: 0.3328 - acc: 0.8616 - binary_accuracy: 0.8616 - val_loss: 0.2233 - val_acc: 0.9236 - val_binary_accuracy: 0.9236\n",
      "Epoch 3/10\n",
      "3244/3244 [==============================] - 2s 521us/step - loss: 0.1885 - acc: 0.9239 - binary_accuracy: 0.9239 - val_loss: 0.1557 - val_acc: 0.9433 - val_binary_accuracy: 0.9433\n",
      "Epoch 4/10\n",
      "3244/3244 [==============================] - 2s 662us/step - loss: 0.1032 - acc: 0.9652 - binary_accuracy: 0.9652 - val_loss: 0.1499 - val_acc: 0.9556 - val_binary_accuracy: 0.9556\n",
      "Epoch 5/10\n",
      "3244/3244 [==============================] - 2s 533us/step - loss: 0.0542 - acc: 0.9821 - binary_accuracy: 0.9821 - val_loss: 0.1452 - val_acc: 0.9531 - val_binary_accuracy: 0.9531cc: 0.9826 - binary_accuracy: 0.98\n",
      "Epoch 6/10\n",
      "3244/3244 [==============================] - 2s 521us/step - loss: 0.0342 - acc: 0.9877 - binary_accuracy: 0.9877 - val_loss: 0.1533 - val_acc: 0.9568 - val_binary_accuracy: 0.9568\n",
      "Epoch 7/10\n",
      "3244/3244 [==============================] - 2s 533us/step - loss: 0.0300 - acc: 0.9908 - binary_accuracy: 0.9908 - val_loss: 0.1611 - val_acc: 0.9618 - val_binary_accuracy: 0.9618\n",
      "Epoch 8/10\n",
      "3244/3244 [==============================] - 2s 525us/step - loss: 0.0205 - acc: 0.9917 - binary_accuracy: 0.9917 - val_loss: 0.1481 - val_acc: 0.9630 - val_binary_accuracy: 0.9630\n",
      "Epoch 9/10\n",
      "3244/3244 [==============================] - 2s 592us/step - loss: 0.0186 - acc: 0.9923 - binary_accuracy: 0.9923 - val_loss: 0.1696 - val_acc: 0.9581 - val_binary_accuracy: 0.9581\n",
      "Epoch 10/10\n",
      "3244/3244 [==============================] - 2s 614us/step - loss: 0.0152 - acc: 0.9941 - binary_accuracy: 0.9941 - val_loss: 0.1690 - val_acc: 0.9605 - val_binary_accuracy: 0.9605loss: 0.0110 - acc: 0.9946 - b\n"
     ]
    }
   ],
   "source": [
    "# try a simple model first\n",
    "\n",
    "def get_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(num_max,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc',metrics.binary_accuracy])\n",
    "    print('compile done')\n",
    "    return model\n",
    "\n",
    "def check_model(model,x,y):\n",
    "    model.fit(x,y,batch_size=32,epochs=10,verbose=1,validation_split=0.3)\n",
    "\n",
    "m = get_simple_model()\n",
    "check_model(m,mat_texts,tags)\n",
    "\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 0s 38us/step\n",
      "loss: 774.85%\n",
      "acc: 51.52%\n",
      "binary_accuracy: 51.52%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' bad service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' mediocre service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['great service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cnn preproces\n",
    "max_len = 1000\n",
    "cnn_texts_seq = tok.texts_to_sequences(texts)\n",
    "cnn_texts_mat = sequence.pad_sequences(cnn_texts_seq,maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1000, 20)          20000     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 998, 64)           3904      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 40,801\n",
      "Trainable params: 40,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 3244 samples, validate on 811 samples\n",
      "Epoch 1/10\n",
      "3244/3244 [==============================] - 13s 4ms/step - loss: 0.6887 - acc: 0.5351 - binary_accuracy: 0.5351 - val_loss: 0.6719 - val_acc: 0.5536 - val_binary_accuracy: 0.5536\n",
      "Epoch 2/10\n",
      "3244/3244 [==============================] - 11s 3ms/step - loss: 0.5227 - acc: 0.7518 - binary_accuracy: 0.7518 - val_loss: 0.3265 - val_acc: 0.8631 - val_binary_accuracy: 0.8631\n",
      "Epoch 3/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.3644 - acc: 0.8465 - binary_accuracy: 0.8465 - val_loss: 0.2571 - val_acc: 0.9001 - val_binary_accuracy: 0.9001\n",
      "Epoch 4/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.2925 - acc: 0.8807 - binary_accuracy: 0.8807 - val_loss: 0.2330 - val_acc: 0.9199 - val_binary_accuracy: 0.9199\n",
      "Epoch 5/10\n",
      "3244/3244 [==============================] - 11s 3ms/step - loss: 0.2585 - acc: 0.9038 - binary_accuracy: 0.9038 - val_loss: 0.2145 - val_acc: 0.9248 - val_binary_accuracy: 0.9248\n",
      "Epoch 6/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.2331 - acc: 0.9094 - binary_accuracy: 0.9094 - val_loss: 0.2155 - val_acc: 0.9236 - val_binary_accuracy: 0.9236\n",
      "Epoch 7/10\n",
      "3244/3244 [==============================] - 11s 3ms/step - loss: 0.2094 - acc: 0.9211 - binary_accuracy: 0.9211 - val_loss: 0.1930 - val_acc: 0.9445 - val_binary_accuracy: 0.9445\n",
      "Epoch 8/10\n",
      "3244/3244 [==============================] - 11s 3ms/step - loss: 0.1878 - acc: 0.9303 - binary_accuracy: 0.9303 - val_loss: 0.1947 - val_acc: 0.9396 - val_binary_accuracy: 0.9396\n",
      "Epoch 9/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.1706 - acc: 0.9331 - binary_accuracy: 0.9331 - val_loss: 0.2022 - val_acc: 0.9322 - val_binary_accuracy: 0.9322\n",
      "Epoch 10/10\n",
      "3244/3244 [==============================] - 10s 3ms/step - loss: 0.1507 - acc: 0.9457 - binary_accuracy: 0.9457 - val_loss: 0.1934 - val_acc: 0.9470 - val_binary_accuracy: 0.9470\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v1():   \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m1 = get_cnn_model_v1()\n",
    "check_model(m1,cnn_texts_mat,tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 1s 607us/step\n",
      "loss: 11.65%\n",
      "acc: 96.71%\n",
      "binary_accuracy: 96.71%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m1.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m1.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m1.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m1.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0066211]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' bad service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m1.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97473925]]\n",
      "NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['good service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m1.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23116301]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['help me please'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m1.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 50)          50000     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1000, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 998, 64)           9664      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 76,561\n",
      "Trainable params: 76,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3244 samples, validate on 811 samples\n",
      "Epoch 1/10\n",
      "3244/3244 [==============================] - 18s 5ms/step - loss: 0.6827 - acc: 0.5499 - binary_accuracy: 0.5499 - val_loss: 0.6257 - val_acc: 0.7781 - val_binary_accuracy: 0.7781\n",
      "Epoch 2/10\n",
      "3244/3244 [==============================] - 17s 5ms/step - loss: 0.4969 - acc: 0.7722 - binary_accuracy: 0.7722 - val_loss: 0.3190 - val_acc: 0.8545 - val_binary_accuracy: 0.8545\n",
      "Epoch 3/10\n",
      "3244/3244 [==============================] - 17s 5ms/step - loss: 0.3358 - acc: 0.8564 - binary_accuracy: 0.8564 - val_loss: 0.2567 - val_acc: 0.9014 - val_binary_accuracy: 0.9014\n",
      "Epoch 4/10\n",
      "3244/3244 [==============================] - 17s 5ms/step - loss: 0.2739 - acc: 0.8909 - binary_accuracy: 0.8909 - val_loss: 0.2156 - val_acc: 0.9285 - val_binary_accuracy: 0.9285\n",
      "Epoch 5/10\n",
      "3244/3244 [==============================] - 17s 5ms/step - loss: 0.2183 - acc: 0.9232 - binary_accuracy: 0.9232 - val_loss: 0.1937 - val_acc: 0.9396 - val_binary_accuracy: 0.9396\n",
      "Epoch 6/10\n",
      "3244/3244 [==============================] - 18s 5ms/step - loss: 0.1822 - acc: 0.9316 - binary_accuracy: 0.9316 - val_loss: 0.1821 - val_acc: 0.9408 - val_binary_accuracy: 0.9408\n",
      "Epoch 7/10\n",
      "3244/3244 [==============================] - 17s 5ms/step - loss: 0.1408 - acc: 0.9491 - binary_accuracy: 0.9491 - val_loss: 0.2153 - val_acc: 0.9371 - val_binary_accuracy: 0.9371\n",
      "Epoch 8/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.1122 - acc: 0.9649 - binary_accuracy: 0.9649 - val_loss: 0.1744 - val_acc: 0.9482 - val_binary_accuracy: 0.9482\n",
      "Epoch 9/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.0848 - acc: 0.9713 - binary_accuracy: 0.9713 - val_loss: 0.1921 - val_acc: 0.9457 - val_binary_accuracy: 0.9457\n",
      "Epoch 10/10\n",
      "3244/3244 [==============================] - 16s 5ms/step - loss: 0.0707 - acc: 0.9763 - binary_accuracy: 0.9763 - val_loss: 0.1915 - val_acc: 0.9556 - val_binary_accuracy: 0.9556\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v2(): # added embed   \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        50, #!!!!!!!!!!!!!!!!!!!!!!!\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(64,\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m2 = get_cnn_model_v2()\n",
    "check_model(m2,cnn_texts_mat,tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 1s 1ms/step\n",
      "loss: 5.74%\n",
      "acc: 98.44%\n",
      "binary_accuracy: 98.44%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m2.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m2.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m2.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m2.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30235898]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['help me'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m2.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00446203]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array([' bad service'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m2.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1000, 20)          20000     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1000, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 998, 256)          15616     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 101,665\n",
      "Trainable params: 101,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3244 samples, validate on 811 samples\n",
      "Epoch 1/10\n",
      "3244/3244 [==============================] - 32s 10ms/step - loss: 0.6865 - acc: 0.5419 - binary_accuracy: 0.5419 - val_loss: 0.6503 - val_acc: 0.6671 - val_binary_accuracy: 0.6671\n",
      "Epoch 2/10\n",
      "3244/3244 [==============================] - 30s 9ms/step - loss: 0.5386 - acc: 0.7577 - binary_accuracy: 0.7577 - val_loss: 0.3906 - val_acc: 0.8200 - val_binary_accuracy: 0.8200\n",
      "Epoch 3/10\n",
      "3244/3244 [==============================] - 30s 9ms/step - loss: 0.3830 - acc: 0.8314 - binary_accuracy: 0.8314 - val_loss: 0.2786 - val_acc: 0.8989 - val_binary_accuracy: 0.8989\n",
      "Epoch 4/10\n",
      "3244/3244 [==============================] - 30s 9ms/step - loss: 0.3191 - acc: 0.8699 - binary_accuracy: 0.8699 - val_loss: 0.2442 - val_acc: 0.9162 - val_binary_accuracy: 0.9162\n",
      "Epoch 5/10\n",
      "3244/3244 [==============================] - 32s 10ms/step - loss: 0.2795 - acc: 0.8863 - binary_accuracy: 0.8863 - val_loss: 0.2298 - val_acc: 0.9162 - val_binary_accuracy: 0.9162\n",
      "Epoch 6/10\n",
      "3244/3244 [==============================] - 31s 10ms/step - loss: 0.2474 - acc: 0.9041 - binary_accuracy: 0.9041 - val_loss: 0.2214 - val_acc: 0.9211 - val_binary_accuracy: 0.9211\n",
      "Epoch 7/10\n",
      "3244/3244 [==============================] - 31s 10ms/step - loss: 0.2199 - acc: 0.9155 - binary_accuracy: 0.9155 - val_loss: 0.2042 - val_acc: 0.9445 - val_binary_accuracy: 0.9445\n",
      "Epoch 8/10\n",
      "3244/3244 [==============================] - 33s 10ms/step - loss: 0.1946 - acc: 0.9254 - binary_accuracy: 0.9254 - val_loss: 0.2119 - val_acc: 0.9445 - val_binary_accuracy: 0.9445\n",
      "Epoch 9/10\n",
      "3244/3244 [==============================] - 30s 9ms/step - loss: 0.1814 - acc: 0.9316 - binary_accuracy: 0.9316 - val_loss: 0.1905 - val_acc: 0.9470 - val_binary_accuracy: 0.9470\n",
      "Epoch 10/10\n",
      "3244/3244 [==============================] - 32s 10ms/step - loss: 0.1552 - acc: 0.9383 - binary_accuracy: 0.9383 - val_loss: 0.2022 - val_acc: 0.9531 - val_binary_accuracy: 0.9531\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model_v3():    # added filter\n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    # 1000 is num_max\n",
    "    model.add(Embedding(1000,\n",
    "                        20,\n",
    "                        input_length=max_len))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(256, #!!!!!!!!!!!!!!!!!!!\n",
    "                     3,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc',metrics.binary_accuracy])\n",
    "    return model\n",
    "\n",
    "m3 = get_cnn_model_v3()\n",
    "check_model(m3,cnn_texts_mat,tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217/1217 [==============================] - 1s 998us/step\n",
      "loss: 5.71%\n",
      "acc: 98.44%\n",
      "binary_accuracy: 98.44%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(cnn_texts_mat, tags, test_size=0.3)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "scores = m2.evaluate(X_test, Y_test, batch_size=128)\n",
    "    \n",
    "print(\"%s: %.2f%%\" % (m3.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (m3.metrics_names[1], scores[1] * 100))\n",
    "print(\"%s: %.2f%%\" % (m3.metrics_names[2], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7769191]]\n",
      "ISSUE\n"
     ]
    }
   ],
   "source": [
    "text = np.array(['help me'])\n",
    "sequences = tok.texts_to_sequences(text)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=num_max)\n",
    "prediction = m3.predict(sequences_matrix)\n",
    "print(prediction)\n",
    "if prediction >0.8 :\n",
    "    print(\"NOTISSUE\")\n",
    "else :\n",
    "    print(\"ISSUE\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
