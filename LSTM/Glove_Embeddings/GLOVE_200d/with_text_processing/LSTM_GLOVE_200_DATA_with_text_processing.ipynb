{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import pickle\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/debadridtt/A-Review-of-Different-Word-Embeddings-for-Sentiment-Classification-using-Deep-Learning/blob/master/LSTM%20Experiment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  I had ordered a data cable, got a very well fi...  NOTISSUE\n",
      "1                                   Love This Phone.  NOTISSUE\n",
      "2                I get a very well finished product.  NOTISSUE\n",
      "3                            I could not be happier.  NOTISSUE\n",
      "4  I was looking for this headset for a long time...  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = 'C:/Users/khmar/Desktop/ISSUE/dataset/CSV/data_ameliorate/data.csv'\n",
    "df = pd.read_csv(DATA_FILE,delimiter=';',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOTISSUE    2030\n",
       "ISSUE       2025\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts() #imbalanced Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khmar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                      # the natural langauage toolkit, open-source NLP\n",
    "import gensim\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing\n",
    "import re# Help in preprocessing the data, very efficiently\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS ={\n",
    " 'a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'across',\n",
    " 'after',\n",
    " 'afterwards',\n",
    " 'again',\n",
    " 'against',\n",
    " 'all',\n",
    " 'almost',\n",
    " 'alone',\n",
    " 'along',\n",
    " 'already',\n",
    " 'also',\n",
    " 'although',\n",
    " 'always',\n",
    " 'am',\n",
    " 'among',\n",
    " 'amongst',\n",
    " 'amount',\n",
    " 'an',\n",
    " 'and',\n",
    " 'another',\n",
    " 'any',\n",
    " 'anyhow',\n",
    " 'anyone',\n",
    " 'anything',\n",
    " 'anyway',\n",
    " 'anywhere',\n",
    " 'are',\n",
    " 'around',\n",
    " 'as',\n",
    " 'at',\n",
    " 'back',\n",
    " 'be',\n",
    " 'became',\n",
    " 'because',\n",
    " 'become',\n",
    " 'becomes',\n",
    " 'becoming',\n",
    " 'been',\n",
    " 'before',\n",
    " 'beforehand',\n",
    " 'behind',\n",
    " 'being',\n",
    " 'below',\n",
    " 'beside',\n",
    " 'besides',\n",
    " 'between',\n",
    " 'beyond',\n",
    " 'both',\n",
    " 'bottom',\n",
    " 'but',\n",
    " 'by',\n",
    " 'ca',\n",
    " 'call',\n",
    " 'can',\n",
    " 'cannot',\n",
    " 'could',\n",
    " 'did',\n",
    " 'do',\n",
    " 'does',\n",
    " 'doing',\n",
    " 'done',\n",
    " 'down',\n",
    " 'due',\n",
    " 'during',\n",
    " 'each',\n",
    " 'eight',\n",
    " 'either',\n",
    " 'eleven',\n",
    " 'else',\n",
    " 'elsewhere',\n",
    " 'empty',\n",
    " 'enough',\n",
    " 'even',\n",
    " 'ever',\n",
    " 'every',\n",
    " 'everyone',\n",
    " 'everything',\n",
    " 'everywhere',\n",
    " 'except',\n",
    " 'few',\n",
    " 'fifteen',\n",
    " 'fifty',\n",
    " 'first',\n",
    " 'five',\n",
    " 'for',\n",
    " 'former',\n",
    " 'formerly',\n",
    " 'forty',\n",
    " 'four',\n",
    " 'from',\n",
    " 'front',\n",
    " 'full',\n",
    " 'further',\n",
    " 'get',\n",
    " 'give',\n",
    " 'go',\n",
    " 'had',\n",
    " 'has',\n",
    " 'have',\n",
    " 'he',\n",
    " 'hence',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hereafter',\n",
    " 'hereby',\n",
    " 'herein',\n",
    " 'hereupon',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'however',\n",
    " 'hundred',\n",
    " 'i',\n",
    " 'if',\n",
    " 'in',\n",
    " 'indeed',\n",
    " 'into',\n",
    " 'is',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'just',\n",
    " 'keep',\n",
    " 'last',\n",
    " 'latter',\n",
    " 'latterly',\n",
    " 'least',\n",
    " 'less',\n",
    " 'made',\n",
    " 'make',\n",
    " 'many',\n",
    " 'may',\n",
    " 'me',\n",
    " 'meanwhile',\n",
    " 'might',\n",
    " 'mine',\n",
    " 'more',\n",
    " 'moreover',\n",
    " 'most',\n",
    " 'mostly',\n",
    " 'move',\n",
    " 'much',\n",
    " 'must',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'name',\n",
    " 'namely',\n",
    " 'neither',\n",
    " 'never',\n",
    " 'nevertheless',\n",
    " 'next',\n",
    " 'nine',\n",
    " 'nobody',\n",
    " 'none',\n",
    " 'noone',\n",
    " 'nor',\n",
    " 'nothing',\n",
    " 'now',\n",
    " 'nowhere',\n",
    " 'of',\n",
    " 'off',\n",
    " 'often',\n",
    " 'on',\n",
    " 'once',\n",
    " 'one',\n",
    " 'only',\n",
    " 'onto',\n",
    " 'or',\n",
    " 'other',\n",
    " 'others',\n",
    " 'otherwise',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 'part',\n",
    " 'per',\n",
    " 'perhaps',\n",
    " 'please',\n",
    " 'put',\n",
    " 'quite',\n",
    " 'rather',\n",
    " 're',\n",
    " 'really',\n",
    " 'regarding',\n",
    " 'same',\n",
    " 'say',\n",
    " 'see',\n",
    " 'seem',\n",
    " 'seemed',\n",
    " 'seeming',\n",
    " 'seems',\n",
    " 'serious',\n",
    " 'several',\n",
    " 'she',\n",
    " 'should',\n",
    " 'show',\n",
    " 'side',\n",
    " 'since',\n",
    " 'six',\n",
    " 'sixty',\n",
    " 'so',\n",
    " 'some',\n",
    " 'somehow',\n",
    " 'someone',\n",
    " 'something',\n",
    " 'sometime',\n",
    " 'sometimes',\n",
    " 'somewhere',\n",
    " 'still',\n",
    " 'such',\n",
    " 'take',\n",
    " 'ten',\n",
    " 'than',\n",
    " 'that',\n",
    " 'the',\n",
    " 'their',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'thence',\n",
    " 'there',\n",
    " 'thereafter',\n",
    " 'thereby',\n",
    " 'therefore',\n",
    " 'therein',\n",
    " 'thereupon',\n",
    " 'these',\n",
    " 'they',\n",
    " 'third',\n",
    " 'this',\n",
    " 'those',\n",
    " 'though',\n",
    " 'three',\n",
    " 'through',\n",
    " 'throughout',\n",
    " 'thru',\n",
    " 'thus',\n",
    " 'to',\n",
    " 'together',\n",
    " 'too',\n",
    " 'top',\n",
    " 'toward',\n",
    " 'towards',\n",
    " 'twelve',\n",
    " 'twenty',\n",
    " 'two',\n",
    " 'under',\n",
    " 'unless',\n",
    " 'until',\n",
    " 'up',\n",
    " 'upon',\n",
    " 'us',\n",
    " 'used',\n",
    " 'using',\n",
    " 'various',\n",
    " 'very',\n",
    " 'via',\n",
    " 'was',\n",
    " 'we',\n",
    " 'well',\n",
    " 'were',\n",
    " 'what',\n",
    " 'whatever',\n",
    " 'when',\n",
    " 'whence',\n",
    " 'whenever',\n",
    " 'where',\n",
    " 'whereafter',\n",
    " 'whereas',\n",
    " 'whereby',\n",
    " 'wherein',\n",
    " 'whereupon',\n",
    " 'wherever',\n",
    " 'whether',\n",
    " 'which',\n",
    " 'while',\n",
    " 'whither',\n",
    " 'who',\n",
    " 'whoever',\n",
    " 'whole',\n",
    " 'whom',\n",
    " 'whose',\n",
    " 'why',\n",
    " 'will',\n",
    " 'with',\n",
    " 'within',\n",
    " 'without',\n",
    " 'would',\n",
    " 'yet',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    #print(text)\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Removing all the stopwords\n",
    "    text = [word for word in text.split() if word not in STOP_WORDS]\n",
    "\n",
    "    ##Fixing Word Lengthening\n",
    "    #pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    #pattern.sub(r\"\\1\\1\", text)\n",
    "    #print(text)\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(text)\n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "    # Strip all the numerics\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    ##Lemmatisation\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    \n",
    "    # Stemming\n",
    "    text=gensim.parsing.preprocessing.stem_text(text)\n",
    "    #Spellchecker \n",
    "    #correcteur\n",
    "    # find those words that may be misspelled\n",
    "    spell = SpellChecker()\n",
    "    misspelled = text.split()\n",
    "    for i in range(len(misspelled)):\n",
    "    # Get the one `most likely` answer\n",
    "        word = spell.correction(misspelled[i])\n",
    "        misspelled[i]=word\n",
    "        text = \" \".join(misspelled)\n",
    "\n",
    "    # Reduce words to their root form\n",
    "    text = [WordNetLemmatizer().lemmatize(word) for word in text.split()]\n",
    "        \n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    #text=word_tokenize(text)\n",
    "    for word in text:\n",
    "        word=lemmatizer.lemmatize(word,pos='v')\n",
    "        word=lemmatizer.lemmatize(word,pos='n')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['service', 'group']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(\"this servic groop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(transform)\n",
    "texts= df['text']\n",
    "tags= df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'(0         [order, data, call, got, finish, work, product]\n1                                           [love, phone]\n2                                       [finish, product]\n3                                          [not, happier]\n4                        [look, headset, long, time, got]\n5                      [headset, start, ring, no, reason]\n6                    [display, excel, camera, good, year]\n7                                  [battery, life, great]\n8                              [worst, phone, had, month]\n9       [not, good, item, work, start, have, problem, ...\n10      [embarrass, ear, hurt, try, push, ear, plug, ear]\n11                                 [protect, phone, side]\n12      [average, phone, bad, battery, life, over, wea...\n13      [clear, skye, call, long, battery, life, long,...\n14                               [solo, technology, suck]\n15                            [great, hand, free, device]\n16      [self, portrait, outside, exterior, display, c...\n17                                     [problem, mention]\n18             [try, handsome, gadget, final, work, well]\n19                                          [magic, help]\n20         [worst, piece, crap, version, custom, service]\n21                                 [poor, sound, quality]\n22                                  [best, phone, market]\n23                                           [work, well]\n24         [company, ship, product, promptly, work, well]\n25                                        [exactly, want]\n26      [picture, resolute, far, compare, price, phone...\n27                                          [great, deal]\n28                     [excel, product, satisfy, purchas]\n29            [highly, recommend, encourage, people, try]\n                              ...                        \n4025                     [flair, barend, absolute, amaze]\n4026                [frozen, margarita, wai, sugar, last]\n4027                                 [good, order, twice]\n4028    [nutshell, restaurant, smell, like, combine, d...\n4029                              [girlfriend, veal, bad]\n4030                                [unfortun, not, good]\n4031                            [pretty, satisfy, expert]\n4032                  [join, club, awesome, offer, email]\n4033    [perfect, me, like, beer, ic, cold, case, colder]\n4034    [bland, flavourless, good, wai, describe, bare...\n4035            [chain, no, fan, of, beat, place, easily]\n4036                                        [macho, have]\n4037                                    [not, come, back]\n4038                  [nathan, word, place, pretty, well]\n4039    [staff, super, nice, quick, crazy, crowd, down...\n4040         [great, atmosphere, friendly, fast, service]\n4041       [receive, pity, huge, lot, meat, thumb, there]\n4042                                   [food, arrive, me]\n4043    [pai, hot, dog, fri, look, like, came, kid, me...\n4044              [classic, main, lobster, roll, fantasy]\n4045    [brother, law, work, mall, at, dai, guess, sic...\n4046                     [good, go, review, place, twice]\n4047                    [chip, salsa, good, salsa, fresh]\n4048                                       [place, great]\n4049                                     [mediocre, food]\n4050                             [inside, impress, place]\n4051                           [service, super, friendly]\n4052                       [sad, little, beget, overcook]\n4053                              [place, nice, surprise]\n4054                           [live, music, total, blow]\nName: text, Length: 4055, dtype: object, 0       NOTISSUE\n1       NOTISSUE\n2       NOTISSUE\n3       NOTISSUE\n4       NOTISSUE\n5          ISSUE\n6       NOTISSUE\n7       NOTISSUE\n8          ISSUE\n9          ISSUE\n10         ISSUE\n11      NOTISSUE\n12         ISSUE\n13      NOTISSUE\n14         ISSUE\n15      NOTISSUE\n16      NOTISSUE\n17         ISSUE\n18      NOTISSUE\n19      NOTISSUE\n20         ISSUE\n21         ISSUE\n22      NOTISSUE\n23      NOTISSUE\n24      NOTISSUE\n25      NOTISSUE\n26         ISSUE\n27      NOTISSUE\n28      NOTISSUE\n29      NOTISSUE\n          ...   \n4025    NOTISSUE\n4026       ISSUE\n4027    NOTISSUE\n4028       ISSUE\n4029       ISSUE\n4030       ISSUE\n4031    NOTISSUE\n4032    NOTISSUE\n4033    NOTISSUE\n4034       ISSUE\n4035       ISSUE\n4036    NOTISSUE\n4037       ISSUE\n4038    NOTISSUE\n4039    NOTISSUE\n4040    NOTISSUE\n4041    NOTISSUE\n4042       ISSUE\n4043       ISSUE\n4044    NOTISSUE\n4045       ISSUE\n4046    NOTISSUE\n4047    NOTISSUE\n4048    NOTISSUE\n4049       ISSUE\n4050    NOTISSUE\n4051    NOTISSUE\n4052       ISSUE\n4053    NOTISSUE\n4054    NOTISSUE\nName: label, Length: 4055, dtype: object)' is an invalid key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8a09c3838e60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2655\u001b[0m                                  'backfill or nearest lookups')\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(0         [order, data, call, got, finish, work, product]\n1                                           [love, phone]\n2                                       [finish, product]\n3                                          [not, happier]\n4                        [look, headset, long, time, got]\n5                      [headset, start, ring, no, reason]\n6                    [display, excel, camera, good, year]\n7                                  [battery, life, great]\n8                              [worst, phone, had, month]\n9       [not, good, item, work, start, have, problem, ...\n10      [embarrass, ear, hurt, try, push, ear, plug, ear]\n11                                 [protect, phone, side]\n12      [average, phone, bad, battery, life, over, wea...\n13      [clear, skye, call, long, battery, life, long,...\n14                               [solo, technology, suck]\n15                            [great, hand, free, device]\n16      [self, portrait, outside, exterior, display, c...\n17                                     [problem, mention]\n18             [try, handsome, gadget, final, work, well]\n19                                          [magic, help]\n20         [worst, piece, crap, version, custom, service]\n21                                 [poor, sound, quality]\n22                                  [best, phone, market]\n23                                           [work, well]\n24         [company, ship, product, promptly, work, well]\n25                                        [exactly, want]\n26      [picture, resolute, far, compare, price, phone...\n27                                          [great, deal]\n28                     [excel, product, satisfy, purchas]\n29            [highly, recommend, encourage, people, try]\n                              ...                        \n4025                     [flair, barend, absolute, amaze]\n4026                [frozen, margarita, wai, sugar, last]\n4027                                 [good, order, twice]\n4028    [nutshell, restaurant, smell, like, combine, d...\n4029                              [girlfriend, veal, bad]\n4030                                [unfortun, not, good]\n4031                            [pretty, satisfy, expert]\n4032                  [join, club, awesome, offer, email]\n4033    [perfect, me, like, beer, ic, cold, case, colder]\n4034    [bland, flavourless, good, wai, describe, bare...\n4035            [chain, no, fan, of, beat, place, easily]\n4036                                        [macho, have]\n4037                                    [not, come, back]\n4038                  [nathan, word, place, pretty, well]\n4039    [staff, super, nice, quick, crazy, crowd, down...\n4040         [great, atmosphere, friendly, fast, service]\n4041       [receive, pity, huge, lot, meat, thumb, there]\n4042                                   [food, arrive, me]\n4043    [pai, hot, dog, fri, look, like, came, kid, me...\n4044              [classic, main, lobster, roll, fantasy]\n4045    [brother, law, work, mall, at, dai, guess, sic...\n4046                     [good, go, review, place, twice]\n4047                    [chip, salsa, good, salsa, fresh]\n4048                                       [place, great]\n4049                                     [mediocre, food]\n4050                             [inside, impress, place]\n4051                           [service, super, friendly]\n4052                       [sad, little, beget, overcook]\n4053                              [place, nice, surprise]\n4054                           [live, music, total, blow]\nName: text, Length: 4055, dtype: object, 0       NOTISSUE\n1       NOTISSUE\n2       NOTISSUE\n3       NOTISSUE\n4       NOTISSUE\n5          ISSUE\n6       NOTISSUE\n7       NOTISSUE\n8          ISSUE\n9          ISSUE\n10         ISSUE\n11      NOTISSUE\n12         ISSUE\n13      NOTISSUE\n14         ISSUE\n15      NOTISSUE\n16      NOTISSUE\n17         ISSUE\n18      NOTISSUE\n19      NOTISSUE\n20         ISSUE\n21         ISSUE\n22      NOTISSUE\n23      NOTISSUE\n24      NOTISSUE\n25      NOTISSUE\n26         ISSUE\n27      NOTISSUE\n28      NOTISSUE\n29      NOTISSUE\n          ...   \n4025    NOTISSUE\n4026       ISSUE\n4027    NOTISSUE\n4028       ISSUE\n4029       ISSUE\n4030       ISSUE\n4031    NOTISSUE\n4032    NOTISSUE\n4033    NOTISSUE\n4034       ISSUE\n4035       ISSUE\n4036    NOTISSUE\n4037       ISSUE\n4038    NOTISSUE\n4039    NOTISSUE\n4040    NOTISSUE\n4041    NOTISSUE\n4042       ISSUE\n4043       ISSUE\n4044    NOTISSUE\n4045       ISSUE\n4046    NOTISSUE\n4047    NOTISSUE\n4048    NOTISSUE\n4049       ISSUE\n4050    NOTISSUE\n4051    NOTISSUE\n4052       ISSUE\n4053    NOTISSUE\n4054    NOTISSUE\nName: label, Length: 4055, dtype: object)' is an invalid key"
     ]
    }
   ],
   "source": [
    "df=df[texts,tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of lists  \n",
    "dict = {'text': texts, 'label': tags } \n",
    "     \n",
    "df = pd.DataFrame(dict) \n",
    "  \n",
    "# saving the dataframe \n",
    "df.to_csv('DATA_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                     text     label\n",
      "0   0  order data cabl got finish work product  NOTISSUE\n",
      "1   1                               love phone  NOTISSUE\n",
      "2   2                           finish product  NOTISSUE\n",
      "3   3                              not happier  NOTISSUE\n",
      "4   4               look headset long time got  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = 'C:/Users/khmar/git_repo/IssueModelTraining/DATA/DATA_text_preprocessing.csv'\n",
    "df = pd.read_csv(DATA_FILE,delimiter=',',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text     label\n",
      "0           0  ['order', 'data', 'call', 'got', 'finish', 'wo...  NOTISSUE\n",
      "1           1                                  ['love', 'phone']  NOTISSUE\n",
      "2           2                              ['finish', 'product']  NOTISSUE\n",
      "3           3                                 ['not', 'happier']  NOTISSUE\n",
      "4           4         ['look', 'headset', 'long', 'time', 'got']  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = 'DATA_preprocessing.csv'\n",
    "df = pd.read_csv(DATA_FILE,delimiter=',',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df)) < 0.7 # Splitting into train(70%) and test(30%) randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=df[msk]\n",
    "test_df=df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2812, 3)\n",
      "(1243, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking target values for train data:\n",
      "\n",
      "ISSUE       1427\n",
      "NOTISSUE    1385\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "Checking target values for test data:\n",
      "\n",
      "NOTISSUE    645\n",
      "ISSUE       598\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Checking target values for train data:\\n')\n",
    "print(train_df['label'].value_counts(),'\\n')\n",
    "print('Checking target values for test data:\\n')\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISSUE       1427\n",
       "NOTISSUE    1385\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_df['text']\n",
    "y_train=train_df['label']\n",
    "x_test=test_df['text']\n",
    "y_test=test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = x_train.astype(str)\n",
    "texts_test = x_test.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_df['text']\n",
    "y_train=train_df['label']\n",
    "x_test=test_df['text']\n",
    "y_test=test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLoVe Embedding 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "maxlen = 120  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = x_train.astype(str)\n",
    "texts_test = x_test.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tok = 'LSTM_token_GLOVE_200_DATA_with_text_processing.sav'\n",
    "pickle.dump(tokenizer, open(file_tok, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding=\"pre\", truncating=\"post\")\n",
    "test_data = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n",
    "                          truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2857 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2812, 200)\n",
      "Shape of data test tensor: (1243, 200)\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 200\n",
    "#pad sequences are used to bring all sentences to same size.\n",
    "# pad sequences with 0s\n",
    "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of data test tensor:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map({\"NOTISSUE\": 1, \"ISSUE\" : 0 })\n",
    "y_test = y_test.map({\"NOTISSUE\": 1, \"ISSUE\" : 0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = {}\n",
    "f = open('C:/Users/khmar/Desktop/GLoVE/glove.6B.200d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 200))\n",
    "#Found 1489 unique tokens : word_index\n",
    "#print(len(word_index) + 1) #### 1490 \n",
    "#print(word_index.items()) ### unique tokens : words :word_index\n",
    "#print('embedding_matrix',embedding_matrix)\n",
    "out_of_vocab={}\n",
    "for word, i in word_index.items():\n",
    "    #print(i)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    #print('embedding_vector' ,embedding_vector )\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        #print('word: ', word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        #print('embedding_matrix :',embedding_matrix[i] )\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "    if embedding_vector is  None:\n",
    "        #print('word not found :',word)\n",
    "        out_of_vocab[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_output= \"C:/Users/khmar/Desktop/out_of_vocab_glove_200_without_text_processing.txt\"\n",
    "#with open(file_output, \"w\") as f1:\n",
    "    #for word, i in out_of_vocab.items():\n",
    "        #print(out_of_vocab.get(word))\n",
    "        #f1.write(out_of_vocab.get(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 2856 out of vocab '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'There are {len(out_of_vocab)} out of vocab '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model_glove= Sequential()\n",
    "model_glove.add(Embedding(len(word_index) + 1,\n",
    "                            200,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "model_glove.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,input_shape=(1,)))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2812 samples, validate on 1243 samples\n",
      "Epoch 1/2\n",
      "2812/2812 [==============================] - 28s 10ms/step - loss: 0.6936 - acc: 0.4968 - val_loss: 0.6944 - val_acc: 0.4811\n",
      "Epoch 2/2\n",
      "2812/2812 [==============================] - 29s 10ms/step - loss: 0.6933 - acc: 0.5000 - val_loss: 0.6945 - val_acc: 0.4811\n",
      "Wall time: 58.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279ed6b0358>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_glove.fit(data, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=(test_data, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1266/1266 [==============================] - 3s 2ms/step\n",
      "loss: 47.05%\n",
      "acc: 76.15%\n"
     ]
    }
   ],
   "source": [
    "scores = model_glove.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print(\"%s: %.2f%%\" % (model_glove.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (model_glove.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_glove.save('LSTM_model_glove_200_DATA_with_text_processing.sav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
