{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/debadridtt/A-Review-of-Different-Word-Embeddings-for-Sentiment-Classification-using-Deep-Learning/blob/master/LSTM%20Experiment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  I had ordered a data cable, got a very well fi...  NOTISSUE\n",
      "1                                   Love This Phone.  NOTISSUE\n",
      "2                I get a very well finished product.  NOTISSUE\n",
      "3                            I could not be happier.  NOTISSUE\n",
      "4  I was looking for this headset for a long time...  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "DATA_FILE = 'C:/Users/khmar/Desktop/ISSUE/dataset/CSV/data_ameliorate/data.csv'\n",
    "df = pd.read_csv(DATA_FILE,delimiter=';',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOTISSUE    2030\n",
       "ISSUE       2025\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts() #imbalanced Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4055, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocessing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khmar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                      # the natural langauage toolkit, open-source NLP\n",
    "import gensim\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing\n",
    "import re# Help in preprocessing the data, very efficiently\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS ={\n",
    " 'a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'across',\n",
    " 'after',\n",
    " 'afterwards',\n",
    " 'again',\n",
    " 'against',\n",
    " 'all',\n",
    " 'almost',\n",
    " 'alone',\n",
    " 'along',\n",
    " 'already',\n",
    " 'also',\n",
    " 'although',\n",
    " 'always',\n",
    " 'am',\n",
    " 'among',\n",
    " 'amongst',\n",
    " 'amount',\n",
    " 'an',\n",
    " 'and',\n",
    " 'another',\n",
    " 'any',\n",
    " 'anyhow',\n",
    " 'anyone',\n",
    " 'anything',\n",
    " 'anyway',\n",
    " 'anywhere',\n",
    " 'are',\n",
    " 'around',\n",
    " 'as',\n",
    " 'at',\n",
    " 'back',\n",
    " 'be',\n",
    " 'became',\n",
    " 'because',\n",
    " 'become',\n",
    " 'becomes',\n",
    " 'becoming',\n",
    " 'been',\n",
    " 'before',\n",
    " 'beforehand',\n",
    " 'behind',\n",
    " 'being',\n",
    " 'below',\n",
    " 'beside',\n",
    " 'besides',\n",
    " 'between',\n",
    " 'beyond',\n",
    " 'both',\n",
    " 'bottom',\n",
    " 'but',\n",
    " 'by',\n",
    " 'ca',\n",
    " 'call',\n",
    " 'can',\n",
    " 'cannot',\n",
    " 'could',\n",
    " 'did',\n",
    " 'do',\n",
    " 'does',\n",
    " 'doing',\n",
    " 'done',\n",
    " 'down',\n",
    " 'due',\n",
    " 'during',\n",
    " 'each',\n",
    " 'eight',\n",
    " 'either',\n",
    " 'eleven',\n",
    " 'else',\n",
    " 'elsewhere',\n",
    " 'empty',\n",
    " 'enough',\n",
    " 'even',\n",
    " 'ever',\n",
    " 'every',\n",
    " 'everyone',\n",
    " 'everything',\n",
    " 'everywhere',\n",
    " 'except',\n",
    " 'few',\n",
    " 'fifteen',\n",
    " 'fifty',\n",
    " 'first',\n",
    " 'five',\n",
    " 'for',\n",
    " 'former',\n",
    " 'formerly',\n",
    " 'forty',\n",
    " 'four',\n",
    " 'from',\n",
    " 'front',\n",
    " 'full',\n",
    " 'further',\n",
    " 'get',\n",
    " 'give',\n",
    " 'go',\n",
    " 'had',\n",
    " 'has',\n",
    " 'have',\n",
    " 'he',\n",
    " 'hence',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hereafter',\n",
    " 'hereby',\n",
    " 'herein',\n",
    " 'hereupon',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'however',\n",
    " 'hundred',\n",
    " 'i',\n",
    " 'if',\n",
    " 'in',\n",
    " 'indeed',\n",
    " 'into',\n",
    " 'is',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'just',\n",
    " 'keep',\n",
    " 'last',\n",
    " 'latter',\n",
    " 'latterly',\n",
    " 'least',\n",
    " 'less',\n",
    " 'made',\n",
    " 'make',\n",
    " 'many',\n",
    " 'may',\n",
    " 'me',\n",
    " 'meanwhile',\n",
    " 'might',\n",
    " 'mine',\n",
    " 'more',\n",
    " 'moreover',\n",
    " 'most',\n",
    " 'mostly',\n",
    " 'move',\n",
    " 'much',\n",
    " 'must',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'name',\n",
    " 'namely',\n",
    " 'neither',\n",
    " 'never',\n",
    " 'nevertheless',\n",
    " 'next',\n",
    " 'nine',\n",
    " 'nobody',\n",
    " 'none',\n",
    " 'noone',\n",
    " 'nor',\n",
    " 'nothing',\n",
    " 'now',\n",
    " 'nowhere',\n",
    " 'of',\n",
    " 'off',\n",
    " 'often',\n",
    " 'on',\n",
    " 'once',\n",
    " 'one',\n",
    " 'only',\n",
    " 'onto',\n",
    " 'or',\n",
    " 'other',\n",
    " 'others',\n",
    " 'otherwise',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 'part',\n",
    " 'per',\n",
    " 'perhaps',\n",
    " 'please',\n",
    " 'put',\n",
    " 'quite',\n",
    " 'rather',\n",
    " 're',\n",
    " 'really',\n",
    " 'regarding',\n",
    " 'same',\n",
    " 'say',\n",
    " 'see',\n",
    " 'seem',\n",
    " 'seemed',\n",
    " 'seeming',\n",
    " 'seems',\n",
    " 'serious',\n",
    " 'several',\n",
    " 'she',\n",
    " 'should',\n",
    " 'show',\n",
    " 'side',\n",
    " 'since',\n",
    " 'six',\n",
    " 'sixty',\n",
    " 'so',\n",
    " 'some',\n",
    " 'somehow',\n",
    " 'someone',\n",
    " 'something',\n",
    " 'sometime',\n",
    " 'sometimes',\n",
    " 'somewhere',\n",
    " 'still',\n",
    " 'such',\n",
    " 'take',\n",
    " 'ten',\n",
    " 'than',\n",
    " 'that',\n",
    " 'the',\n",
    " 'their',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'thence',\n",
    " 'there',\n",
    " 'thereafter',\n",
    " 'thereby',\n",
    " 'therefore',\n",
    " 'therein',\n",
    " 'thereupon',\n",
    " 'these',\n",
    " 'they',\n",
    " 'third',\n",
    " 'this',\n",
    " 'those',\n",
    " 'though',\n",
    " 'three',\n",
    " 'through',\n",
    " 'throughout',\n",
    " 'thru',\n",
    " 'thus',\n",
    " 'to',\n",
    " 'together',\n",
    " 'too',\n",
    " 'top',\n",
    " 'toward',\n",
    " 'towards',\n",
    " 'twelve',\n",
    " 'twenty',\n",
    " 'two',\n",
    " 'under',\n",
    " 'unless',\n",
    " 'until',\n",
    " 'up',\n",
    " 'upon',\n",
    " 'us',\n",
    " 'used',\n",
    " 'using',\n",
    " 'various',\n",
    " 'very',\n",
    " 'via',\n",
    " 'was',\n",
    " 'we',\n",
    " 'well',\n",
    " 'were',\n",
    " 'what',\n",
    " 'whatever',\n",
    " 'when',\n",
    " 'whence',\n",
    " 'whenever',\n",
    " 'where',\n",
    " 'whereafter',\n",
    " 'whereas',\n",
    " 'whereby',\n",
    " 'wherein',\n",
    " 'whereupon',\n",
    " 'wherever',\n",
    " 'whether',\n",
    " 'which',\n",
    " 'while',\n",
    " 'whither',\n",
    " 'who',\n",
    " 'whoever',\n",
    " 'whole',\n",
    " 'whom',\n",
    " 'whose',\n",
    " 'why',\n",
    " 'will',\n",
    " 'with',\n",
    " 'within',\n",
    " 'without',\n",
    " 'would',\n",
    " 'yet',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import parsing\n",
    "from gensim.parsing.preprocessing import split_alphanum\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "\n",
    "\n",
    "def normaliser_word(word):\n",
    "    slangs_dict = {\n",
    "    'awsm': 'awesome',\n",
    "    \"aamof\": \"as a matter of fact\",\n",
    "    \"abt\": \"about\",\n",
    "    \"abt2\": \"about to\",\n",
    "    \"ac\": \"air conditioning\",\n",
    "    \"ace\": \"solo winner\",\n",
    "    \"ack\": \"acknowledged\",\n",
    "    \"admin\": \"administrator\",\n",
    "    \"thr\": \"there\",\n",
    "    \"frm\": \"from\",\n",
    "    \"aggro\": \"aggression\",\n",
    "    \"agl\": \"angel\",\n",
    "    \"dob\": \"date of birth\",\n",
    "    \"ai\": \"artificial intelligence\",\n",
    "    \"aiic\": \"as if i care\",\n",
    "    \"aka\": \"also known as\",\n",
    "    \"alap\": \"as long as possible\",\n",
    "    \"alol\": \"actually laughing out loud\",\n",
    "    \"ama\": \"ask me anything\",\n",
    "    \"amap\": \"as much as possible\",\n",
    "    \"amazn\": \"amazing\",\n",
    "    \"ammo\": \"ammunition\",\n",
    "    \"ams\": \"ask me something\",\n",
    "    \"anon\": \"anonymous\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"asat\": \"as simple as that\",\n",
    "    \"awks\": \"awkward\",\n",
    "    \"awl\": \"always with love\",\n",
    "    \"ayk\": \"as you know\",\n",
    "    \"azm\": \"awesome\",\n",
    "    \"b\": \"be\",\n",
    "    \"b&w\": \"black and white\",\n",
    "    \"b-day\": \"birthday\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"bcoz\": \"because\",\n",
    "    \"bcos\": \"because\",\n",
    "    \"bcz\": \"because\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"btw\": \"between\",\n",
    "    \"b4\": \"before\",\n",
    "    \"bai\": \"bye\",\n",
    "    \"bb\": \"bye bye\",\n",
    "    \"bc\": \"abuse\",\n",
    "    \"mc\": \"abuse\",\n",
    "    \"bcc\": \"blind carbon copy\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"biz\": \"business\",\n",
    "    \"bk\": \"back\",\n",
    "    \"bo\": \"back off\",\n",
    "    \"bro\": \"brother\",\n",
    "    \"btwn\": \"between\",\n",
    "    \"c\": \"see\",\n",
    "    \"char\": \"character\",\n",
    "    \"combo\": \"combination\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cu2\": \"see you too\",\n",
    "    \"cu2mr\": \"see you tomorrow\",\n",
    "    \"cya\": \"see ya\",\n",
    "    \"cyal8r\": \"see you later\",\n",
    "    \"cyb\": \"call you back\",\n",
    "    \"cye\": \"check your e-mail\",\n",
    "    \"cyf\": \"check your facebook\",\n",
    "    \"cyfb\": \"check your facebook\",\n",
    "    \"cyl\": \"catch ya later\",\n",
    "    \"cym\": \"check your myspace\",\n",
    "    \"cyo\": \"see you online\",\n",
    "    \"d8\": \"date\",\n",
    "    \"da\": \"the\",\n",
    "    \"dece\": \"decent\",\n",
    "    \"ded\": \"dead\",\n",
    "    \"dept\": \"department\",\n",
    "    \"dis\": \"this\",\n",
    "    \"ditto\": \"same\",\n",
    "    \"diva\": \"rude woman\",\n",
    "    \"dk\": \"don't know\",\n",
    "    \"dlm\": \"don't leave me\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"dnd\": \"do not disturb\",\n",
    "    \"dno\": \"don't know\",\n",
    "    \"dnt\": \"don't\",\n",
    "    \"e1\": \"everyone\",\n",
    "    \"eg\": \"for example\",\n",
    "    \"emc2\": \"genius\",\n",
    "    \"emo\": \"emotional\",\n",
    "    \"enuf\": \"enough\",\n",
    "    \"eod\": \"end of discussion\",\n",
    "    \"eof\": \"end of file\",\n",
    "    \"eom\": \"end of message\",\n",
    "    \"eta\": \"estimated time of arrival\",\n",
    "    \"every1\": \"everyone\",\n",
    "    \"evs\": \"whatever\",\n",
    "    \"exp\": \"experience\",\n",
    "    \"f\": \"female\",\n",
    "    \"f2f\": \"face to face\",\n",
    "    \"f2p\": \"free to play\",\n",
    "    \"f2t\": \"free to talk\",\n",
    "    \"f9\": \"fine\",\n",
    "    \"fab\": \"fabulous\",\n",
    "    \"fail\": \"failure\",\n",
    "    \"faq\": \"frequently asked questions\",\n",
    "    \"fav\": \"favorite\",\n",
    "    \"fave\": \"favorite\",\n",
    "    \"favs\": \"favorites\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"fbc\": \"facebook chat\",\n",
    "    \"fbf\": \"facebook friend\",\n",
    "    \"fml\": \"family\",\n",
    "    \"fn\": \"fine\",\n",
    "    \"fo\": \"freaking out\",\n",
    "    \"fri\": \"friday\",\n",
    "    \"frnd\": \"friend\",\n",
    "    \"fu\": \"fuck you\",\n",
    "    \"fugly\": \"fucking ugly\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"g1\": \"good one\",\n",
    "    \"g2b\": \"going to bed\",\n",
    "    \"g2cu\": \"good to see you\",\n",
    "    \"g2g\": \"good to go\",\n",
    "    \"g4i\": \"go for it\",\n",
    "    \"g4n\": \"good for nothing\",\n",
    "    \"g4u\": \"good for you\",\n",
    "    \"g9\": \"goodnight\",\n",
    "    \"ga\": \"go ahead\",\n",
    "    \"ge\": \"good evening\",\n",
    "    \"gl\": \"good luck\",\n",
    "    \"gm\": \"good morning\",\n",
    "    \"gn\": \"goodnight\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"goon\": \"idiot\",\n",
    "    \"gorge\": \"gorgeous\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"grats\": \"congratulations\",\n",
    "    \"gratz\": \"congratulations\",\n",
    "    \"grl\": \"girl\",\n",
    "    \"gt2t\": \"got time to talk\",\n",
    "    \"gtg\": \"good to go\",\n",
    "    \"gud\": \"good\",\n",
    "    \"gv\": \"give\",\n",
    "    \"gvn\": \"given\",\n",
    "    \"gw\": \"good work\",\n",
    "    \"h/o\": \"hold on\",\n",
    "    \"h/p\": \"hold please\",\n",
    "    \"h/t\": \"hat tip\",\n",
    "    \"h/u\": \"hook up\",\n",
    "    \"h2cus\": \"hope to see you soon\",\n",
    "    \"h4u\": \"hot for you\",\n",
    "    \"h4x0r\": \"hacker\",\n",
    "    \"h4x0rz\": \"hackers\",\n",
    "    \"h8\": \"hate\",\n",
    "    \"h8r\": \"hater\",\n",
    "    \"h8t\": \"hate\",\n",
    "    \"ha\": \"hello again\",\n",
    "    \"haha\": \"laughing\",\n",
    "    \"hai\": \"hi\",\n",
    "    \"hak\": \"hugs and kisses\",\n",
    "    \"han\": \"how about now?\",\n",
    "    \"hav\": \"have\",\n",
    "    \"hax\": \"hacks\",\n",
    "    \"haxor\": \"hacker\",\n",
    "    \"hay\": \"how are you\",\n",
    "    \"hb2u\": \"happy birthday to you\",\n",
    "    \"hbbd\": \"happy belated birthday\",\n",
    "    \"hbd\": \"happy birthday\",\n",
    "    \"hc\": \"how cool\",\n",
    "    \"hcit\": \"how cool is that\",\n",
    "    \"hehe\": \"laughing\",\n",
    "    \"hf\": \"have fun\",\n",
    "    \"hi5\": \"high five\",\n",
    "    \"hig\": \"how's it going?\",\n",
    "    \"hih\": \"hope it helps\",\n",
    "    \"ho\": \"hold on\",\n",
    "    \"hoc\": \"house of cards\",\n",
    "    \"hof\": \"hall of fame\",\n",
    "    \"holla\": \"holler\",\n",
    "    \"hom\": \"hit or miss\",\n",
    "    \"hood\": \"neighborhood\",\n",
    "    \"hoops\": \"basketball\",\n",
    "    \"hottie\": \"attractive person\",\n",
    "    \"hr\": \"human resources\",\n",
    "    \"hru\": \"how are you\",\n",
    "    \"hry\": \"hurry\",\n",
    "    \"hubby\": \"husband\",\n",
    "    \"hwk\": \"homework\",\n",
    "    \"hwp\": \"height weight proportionate\",\n",
    "    \"hwu\": \"hey, what's up?\",\n",
    "    \"hxc\": \"hardcore\",\n",
    "    \"h^\": \"hook up\",\n",
    "    \"i8\": \"i ate\",\n",
    "    \"i8u\": \"i hate you\",\n",
    "    \"ia\": \"i agree\",\n",
    "    \"iab\": \"in a bit\",\n",
    "    \"iac\": \"in any case\",\n",
    "    \"iad\": \"it all depends\",\n",
    "    \"iae\": \"in any event\",\n",
    "    \"iag\": \"it's all good\",\n",
    "    \"iagw\": \"in a good way\",\n",
    "    \"iail\": \"i am in love\",\n",
    "    \"iam\": \"in a minute\",\n",
    "    \"ic\": \"i see\",\n",
    "    \"id10t\": \"idiot\",\n",
    "    \"idc\": \"i don't care\",\n",
    "    \"idd\": \"indeed\",\n",
    "    \"idi\": \"i doubt it\",\n",
    "    \"idk\": \"i don't know\",\n",
    "    \"idky\": \"i don't know why\",\n",
    "    \"idmb\": \"i'll do my best\",\n",
    "    \"idn\": \"i don't know\",\n",
    "    \"idnk\": \"i do not know\",\n",
    "    \"idr\": \"i don't remember\",\n",
    "    \"idt\": \"i don't think\",\n",
    "    \"idts\": \"i don't think so\",\n",
    "    \"idtt\": \"i'll drink to that\",\n",
    "    \"idu\": \"i don't understand\",\n",
    "    \"ie\": \"that is\",\n",
    "    \"ig2p\": \"i got to pee\",\n",
    "    \"iggy\": \"ignored\",\n",
    "    \"ight\": \"alright\",\n",
    "    \"igi\": \"i get it\",\n",
    "    \"ign\": \"in-game name\",\n",
    "    \"igtp\": \"i get the point\",\n",
    "    \"ih8u\": \"i hate you\",\n",
    "    \"ihu\": \"i hate you\",\n",
    "    \"ihy\": \"i hate you\",\n",
    "    \"ii\": \"i'm impressed\",\n",
    "    \"iiok\": \"if i only knew\",\n",
    "    \"iir\": \"if i remember\",\n",
    "    \"iirc\": \"if i remember correctly\",\n",
    "    \"iit\": \"i'm impressed too\",\n",
    "    \"iiuc\": \"if i understand correctly\",\n",
    "    \"ik\": \"i know\",\n",
    "    \"ikhyf\": \"i know how you feel\",\n",
    "    \"ikr\": \"i know, right?\",\n",
    "    \"ikwum\": \"i know what you mean\",\n",
    "    \"ikwym\": \"i know what you mean\",\n",
    "    \"ikyd\": \"i know you did\",\n",
    "    \"ilu\": \"i like you\",\n",
    "    \"ilu2\": \"i love you too\",\n",
    "    \"ilub\": \"i love you baby\",\n",
    "    \"ilyk\": \"i'll let you know\",\n",
    "    \"ilyl\": \"i love you lots\",\n",
    "    \"ilysm\": \"i love you so much\",\n",
    "    \"ima\": \"i'm\",\n",
    "    \"imma\": \"i'm gonna\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imy\": \"i miss you\",\n",
    "    \"inb4\": \"in before\",\n",
    "    \"inc\": \"incoming\",\n",
    "    \"indie\": \"independent\",\n",
    "    \"info\": \"information\",\n",
    "    \"init\": \"initialize\",\n",
    "    \"ipo\": \"initial public offering\",\n",
    "    \"ir\": \"in room\",\n",
    "    \"ir8\": \"irate\",\n",
    "    \"irdk\": \"i really don't know\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"iyo\": \"in your opinion\",\n",
    "    \"iyq\": \"i like you\",\n",
    "    \"j/k\": \"just kidding\",\n",
    "    \"j/p\": \"just playing\",\n",
    "    \"j/w\": \"just wondering\",\n",
    "    \"j2lyk\": \"just to let you know\",\n",
    "    \"j4f\": \"just for fun\",\n",
    "    \"j4g\": \"just for grins\",\n",
    "    \"jas\": \"just a second\",\n",
    "    \"jb/c\": \"just because\",\n",
    "    \"joshing\": \"joking\",\n",
    "    \"k\": \"ok\",\n",
    "    \"k3u\": \"i love you\",\n",
    "    \"kappa\": \"sarcasm\",\n",
    "    \"kek\": \"korean laugh\",\n",
    "    \"keke\": \"korean laugh\",\n",
    "    \"kewl\": \"cool\",\n",
    "    \"kewt\": \"cute\",\n",
    "    \"kfc\": \"kentucky fried chicken\",\n",
    "    \"kgo\": \"ok, go\",\n",
    "    \"kik\": \"laughing out loud\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"kk\": \"ok\",\n",
    "    \"kl\": \"kool\",\n",
    "    \"km\": \"kiss me\",\n",
    "    \"kma\": \"kiss my ass\",\n",
    "    \"knp\": \"ok, no problem\",\n",
    "    \"kw\": \"know\",\n",
    "    \"kwl\": \"cool\",\n",
    "    \"l2m\": \"listening to music\",\n",
    "    \"l2p\": \"learn to play\",\n",
    "    \"l33t\": \"leet\",\n",
    "    \"l8\": \"late\",\n",
    "    \"l8er\": \"later\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"la\": \"laughing a lot\",\n",
    "    \"laf\": \"laugh\",\n",
    "    \"laffing\": \"laughing\",\n",
    "    \"lafs\": \"love at first sight\",\n",
    "    \"lam\": \"leave a message\",\n",
    "    \"lamer\": \"lame person\",\n",
    "    \"legit\": \"legitimate\",\n",
    "    \"lemeno\": \"let me know\",\n",
    "    \"lil\": \"little\",\n",
    "    \"lk\": \"like\",\n",
    "    \"llol\": \"literally laughing out loud\",\n",
    "    \"lmho\": \"laughing my head off\",\n",
    "    \"loi\": \"laughing on the inside\",\n",
    "    \"lola\": \"love often, laugh a lot\",\n",
    "    \"lolol\": \"lots of laugh out louds\",\n",
    "    \"lolz\": \"laugh out louds\",\n",
    "    \"ltr\": \"later\",\n",
    "    \"lulz\": \"lol\",\n",
    "    \"luv\": \"love\",\n",
    "    \"luzr\": \"loser\",\n",
    "    \"lv\": \"love\",\n",
    "    \"ly\": \"love ya\",\n",
    "    \"lya\": \"love you always\",\n",
    "    \"lyk\": \"let you know\",\n",
    "    \"lyn\": \"lying\",\n",
    "    \"lysm\": \"love you so much\",\n",
    "    \"m\": \"male\",\n",
    "    \"mcd\": \"mcdonald's\",\n",
    "    \"mcds\": \"mcdonald's\",\n",
    "    \"md@u\": \"mad at you\",\n",
    "    \"me2\": \"me too\",\n",
    "    \"meh\": \"whatever\",\n",
    "    \"mf\": \"mother fucker\",\n",
    "    \"mfb\": \"mother fucking bitch\",\n",
    "    \"mgmt\": \"management\",\n",
    "    \"mid\": \"middle\",\n",
    "    \"mil\": \"mother-in-law\",\n",
    "    \"min\": \"minute\",\n",
    "    \"mins\": \"minutes\",\n",
    "    \"mk\": \"okay\",\n",
    "    \"mkay\": \"ok\",\n",
    "    \"mmk\": \"ok\",\n",
    "    \"mms\": \"multimedia messaging service\",\n",
    "    \"mng\": \"manage\",\n",
    "    \"mngr\": \"manager\",\n",
    "    \"mod\": \"modification\",\n",
    "    \"mofo\": \"mother fucking\",\n",
    "    \"mojo\": \"attractive talent\",\n",
    "    \"moss\": \"chill\",\n",
    "    \"ms\": \"miss\",\n",
    "    \"msg\": \"message\",\n",
    "    \"mtg\": \"meeting\",\n",
    "    \"mth\": \"month\",\n",
    "    \"mu\": \"miss you\",\n",
    "    \"mu@\": \"meet you at\",\n",
    "    \"muah\": \"kiss\",\n",
    "    \"mula\": \"money\",\n",
    "    \"mwa\": \"kiss\",\n",
    "    \"mwah\": \"kiss\",\n",
    "    \"n/m\": \"nevermind\",\n",
    "    \"n/m/h\": \"nothing much here\",\n",
    "    \"n/r\": \"no reserve\",\n",
    "    \"n00b\": \"newbie\",\n",
    "    \"n1\": \"nice one\",\n",
    "    \"n1c\": \"no one cares\",\n",
    "    \"n2m\": \"not too much\",\n",
    "    \"n2mh\": \"not too much here\",\n",
    "    \"n2w\": \"not to worry\",\n",
    "    \"n64\": \"nintendo 64\",\n",
    "    \"n8kd\": \"naked\",\n",
    "    \"nac\": \"not a chance\",\n",
    "    \"nah\": \"no\",\n",
    "    \"nal\": \"nationality\",\n",
    "    \"narc\": \"tattle tale\",\n",
    "    \"nark\": \"informant\",\n",
    "    \"naw\": \"no\",\n",
    "    \"nb\": \"not bad\",\n",
    "    \"nbd\": \"no big deal\",\n",
    "    \"nbjf\": \"no brag, just fact\",\n",
    "    \"nd\": \"and\",\n",
    "    \"ne\": \"any\",\n",
    "    \"ne1\": \"anyone\",\n",
    "    \"ne1er\": \"anyone here\",\n",
    "    \"neh\": \"no\",\n",
    "    \"nemore\": \"anymore\",\n",
    "    \"neva\": \"never\",\n",
    "    \"neway\": \"anyway\",\n",
    "    \"newaze\": \"anyways\",\n",
    "    \"newb\": \"newbie\",\n",
    "    \"nite\": \"night\",\n",
    "    \"nn2r\": \"no need to reply\",\n",
    "    \"nnito\": \"not necessarily in that order\",\n",
    "    \"nnto\": \"no need to open\",\n",
    "    \"nntr\": \"no need to reply\",\n",
    "    \"no1\": \"no one\",\n",
    "    \"noob\": \"newbie\",\n",
    "    \"nooblet\": \"young newbie\",\n",
    "    \"nooblord\": \"ultimate newbie\",\n",
    "    \"notch\": \"minecraft creator\",\n",
    "    \"nottie\": \"unattractive person\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"nub\": \"newbie\",\n",
    "    \"nuff\": \"enough\",\n",
    "    \"nufn\": \"nothing\",\n",
    "    \"num\": \"tasty\",\n",
    "    \"nvm\": \"nevermind\",\n",
    "    \"nvr\": \"never\",\n",
    "    \"nvrm\": \"nevermind\",\n",
    "    \"nw\": \"no way\",\n",
    "    \"nxt\": \"next\",\n",
    "    \"o4u\": \"only for you\",\n",
    "    \"obtw\": \"oh, by the way\",\n",
    "    \"obv\": \"obviously\",\n",
    "    \"obvi\": \"obviously\",\n",
    "    \"oc\": \"of course\",\n",
    "    \"ohemgee\": \"oh my gosh\",\n",
    "    \"oic\": \"oh, i see\",\n",
    "    \"oicn\": \"oh, i see now\",\n",
    "    \"oiy\": \"hi\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"onl\": \"online\",\n",
    "    \"onoz\": \"oh no\",\n",
    "    \"orly\": \"oh really\",\n",
    "    \"otay\": \"okay\",\n",
    "    \"otw\": \"on the way\",\n",
    "    \"outta\": \"out of\",\n",
    "    \"ovie\": \"overlord\",\n",
    "    \"ownage\": \"completely owned\",\n",
    "    \"p/d\": \"per day\",\n",
    "    \"p/m\": \"per month\",\n",
    "    \"p/y\": \"per year\",\n",
    "    \"p911\": \"parent alert!\",\n",
    "    \"p@h\": \"parents at home\",\n",
    "    \"pc\": \"personal computer\",\n",
    "    \"pda\": \"public display of affection\",\n",
    "    \"pic\": \"picture\",\n",
    "    \"pj\": \"poor joke\",\n",
    "    \"pl8\": \"plate\",\n",
    "    \"pld\": \"played\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"plzrd\": \"please read\",\n",
    "    \"pov\": \"point of view\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"ppp\": \"peace\",\n",
    "    \"prof\": \"professor\",\n",
    "    \"prolly\": \"probably\",\n",
    "    \"promo\": \"promotion\",\n",
    "    \"props\": \"recognition\",\n",
    "    \"prot\": \"protection\",\n",
    "    \"prvt\": \"private\",\n",
    "    \"ps\": \"postscript\",\n",
    "    \"ps2\": \"playstation 2\",\n",
    "    \"ps3\": \"playstation 3\",\n",
    "    \"psa\": \"public service announcement\",\n",
    "    \"psog\": \"pure stroke of genius\",\n",
    "    \"psp\": \"playstation portable\",\n",
    "    \"ptm\": \"please tell me\",\n",
    "    \"pwd\": \"password\",\n",
    "    \"psd\": \"password\",\n",
    "    \"pswd\": \"password\",\n",
    "    \"pwnd\": \"owned\",\n",
    "    \"pwned\": \"owned\",\n",
    "    \"pwnt\": \"owned\",\n",
    "    \"q4u\": \"question for you\",\n",
    "    \"qfe\": \"quoted for emphasis\",\n",
    "    \"qft\": \"quoted for truth\",\n",
    "    \"qq\": \"quick question\",\n",
    "    \"qqn\": \"looking\",\n",
    "    \"qrg\": \"quick reference guide\",\n",
    "    \"qt\": \"cutie\",\n",
    "    \"qtpi\": \"cutie pie\",\n",
    "    \"r\": \"are\",\n",
    "    \"r8\": \"rate\",\n",
    "    \"rdy\": \"ready\",\n",
    "    \"re\": \"replay\",\n",
    "    \"rehi\": \"hi again\",\n",
    "    \"rents\": \"parents\",\n",
    "    \"rep\": \"reputation\",\n",
    "    \"resq\": \"rescue\",\n",
    "    \"rgd\": \"regard\",\n",
    "    \"rgds\": \"regards\",\n",
    "    \"ridic\": \"ridiculous\",\n",
    "    \"rip\": \"rest in peace\",\n",
    "    \"rl\": \"real life\",\n",
    "    \"rlrt\": \"real life retweet\",\n",
    "    \"rly\": \"really\",\n",
    "    \"rm\": \"room\",\n",
    "    \"rn\": \"run\",\n",
    "    \"rnt\": \"aren't\",\n",
    "    \"rof\": \"laughing\",\n",
    "    \"rofl\": \"laughing\",\n",
    "    \"roflmao\": \"laughing\",\n",
    "    \"roflol\": \"laughing out loud\",\n",
    "    \"rolf\": \"laughing\",\n",
    "    \"ru\": \"are you\",\n",
    "    \"ruc\": \"are you coming?\",\n",
    "    \"rut\": \"are you there?\",\n",
    "    \"rx\": \"prescription\",\n",
    "    \"s/o\": \"sold out\",\n",
    "    \"s/u\": \"shut up\",\n",
    "    \"s/w\": \"software\",\n",
    "    \"s2r\": \"send to receive\",\n",
    "    \"s2s\": \"sorry to say\",\n",
    "    \"s2u\": \"same to you\",\n",
    "    \"samzd\": \"still amazed\",\n",
    "    \"sd\": \"sweet dreams\",\n",
    "    \"sec\": \"second\",\n",
    "    \"sho\": \"sure\",\n",
    "    \"sh^\": \"shut up\",\n",
    "    \"siul8r\": \"see you later\",\n",
    "    \"siv\": \"bad goaltender\",\n",
    "    \"sk8\": \"skate\",\n",
    "    \"sk8r\": \"skater\",\n",
    "    \"sly\": \"still love you\",\n",
    "    \"smf\": \"so much fun\",\n",
    "    \"smooch\": \"kiss\",\n",
    "    \"sorta\": \"sort of\",\n",
    "    \"spec\": \"specialization\",\n",
    "    \"spk\": \"speak\",\n",
    "    \"spkr\": \"speaker\",\n",
    "    \"srry\": \"sorry\",\n",
    "    \"srs\": \"serious\",\n",
    "    \"srsly\": \"seriously\",\n",
    "    \"sry\": \"sorry\",\n",
    "    \"stpd\": \"stupid\",\n",
    "    \"str\": \"strength\",\n",
    "    \"str8\": \"straight\",\n",
    "    \"sup\": \"what's up\",\n",
    "    \"syl\": \"see you later\",\n",
    "    \"sync\": \"synchronize\",\n",
    "    \"t2go\": \"time to go\",\n",
    "    \"t2m\": \"talk to me\",\n",
    "    \"t2u\": \"talk to you\",\n",
    "    \"t2ul\": \"talk to you later\",\n",
    "    \"t2ul8er\": \"talk to you later\",\n",
    "    \"t2ul8r\": \"talk to you later\",\n",
    "    \"t4lmk\": \"thanks for letting me know\",\n",
    "    \"t4p\": \"thanks for posting\",\n",
    "    \"t4t\": \"thanks for trade\",\n",
    "    \"tc\": \"take care\",\n",
    "    \"teh\": \"the\",\n",
    "    \"teme\": \"tell me\",\n",
    "    \"tg\": \"thank goodness\",\n",
    "    \"thnq\": \"thank you\",\n",
    "    \"tho\": \"though\",\n",
    "    \"thru\": \"through\",\n",
    "    \"tht\": \"that\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"tl\": \"tell\",\n",
    "    \"tlk\": \"talk\",\n",
    "    \"tlkin\": \"talking\",\n",
    "    \"tlking\": \"talking\",\n",
    "    \"tomoz\": \"tomorrow\",\n",
    "    \"tq\": \"thank you\",\n",
    "    \"tqvm\": \"thank you very much\",\n",
    "    \"tru\": \"true\",\n",
    "    \"ttl\": \"talk to you later\",\n",
    "    \"ttly\": \"totally\",\n",
    "    \"ttul\": \"talk to you later\",\n",
    "    \"tty\": \"talk to you\",\n",
    "    \"tu\": \"thank you\",\n",
    "    \"tude\": \"attitude\",\n",
    "    \"tx\": \"thanks\",\n",
    "    \"txt\": \"text\",\n",
    "    \"txtin\": \"texting\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"tyfa\": \"thank you for asking\",\n",
    "    \"tyl\": \"thank you lord\",\n",
    "    \"tym\": \"thank you much\",\n",
    "    \"tyt\": \"take your time\",\n",
    "    \"tyvm\": \"thank you very much\",\n",
    "    \"u\": \"you\",\n",
    "    \"u-ok\": \"you ok?\",\n",
    "    \"u/l\": \"upload\",\n",
    "    \"u2\": \"you too\",\n",
    "    \"u2u\": \"up to you\",\n",
    "    \"uok\": \"you ok?\",\n",
    "    \"ur\": \"your\",\n",
    "    \"ut\": \"you there?\",\n",
    "    \"veggies\": \"vegetables\",\n",
    "    \"vry\": \"very\",\n",
    "    \"vs\": \"versus\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/b\": \"welcome back\",\n",
    "    \"w/e\": \"whatever\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"w2f\": \"way too funny\",\n",
    "    \"w2g\": \"way to go\",\n",
    "    \"w2k\": \"windows 2000\",\n",
    "    \"w4u\": \"wait for you\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"w84m\": \"wait for me\",\n",
    "    \"w8am\": \"wait a minute\",\n",
    "    \"w8ing\": \"waiting\",\n",
    "    \"w8n\": \"waiting\",\n",
    "    \"wa\": \"what\",\n",
    "    \"waa\": \"crying\",\n",
    "    \"wack\": \"strange\",\n",
    "    \"wan2\": \"want to\",\n",
    "    \"wannabe\": \"want to be\",\n",
    "    \"wat\": \"what\",\n",
    "    \"watev\": \"whatever\",\n",
    "    \"watevs\": \"whatever\",\n",
    "    \"wlcm\": \"welcome\",\n",
    "    \"wha\": \"what\",\n",
    "    \"whipped\": \"tired\",\n",
    "    \"wht\": \"what\",\n",
    "    \"wk\": \"week\",\n",
    "    \"wknd\": \"weekend\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"wtg\": \"way to go\",\n",
    "    \"wup\": \"what's up?\",\n",
    "    \"ya\": \"yes\",\n",
    "    \"yeap\": \"yes\",\n",
    "    \"yep\": \"yes\",\n",
    "    \"yepperz\": \"yes\",\n",
    "    \"yesh\": \"yes\",\n",
    "    \"yo\": \"hi\",\n",
    "    \"yr\": \"your\",\n",
    "    \"yrs\": \"years\",\n",
    "    \"yt\": \"you there?\",\n",
    "    \"yt?\": \"you there?\",\n",
    "    \"yup\": \"yes\",\n",
    "    \"yupz\": \"ok\",\n",
    "    \"zzz\": \"sleeping\",\n",
    "    }\n",
    "    word = word.lower()\n",
    "    word = word.split()\n",
    "    for i in range(len(word)):\n",
    "        word[i] = slangs_dict.get(word[i], word[i])\n",
    "    word = \" \".join(word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def replace_word(word):\n",
    "    switcher = {\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn\": \"could not\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won\": \"will not\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn\": \"must not\",\n",
    "        \"that'll\": \"that will\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn\": \"should not\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"haven\": \"have not\",\n",
    "        \"hadn't\": \"have not\",\n",
    "        \"hadn\": \"have not\",\n",
    "        \"hasn't\": \"have not\",\n",
    "        \"hasn\": \"have not\",\n",
    "        \"didn't\": \"do not\",\n",
    "        \"didn\": \"do not\",\n",
    "        \"doesn't\": \"do not\",\n",
    "        \"doesn\": \"do not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"don\": \"do not\", \n",
    "        \"isn't\": \"be not\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"aren\":\"are not\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"weren\":\"were not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"wouldn\":\"would not\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"needn\":\"need not\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"wasn\":\"was not\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"mightn\":\"might not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"shan\":\"shall not\", \n",
    "        \"can't\":\"can not\",\n",
    "        \"i'm\":\"i am\",\n",
    "        \"i'd\":\"i would\",\n",
    "        \"i'll\":\"i will\",\n",
    "        \"i've\":\"i have\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"here's\":\"here is\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"0\":\"zero\",\n",
    "        \"1\":\"one\",\n",
    "        \"2\":\"two\",\n",
    "        \"3\":\"three\",\n",
    "        \"4\":\"four\",\n",
    "        \"5\":\"five\",\n",
    "        \"6\":\"six\",\n",
    "        \"7\":\"oseven\",\n",
    "        \"8\":\"eight\",\n",
    "        \"9\":\"nine\",\n",
    "        \"10\":\"ten\",\n",
    "    }\n",
    "    word = word.lower()\n",
    "    word = word.split()\n",
    "    for i in range(len(word)):\n",
    "        word[i] = switcher.get(word[i], word[i])\n",
    "    word = \" \".join(word)\n",
    "    return word\n",
    "\n",
    "def transformText(text):\n",
    "    text = split_alphanum(text)\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    text = replace_word(text)\n",
    "    text = normaliser_word(text)\n",
    "    #stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    # Removing all the stopwords\n",
    "    filtered_words = [word for word in text.split() if word not in STOP_WORDS]\n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    # Correct words\n",
    "    spell = SpellChecker()\n",
    "    misspelled = text.split()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(misspelled)):\n",
    "        # Get the one `most likely` answer\n",
    "        word = spell.correction(misspelled[i])\n",
    "        misspelled[i]=word\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"v\")\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"n\")\n",
    "    text = \" \".join(misspelled)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'service go group'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformText(\"I have so 2 servic going groop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(transformText)\n",
    "texts= df['text']\n",
    "tags= df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'(0       have order data cable get well finish work pro...\n1                                              love phone\n2                                 get well finish product\n3                                          not be happier\n4                  be look headset long time now have get\n5                            headset start ring no reason\n6                display be excellent camera be good year\n7                              battery life be also great\n8               worst phone have ever have have few month\n9       not good item work start have problem auto rev...\n10       be embarrass also ear hurt try push ear plug ear\n11                                     protect phone side\n12      be average phone bad battery life operate weak...\n13           clear skye call long battery life long range\n14                                   solo technology suck\n15                                 great hand free device\n16      can even take self portrait outside exterior d...\n17                       same problem others have mention\n18      try many many handsfree gadget be one finally ...\n19                                           magical help\n20      worst piece of crap ever along version custome...\n21                                     poor sound quality\n22                                      best phone market\n23                                              work well\n24                company ship product promptly work well\n25                                           exactly want\n26      picture resolution be far comparably price pho...\n27                                          be great deal\n28                     excellent product satisfy purchase\n29             highly recommend encourage people give try\n                              ...                        \n4025                  flair bartender be absolutely amaze\n4026                 freeze margarita be way sugary taste\n4027                                  be good order twice\n4028    nutshell 1 restaurant smell like combination o...\n4029                            girlfriend be veal be bad\n4030                            unfortunately be not good\n4031                       have pretty satisfy experience\n4032                join club get awesome offer via email\n4033    perfect someone like beer ice cold case even c...\n4034    bland flavourless be good way of describe bare...\n4035                    chain no fan of beat place easily\n4036                                        nacho be have\n4037                                     not be come back\n4038    do nothave many word say place do everything p...\n4039    staff be super nice quick even crazy crowd of ...\n4040               great atmosphere friendly fast service\n4041       receive pity be huge do have lot of meat thumb\n4042                                       food arrive be\n4043    pay 7 85 hot dog fry look like come of kid be ...\n4044              classic maine lobster roll be fantastic\n4045    brother law work mall eat same day guess be si...\n4046                      good go have review place twice\n4047             chip salsa be really good salsa be fresh\n4048                                       place be great\n4049                                        mediocre food\n4050                          get inside be impress place\n4051                            service be super friendly\n4052                     be sad little vegetable overcook\n4053                               place be nice surprise\n4054                              live music totally blow\nName: text, Length: 4055, dtype: object, 0       NOTISSUE\n1       NOTISSUE\n2       NOTISSUE\n3       NOTISSUE\n4       NOTISSUE\n5          ISSUE\n6       NOTISSUE\n7       NOTISSUE\n8          ISSUE\n9          ISSUE\n10         ISSUE\n11      NOTISSUE\n12         ISSUE\n13      NOTISSUE\n14         ISSUE\n15      NOTISSUE\n16      NOTISSUE\n17         ISSUE\n18      NOTISSUE\n19      NOTISSUE\n20         ISSUE\n21         ISSUE\n22      NOTISSUE\n23      NOTISSUE\n24      NOTISSUE\n25      NOTISSUE\n26         ISSUE\n27      NOTISSUE\n28      NOTISSUE\n29      NOTISSUE\n          ...   \n4025    NOTISSUE\n4026       ISSUE\n4027    NOTISSUE\n4028       ISSUE\n4029       ISSUE\n4030       ISSUE\n4031    NOTISSUE\n4032    NOTISSUE\n4033    NOTISSUE\n4034       ISSUE\n4035       ISSUE\n4036    NOTISSUE\n4037       ISSUE\n4038    NOTISSUE\n4039    NOTISSUE\n4040    NOTISSUE\n4041    NOTISSUE\n4042       ISSUE\n4043       ISSUE\n4044    NOTISSUE\n4045       ISSUE\n4046    NOTISSUE\n4047    NOTISSUE\n4048    NOTISSUE\n4049       ISSUE\n4050    NOTISSUE\n4051    NOTISSUE\n4052       ISSUE\n4053    NOTISSUE\n4054    NOTISSUE\nName: label, Length: 4055, dtype: object)' is an invalid key",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8a09c3838e60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2655\u001b[0m                                  'backfill or nearest lookups')\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(0       have order data cable get well finish work pro...\n1                                              love phone\n2                                 get well finish product\n3                                          not be happier\n4                  be look headset long time now have get\n5                            headset start ring no reason\n6                display be excellent camera be good year\n7                              battery life be also great\n8               worst phone have ever have have few month\n9       not good item work start have problem auto rev...\n10       be embarrass also ear hurt try push ear plug ear\n11                                     protect phone side\n12      be average phone bad battery life operate weak...\n13           clear skye call long battery life long range\n14                                   solo technology suck\n15                                 great hand free device\n16      can even take self portrait outside exterior d...\n17                       same problem others have mention\n18      try many many handsfree gadget be one finally ...\n19                                           magical help\n20      worst piece of crap ever along version custome...\n21                                     poor sound quality\n22                                      best phone market\n23                                              work well\n24                company ship product promptly work well\n25                                           exactly want\n26      picture resolution be far comparably price pho...\n27                                          be great deal\n28                     excellent product satisfy purchase\n29             highly recommend encourage people give try\n                              ...                        \n4025                  flair bartender be absolutely amaze\n4026                 freeze margarita be way sugary taste\n4027                                  be good order twice\n4028    nutshell 1 restaurant smell like combination o...\n4029                            girlfriend be veal be bad\n4030                            unfortunately be not good\n4031                       have pretty satisfy experience\n4032                join club get awesome offer via email\n4033    perfect someone like beer ice cold case even c...\n4034    bland flavourless be good way of describe bare...\n4035                    chain no fan of beat place easily\n4036                                        nacho be have\n4037                                     not be come back\n4038    do nothave many word say place do everything p...\n4039    staff be super nice quick even crazy crowd of ...\n4040               great atmosphere friendly fast service\n4041       receive pity be huge do have lot of meat thumb\n4042                                       food arrive be\n4043    pay 7 85 hot dog fry look like come of kid be ...\n4044              classic maine lobster roll be fantastic\n4045    brother law work mall eat same day guess be si...\n4046                      good go have review place twice\n4047             chip salsa be really good salsa be fresh\n4048                                       place be great\n4049                                        mediocre food\n4050                          get inside be impress place\n4051                            service be super friendly\n4052                     be sad little vegetable overcook\n4053                               place be nice surprise\n4054                              live music totally blow\nName: text, Length: 4055, dtype: object, 0       NOTISSUE\n1       NOTISSUE\n2       NOTISSUE\n3       NOTISSUE\n4       NOTISSUE\n5          ISSUE\n6       NOTISSUE\n7       NOTISSUE\n8          ISSUE\n9          ISSUE\n10         ISSUE\n11      NOTISSUE\n12         ISSUE\n13      NOTISSUE\n14         ISSUE\n15      NOTISSUE\n16      NOTISSUE\n17         ISSUE\n18      NOTISSUE\n19      NOTISSUE\n20         ISSUE\n21         ISSUE\n22      NOTISSUE\n23      NOTISSUE\n24      NOTISSUE\n25      NOTISSUE\n26         ISSUE\n27      NOTISSUE\n28      NOTISSUE\n29      NOTISSUE\n          ...   \n4025    NOTISSUE\n4026       ISSUE\n4027    NOTISSUE\n4028       ISSUE\n4029       ISSUE\n4030       ISSUE\n4031    NOTISSUE\n4032    NOTISSUE\n4033    NOTISSUE\n4034       ISSUE\n4035       ISSUE\n4036    NOTISSUE\n4037       ISSUE\n4038    NOTISSUE\n4039    NOTISSUE\n4040    NOTISSUE\n4041    NOTISSUE\n4042       ISSUE\n4043       ISSUE\n4044    NOTISSUE\n4045       ISSUE\n4046    NOTISSUE\n4047    NOTISSUE\n4048    NOTISSUE\n4049       ISSUE\n4050    NOTISSUE\n4051    NOTISSUE\n4052       ISSUE\n4053    NOTISSUE\n4054    NOTISSUE\nName: label, Length: 4055, dtype: object)' is an invalid key"
     ]
    }
   ],
   "source": [
    "df=df[texts,tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of lists  \n",
    "dict = {'text': texts, 'label': tags } \n",
    "     \n",
    "df = pd.DataFrame(dict) \n",
    "  \n",
    "# saving the dataframe \n",
    "df.to_csv('DATA2_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'DATA2_preprocessing.csv'\n",
    "df = pd.read_csv(DATA_FILE,delimiter=',',encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df)) < 0.7 # Splitting into train(70%) and test(30%) randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=df[msk]\n",
    "test_df=df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2857, 3)\n",
      "(1198, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking target values for train data:\n",
      "\n",
      "ISSUE       1439\n",
      "NOTISSUE    1418\n",
      "Name: label, dtype: int64 \n",
      "\n",
      "Checking target values for test data:\n",
      "\n",
      "NOTISSUE    612\n",
      "ISSUE       586\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Checking target values for train data:\\n')\n",
    "print(train_df['label'].value_counts(),'\\n')\n",
    "print('Checking target values for test data:\\n')\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ISSUE       1439\n",
       "NOTISSUE    1418\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train_df['text']\n",
    "y_train=train_df['label']\n",
    "x_test=test_df['text']\n",
    "y_test=test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLOVE EMBEDDING 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "maxlen = 120  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_train = x_train.astype(str)\n",
    "texts_test = x_test.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(texts_train)\n",
    "sequences = tokenizer.texts_to_sequences(texts_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, \n",
    "                     padding=\"pre\", truncating=\"post\")\n",
    "test_data = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding=\"pre\",\n",
    "                          truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2857, 200)\n",
      "Shape of test_data tensor: (1198, 200)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_tok = 'LSTM_token_glove_300d_DATA_wit_text_processing.sav'\n",
    "pickle.dump(tokenizer, open(file_tok, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3170 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (2857, 200)\n",
      "Shape of data test tensor: (1198, 200)\n"
     ]
    }
   ],
   "source": [
    "#pad sequences are used to bring all sentences to same size.\n",
    "# pad sequences with 0s\n",
    "x_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of data test tensor:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map({\"NOTISSUE\": 1, \"ISSUE\" : 0 })\n",
    "y_test = y_test.map({\"NOTISSUE\": 1, \"ISSUE\" : 0 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = {}\n",
    "f = open('C:/Users/khmar/Desktop/GLOVE/glove.6B.300d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word not found : nothave\n",
      "word not found : notwaste\n",
      "word not found : notknow\n",
      "word not found : overprice\n",
      "word not found : notrecommend\n",
      "word not found : flavourful\n",
      "word not found : notbother\n",
      "word not found : notexpect\n",
      "word not found : notalways\n",
      "word not found : notunderstand\n",
      "word not found : notenjoy\n",
      "word not found : flavourless\n",
      "word not found : notwait\n",
      "word not found : notseem\n",
      "word not found : notsave\n",
      "word not found : appal\n",
      "word not found : notbelieve\n",
      "word not found : notwant\n",
      "word not found : nothelp\n",
      "word not found : notskimp\n",
      "word not found : shawarrrrrrma\n",
      "word not found : notdisappoint\n",
      "word not found : 99900\n",
      "word not found : barset\n",
      "word not found : plantronincs\n",
      "word not found : 3715\n",
      "word not found : notproduce\n",
      "word not found : flipphones\n",
      "word not found : notupload\n",
      "word not found : a325\n",
      "word not found : notslide\n",
      "word not found : tracfonewebsite\n",
      "word not found : 5320\n",
      "word not found : supertooth\n",
      "word not found : 3265\n",
      "word not found : 8125\n",
      "word not found : nottrust\n",
      "word not found : 8525\n",
      "word not found : notreceive\n",
      "word not found : notcharge\n",
      "word not found : s740\n",
      "word not found : notreceived\n",
      "word not found : bs2000\n",
      "word not found : notalready\n",
      "word not found : notclick\n",
      "word not found : soooooooo\n",
      "word not found : notquite\n",
      "word not found : notconvert\n",
      "word not found : notprocess\n",
      "word not found : notspent\n",
      "word not found : notlaugh\n",
      "word not found : notanything\n",
      "word not found : baaaaaad\n",
      "word not found : norber\n",
      "word not found : stylize\n",
      "word not found : nomish\n",
      "word not found : notregret\n",
      "word not found : notforget\n",
      "word not found : brainsucking\n",
      "word not found : nottaste\n",
      "word not found : notcomplain\n",
      "word not found : nothelpful\n",
      "word not found : infatuate\n",
      "word not found : riingtones\n",
      "word not found : nomole\n",
      "word not found : notchanged\n",
      "word not found : notgrip\n",
      "word not found : notdefective\n",
      "word not found : cheesecurds\n",
      "word not found : smashburger\n",
      "word not found : notclosed\n",
      "word not found : waaaaaayyyyyyyyyy\n",
      "word not found : douchebaggery\n",
      "word not found : chipolte\n",
      "word not found : notproperly\n",
      "word not found : albondigas\n",
      "word not found : untasted\n",
      "word not found : profiterole\n",
      "word not found : soooooo\n",
      "word not found : ciob\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "#Found 1489 unique tokens : word_index\n",
    "#print(len(word_index) + 1) #### 1490 \n",
    "#print(word_index.items()) ### unique tokens : words :word_index\n",
    "#print('embedding_matrix',embedding_matrix)\n",
    "out_of_vocab={}\n",
    "for word, i in word_index.items():\n",
    "    #print(i)\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    #print('embedding_vector' ,embedding_vector )\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        #print('word: ', word)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        #print('embedding_matrix :',embedding_matrix[i] )\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "    if embedding_vector is  None:\n",
    "        print('word not found :',word)\n",
    "        out_of_vocab[i] = word\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 80 out of vocab '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'There are {len(out_of_vocab)} out of vocab '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 76 out of vocab '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'There are {len(out_of_vocab)} out of vocab '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model_glove= Sequential()\n",
    "model_glove.add(Embedding(len(word_index) + 1,\n",
    "                            300,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False))\n",
    "model_glove.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2,input_shape=(1,)))\n",
    "model_glove.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\khmar\\ana\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2857 samples, validate on 1198 samples\n",
      "Epoch 1/2\n",
      "2857/2857 [==============================] - 30s 10ms/step - loss: 0.5412 - acc: 0.7347 - val_loss: 0.4575 - val_acc: 0.8030\n",
      "Epoch 2/2\n",
      "2857/2857 [==============================] - 23s 8ms/step - loss: 0.4143 - acc: 0.8299 - val_loss: 0.4377 - val_acc: 0.8005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x156f5b3eb38>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glove.fit(data, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=2,\n",
    "          validation_data=(test_data, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1189/1189 [==============================] - 3s 3ms/step\n",
      "loss: 40.89%\n",
      "acc: 81.41%\n"
     ]
    }
   ],
   "source": [
    "scores = model_glove.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print(\"%s: %.2f%%\" % (model_glove.metrics_names[0], scores[0] * 100))\n",
    "print(\"%s: %.2f%%\" % (model_glove.metrics_names[1], scores[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_glove.save('LSTM_model_glove_300_DATA_with_text_processing.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
