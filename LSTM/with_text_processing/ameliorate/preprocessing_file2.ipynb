{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  I had ordered a data cable, got a very well fi...  NOTISSUE\n",
      "1                                   Love This Phone.  NOTISSUE\n",
      "2                I get a very well finished product.  NOTISSUE\n",
      "3                             I couldn't be happier.  NOTISSUE\n",
      "4  I was looking for this headset for a long time...  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DATA_FILE = os.path.abspath('DATA/DATA_not_ameliorate.csv')\n",
    "df = pd.read_csv(DATA_FILE,delimiter=';',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOTISSUE    2028\n",
       "ISSUE       2027\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts() #balanced Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4055, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List stop words \n",
    "stop_words_list={\n",
    " 'a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'across',\n",
    " 'after',\n",
    " 'afterwards',\n",
    " 'against',\n",
    " 'all',\n",
    " 'almost',\n",
    " 'alone',\n",
    " 'along',\n",
    " 'already',\n",
    " 'also',\n",
    " 'although',\n",
    " 'always',\n",
    " #'am',\n",
    " 'among',\n",
    " 'amongst',\n",
    " 'amount',\n",
    " 'an',\n",
    " 'and',\n",
    " 'another',\n",
    " 'any',\n",
    " 'anyhow',\n",
    " 'anyone',\n",
    " 'anything',\n",
    " 'anyway',\n",
    " 'anywhere',\n",
    " 'are',\n",
    " 'around',\n",
    " 'as',\n",
    " 'at',\n",
    " #'became',\n",
    " 'because',\n",
    " #'become',\n",
    " #'becomes',\n",
    " #'becoming',\n",
    " #'been',\n",
    " 'before',\n",
    " 'beforehand',\n",
    " #'behind',\n",
    " #'being',\n",
    " 'below',\n",
    " 'beside',\n",
    " 'besides',\n",
    " 'between',\n",
    " 'beyond',\n",
    " 'both',\n",
    " 'bottom',\n",
    " 'but',\n",
    " 'by',\n",
    " 'ca',\n",
    " #'call',\n",
    " 'can',\n",
    " 'cannot',\n",
    " 'could',\n",
    " 'did',\n",
    " #'do',\n",
    " #'does',\n",
    " #'doing',\n",
    " 'done',\n",
    " 'down',\n",
    " 'due',\n",
    " 'during',\n",
    " 'each',\n",
    " 'either',\n",
    " 'eleven',\n",
    " 'else',\n",
    " 'elsewhere',\n",
    " 'empty',\n",
    " 'enough',\n",
    " 'even',\n",
    " 'ever',\n",
    " 'every',\n",
    " 'everyone',\n",
    " #'everything',\n",
    " 'everywhere',\n",
    " 'except',\n",
    " 'few',\n",
    " 'first',\n",
    " 'for',\n",
    " 'former',\n",
    " 'formerly',\n",
    " 'from',\n",
    " 'front',\n",
    " 'full',\n",
    " 'further',\n",
    " #'had',\n",
    " #'has',\n",
    " #'have',\n",
    " 'he',\n",
    " 'hence',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hereafter',\n",
    " 'hereby',\n",
    " 'herein',\n",
    " 'hereupon',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'however',\n",
    " 'hundred',\n",
    " 'i',\n",
    " 'if',\n",
    " 'in',\n",
    " 'indeed',\n",
    " 'into',\n",
    " #'is',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'just',\n",
    " 'last',\n",
    " 'latter',\n",
    " 'latterly',\n",
    " 'least',\n",
    " 'less',\n",
    " #'made',\n",
    " #'make',\n",
    " 'many',\n",
    " 'may',\n",
    " 'me',\n",
    " 'meanwhile',\n",
    " 'might',\n",
    " 'mine',\n",
    " #'more',\n",
    " 'moreover',\n",
    " 'most',\n",
    " 'mostly',\n",
    " 'move',\n",
    " 'much',\n",
    " 'must',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'name',\n",
    " 'namely',\n",
    " 'neither',\n",
    " 'nevertheless',\n",
    " 'next',\n",
    " 'noone',\n",
    " 'nor',\n",
    " 'now',\n",
    " 'nowhere',\n",
    " 'of',\n",
    " 'off',\n",
    " 'often',\n",
    " 'on',\n",
    " 'once',\n",
    " 'only',\n",
    " 'onto',\n",
    " 'or',\n",
    " 'other',\n",
    " 'others',\n",
    " 'otherwise',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 'part',\n",
    " 'per',\n",
    " 'perhaps',\n",
    " 'put',\n",
    " 'quite',\n",
    " 'rather',\n",
    " 're',\n",
    " 'really',\n",
    " 'regarding',\n",
    " 'same',\n",
    " #'say',\n",
    " #'see',\n",
    " #'seem',\n",
    " #'seemed',\n",
    " #'seeming',\n",
    " #'seems',\n",
    " 'serious',\n",
    " 'several',\n",
    " 'she',\n",
    " 'should',\n",
    " #'show',\n",
    " 'side',\n",
    " 'since',\n",
    " 'so',\n",
    " 'some',\n",
    " 'somehow',\n",
    " 'someone',\n",
    " 'something',\n",
    " 'sometime',\n",
    " 'sometimes',\n",
    " 'somewhere',\n",
    " 'still',\n",
    " 'such',\n",
    " #'take',\n",
    " 'than',\n",
    " 'that',\n",
    " 'the',\n",
    " 'their',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'thence',\n",
    " 'there',\n",
    " 'thereafter',\n",
    " 'thereby',\n",
    " 'therefore',\n",
    " 'therein',\n",
    " 'thereupon',\n",
    " 'these',\n",
    " 'they',\n",
    " 'third',\n",
    " 'this',\n",
    " 'those',\n",
    " 'though',\n",
    " 'three',\n",
    " 'through',\n",
    " 'throughout',\n",
    " 'thru',\n",
    " 'thus',\n",
    " 'to',\n",
    " 'together',\n",
    " 'too',\n",
    " 'top',\n",
    " #'toward',\n",
    " #'towards',\n",
    " 'twelve',\n",
    " 'under',\n",
    " 'unless',\n",
    " 'until',\n",
    " 'up',\n",
    " 'upon',\n",
    " 'us',\n",
    " #'used',\n",
    " 'using',\n",
    " 'various',\n",
    " 'very',\n",
    " 'via',\n",
    " #'was',\n",
    " 'we',\n",
    " 'were',\n",
    " 'what',\n",
    " 'whatever',\n",
    " 'when',\n",
    " 'whence',\n",
    " 'whenever',\n",
    " 'where',\n",
    " 'whereafter',\n",
    " 'whereas',\n",
    " 'whereby',\n",
    " 'wherein',\n",
    " 'whereupon',\n",
    " 'wherever',\n",
    " 'whether',\n",
    " 'which',\n",
    " 'while',\n",
    " 'whither',\n",
    " 'who',\n",
    " 'whoever',\n",
    " 'whole',\n",
    " 'whom',\n",
    " 'whose',\n",
    " 'why',\n",
    " 'will',\n",
    " 'with',\n",
    " 'within',\n",
    " 'without',\n",
    " 'would',\n",
    " 'yet',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "appos_dict = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"im\": \"I am\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"we'll\": \"will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"didnt\": \"did not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_dict= {\n",
    "    'awsm': 'awesome',\n",
    "    \"aamof\": \"as a matter of fact\",\n",
    "    \"abt\": \"about\",\n",
    "    \"abt2\": \"about to\",\n",
    "    \"ac\": \"air conditioning\",\n",
    "    \"ace\": \"solo winner\",\n",
    "    \"ack\": \"acknowledged\",\n",
    "    \"admin\": \"administrator\",\n",
    "    \"thr\": \"there\",\n",
    "    \"frm\": \"from\",\n",
    "    \"aggro\": \"aggression\",\n",
    "    \"agl\": \"angel\",\n",
    "    \"dob\": \"date of birth\",\n",
    "    \"ai\": \"artificial intelligence\",\n",
    "    \"aiic\": \"as if i care\",\n",
    "    \"aka\": \"also known as\",\n",
    "    \"alap\": \"as long as possible\",\n",
    "    \"alol\": \"actually laughing out loud\",\n",
    "    \"ama\": \"ask me anything\",\n",
    "    \"amap\": \"as much as possible\",\n",
    "    \"amazn\": \"amazing\",\n",
    "    \"ammo\": \"ammunition\",\n",
    "    \"ams\": \"ask me something\",\n",
    "    \"anon\": \"anonymous\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"asat\": \"as simple as that\",\n",
    "    \"awks\": \"awkward\",\n",
    "    \"awl\": \"always with love\",\n",
    "    \"ayk\": \"as you know\",\n",
    "    \"azm\": \"awesome\",\n",
    "    \"b\": \"be\",\n",
    "    \"b&w\": \"black and white\",\n",
    "    \"b-day\": \"birthday\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"bcoz\": \"because\",\n",
    "    \"bcos\": \"because\",\n",
    "    \"bcz\": \"because\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"btw\": \"between\",\n",
    "    \"b4\": \"before\",\n",
    "    \"bai\": \"bye\",\n",
    "    \"bb\": \"bye bye\",\n",
    "    \"bc\": \"abuse\",\n",
    "    \"mc\": \"abuse\",\n",
    "    \"bcc\": \"blind carbon copy\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"biz\": \"business\",\n",
    "    \"bk\": \"back\",\n",
    "    \"bo\": \"back off\",\n",
    "    \"bro\": \"brother\",\n",
    "    \"btwn\": \"between\",\n",
    "    \"c\": \"see\",\n",
    "    \"char\": \"character\",\n",
    "    \"combo\": \"combination\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cu2\": \"see you too\",\n",
    "    \"cu2mr\": \"see you tomorrow\",\n",
    "    \"cya\": \"see ya\",\n",
    "    \"cyal8r\": \"see you later\",\n",
    "    \"cyb\": \"call you back\",\n",
    "    \"cye\": \"check your e-mail\",\n",
    "    \"cyf\": \"check your facebook\",\n",
    "    \"cyfb\": \"check your facebook\",\n",
    "    \"cyl\": \"catch ya later\",\n",
    "    \"cym\": \"check your myspace\",\n",
    "    \"cyo\": \"see you online\",\n",
    "    \"d8\": \"date\",\n",
    "    \"da\": \"the\",\n",
    "    \"dece\": \"decent\",\n",
    "    \"ded\": \"dead\",\n",
    "    \"dept\": \"department\",\n",
    "    \"dis\": \"this\",\n",
    "    \"ditto\": \"same\",\n",
    "    \"diva\": \"rude woman\",\n",
    "    \"dk\": \"don't know\",\n",
    "    \"dlm\": \"don't leave me\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"dnd\": \"do not disturb\",\n",
    "    \"dno\": \"don't know\",\n",
    "    \"dnt\": \"don't\",\n",
    "    \"e1\": \"everyone\",\n",
    "    \"eg\": \"for example\",\n",
    "    \"emc2\": \"genius\",\n",
    "    \"emo\": \"emotional\",\n",
    "    \"enuf\": \"enough\",\n",
    "    \"eod\": \"end of discussion\",\n",
    "    \"eof\": \"end of file\",\n",
    "    \"eom\": \"end of message\",\n",
    "    \"eta\": \"estimated time of arrival\",\n",
    "    \"every1\": \"everyone\",\n",
    "    \"evs\": \"whatever\",\n",
    "    \"exp\": \"experience\",\n",
    "    \"f\": \"female\",\n",
    "    \"f2f\": \"face to face\",\n",
    "    \"f2p\": \"free to play\",\n",
    "    \"f2t\": \"free to talk\",\n",
    "    \"f9\": \"fine\",\n",
    "    \"fab\": \"fabulous\",\n",
    "    \"fail\": \"failure\",\n",
    "    \"faq\": \"frequently asked questions\",\n",
    "    \"fav\": \"favorite\",\n",
    "    \"fave\": \"favorite\",\n",
    "    \"favs\": \"favorites\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"fbc\": \"facebook chat\",\n",
    "    \"fbf\": \"facebook friend\",\n",
    "    \"fml\": \"family\",\n",
    "    \"fn\": \"fine\",\n",
    "    \"fo\": \"freaking out\",\n",
    "    \"fri\": \"friday\",\n",
    "    \"frnd\": \"friend\",\n",
    "    \"fu\": \"fuck you\",\n",
    "    \"fugly\": \"fucking ugly\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"g1\": \"good one\",\n",
    "    \"g2b\": \"going to bed\",\n",
    "    \"g2cu\": \"good to see you\",\n",
    "    \"g2g\": \"good to go\",\n",
    "    \"g4i\": \"go for it\",\n",
    "    \"g4n\": \"good for nothing\",\n",
    "    \"g4u\": \"good for you\",\n",
    "    \"g9\": \"goodnight\",\n",
    "    \"ga\": \"go ahead\",\n",
    "    \"ge\": \"good evening\",\n",
    "    \"gl\": \"good luck\",\n",
    "    \"gm\": \"good morning\",\n",
    "    \"gn\": \"goodnight\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"goon\": \"idiot\",\n",
    "    \"gorge\": \"gorgeous\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"grats\": \"congratulations\",\n",
    "    \"gratz\": \"congratulations\",\n",
    "    \"grl\": \"girl\",\n",
    "    \"gt2t\": \"got time to talk\",\n",
    "    \"gtg\": \"good to go\",\n",
    "    \"gud\": \"good\",\n",
    "    \"gv\": \"give\",\n",
    "    \"gvn\": \"given\",\n",
    "    \"gw\": \"good work\",\n",
    "    \"h/o\": \"hold on\",\n",
    "    \"h/p\": \"hold please\",\n",
    "    \"h/t\": \"hat tip\",\n",
    "    \"h/u\": \"hook up\",\n",
    "    \"h2cus\": \"hope to see you soon\",\n",
    "    \"h4u\": \"hot for you\",\n",
    "    \"h4x0r\": \"hacker\",\n",
    "    \"h4x0rz\": \"hackers\",\n",
    "    \"h8\": \"hate\",\n",
    "    \"h8r\": \"hater\",\n",
    "    \"h8t\": \"hate\",\n",
    "    \"ha\": \"hello again\",\n",
    "    \"haha\": \"laughing\",\n",
    "    \"hai\": \"hi\",\n",
    "    \"hak\": \"hugs and kisses\",\n",
    "    \"han\": \"how about now?\",\n",
    "    \"hav\": \"have\",\n",
    "    \"hax\": \"hacks\",\n",
    "    \"haxor\": \"hacker\",\n",
    "    \"hay\": \"how are you\",\n",
    "    \"hb2u\": \"happy birthday to you\",\n",
    "    \"hbbd\": \"happy belated birthday\",\n",
    "    \"hbd\": \"happy birthday\",\n",
    "    \"hc\": \"how cool\",\n",
    "    \"hcit\": \"how cool is that\",\n",
    "    \"hehe\": \"laughing\",\n",
    "    \"hf\": \"have fun\",\n",
    "    \"hi5\": \"high five\",\n",
    "    \"hig\": \"how's it going?\",\n",
    "    \"hih\": \"hope it helps\",\n",
    "    \"ho\": \"hold on\",\n",
    "    \"hoc\": \"house of cards\",\n",
    "    \"hof\": \"hall of fame\",\n",
    "    \"holla\": \"holler\",\n",
    "    \"hom\": \"hit or miss\",\n",
    "    \"hood\": \"neighborhood\",\n",
    "    \"hoops\": \"basketball\",\n",
    "    \"hottie\": \"attractive person\",\n",
    "    \"hr\": \"human resources\",\n",
    "    \"hru\": \"how are you\",\n",
    "    \"hry\": \"hurry\",\n",
    "    \"hubby\": \"husband\",\n",
    "    \"hwk\": \"homework\",\n",
    "    \"hwp\": \"height weight proportionate\",\n",
    "    \"hwu\": \"hey, what's up?\",\n",
    "    \"hxc\": \"hardcore\",\n",
    "    \"h^\": \"hook up\",\n",
    "    \"i8\": \"i ate\",\n",
    "    \"i8u\": \"i hate you\",\n",
    "    \"ia\": \"i agree\",\n",
    "    \"iab\": \"in a bit\",\n",
    "    \"iac\": \"in any case\",\n",
    "    \"iad\": \"it all depends\",\n",
    "    \"iae\": \"in any event\",\n",
    "    \"iag\": \"it's all good\",\n",
    "    \"iagw\": \"in a good way\",\n",
    "    \"iail\": \"i am in love\",\n",
    "    \"iam\": \"in a minute\",\n",
    "    \"ic\": \"i see\",\n",
    "    \"id10t\": \"idiot\",\n",
    "    \"idc\": \"i don't care\",\n",
    "    \"idd\": \"indeed\",\n",
    "    \"idi\": \"i doubt it\",\n",
    "    \"idk\": \"i don't know\",\n",
    "    \"idky\": \"i don't know why\",\n",
    "    \"idmb\": \"i'll do my best\",\n",
    "    \"idn\": \"i don't know\",\n",
    "    \"idnk\": \"i do not know\",\n",
    "    \"idr\": \"i don't remember\",\n",
    "    \"idt\": \"i don't think\",\n",
    "    \"idts\": \"i don't think so\",\n",
    "    \"idtt\": \"i'll drink to that\",\n",
    "    \"idu\": \"i don't understand\",\n",
    "    \"ie\": \"that is\",\n",
    "    \"ig2p\": \"i got to pee\",\n",
    "    \"iggy\": \"ignored\",\n",
    "    \"ight\": \"alright\",\n",
    "    \"igi\": \"i get it\",\n",
    "    \"ign\": \"in-game name\",\n",
    "    \"igtp\": \"i get the point\",\n",
    "    \"ih8u\": \"i hate you\",\n",
    "    \"ihu\": \"i hate you\",\n",
    "    \"ihy\": \"i hate you\",\n",
    "    \"ii\": \"i'm impressed\",\n",
    "    \"iiok\": \"if i only knew\",\n",
    "    \"iir\": \"if i remember\",\n",
    "    \"iirc\": \"if i remember correctly\",\n",
    "    \"iit\": \"i'm impressed too\",\n",
    "    \"iiuc\": \"if i understand correctly\",\n",
    "    \"ik\": \"i know\",\n",
    "    \"ikhyf\": \"i know how you feel\",\n",
    "    \"ikr\": \"i know, right?\",\n",
    "    \"ikwum\": \"i know what you mean\",\n",
    "    \"ikwym\": \"i know what you mean\",\n",
    "    \"ikyd\": \"i know you did\",\n",
    "    \"ilu\": \"i like you\",\n",
    "    \"ilu2\": \"i love you too\",\n",
    "    \"ilub\": \"i love you baby\",\n",
    "    \"ilyk\": \"i'll let you know\",\n",
    "    \"ilyl\": \"i love you lots\",\n",
    "    \"ilysm\": \"i love you so much\",\n",
    "    \"ima\": \"i'm\",\n",
    "    \"imma\": \"i'm gonna\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imy\": \"i miss you\",\n",
    "    \"inb4\": \"in before\",\n",
    "    \"inc\": \"incoming\",\n",
    "    \"indie\": \"independent\",\n",
    "    \"info\": \"information\",\n",
    "    \"init\": \"initialize\",\n",
    "    \"ipo\": \"initial public offering\",\n",
    "    \"ir\": \"in room\",\n",
    "    \"ir8\": \"irate\",\n",
    "    \"irdk\": \"i really don't know\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"iyo\": \"in your opinion\",\n",
    "    \"iyq\": \"i like you\",\n",
    "    \"j/k\": \"just kidding\",\n",
    "    \"j/p\": \"just playing\",\n",
    "    \"j/w\": \"just wondering\",\n",
    "    \"j2lyk\": \"just to let you know\",\n",
    "    \"j4f\": \"just for fun\",\n",
    "    \"j4g\": \"just for grins\",\n",
    "    \"jas\": \"just a second\",\n",
    "    \"jb/c\": \"just because\",\n",
    "    \"joshing\": \"joking\",\n",
    "    \"k\": \"ok\",\n",
    "    \"k3u\": \"i love you\",\n",
    "    \"kappa\": \"sarcasm\",\n",
    "    \"kek\": \"korean laugh\",\n",
    "    \"keke\": \"korean laugh\",\n",
    "    \"kewl\": \"cool\",\n",
    "    \"kewt\": \"cute\",\n",
    "    \"kfc\": \"kentucky fried chicken\",\n",
    "    \"kgo\": \"ok, go\",\n",
    "    \"kik\": \"laughing out loud\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"kk\": \"ok\",\n",
    "    \"kl\": \"kool\",\n",
    "    \"km\": \"kiss me\",\n",
    "    \"kma\": \"kiss my ass\",\n",
    "    \"knp\": \"ok, no problem\",\n",
    "    \"kw\": \"know\",\n",
    "    \"kwl\": \"cool\",\n",
    "    \"l2m\": \"listening to music\",\n",
    "    \"l2p\": \"learn to play\",\n",
    "    \"l33t\": \"leet\",\n",
    "    \"l8\": \"late\",\n",
    "    \"l8er\": \"later\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"la\": \"laughing a lot\",\n",
    "    \"laf\": \"laugh\",\n",
    "    \"laffing\": \"laughing\",\n",
    "    \"lafs\": \"love at first sight\",\n",
    "    \"lam\": \"leave a message\",\n",
    "    \"lamer\": \"lame person\",\n",
    "    \"legit\": \"legitimate\",\n",
    "    \"lemeno\": \"let me know\",\n",
    "    \"lil\": \"little\",\n",
    "    \"lk\": \"like\",\n",
    "    \"llol\": \"literally laughing out loud\",\n",
    "    \"lmho\": \"laughing my head off\",\n",
    "    \"loi\": \"laughing on the inside\",\n",
    "    \"lola\": \"love often, laugh a lot\",\n",
    "    \"lolol\": \"lots of laugh out louds\",\n",
    "    \"lolz\": \"laugh out louds\",\n",
    "    \"ltr\": \"later\",\n",
    "    \"lulz\": \"lol\",\n",
    "    \"luv\": \"love\",\n",
    "    \"luzr\": \"loser\",\n",
    "    \"lv\": \"love\",\n",
    "    \"ly\": \"love ya\",\n",
    "    \"lya\": \"love you always\",\n",
    "    \"lyk\": \"let you know\",\n",
    "    \"lyn\": \"lying\",\n",
    "    \"lysm\": \"love you so much\",\n",
    "    \"m\": \"male\",\n",
    "    \"mcd\": \"mcdonald's\",\n",
    "    \"mcds\": \"mcdonald's\",\n",
    "    \"md@u\": \"mad at you\",\n",
    "    \"me2\": \"me too\",\n",
    "    \"meh\": \"whatever\",\n",
    "    \"mf\": \"mother fucker\",\n",
    "    \"mfb\": \"mother fucking bitch\",\n",
    "    \"mgmt\": \"management\",\n",
    "    \"mid\": \"middle\",\n",
    "    \"mil\": \"mother-in-law\",\n",
    "    \"min\": \"minute\",\n",
    "    \"mins\": \"minutes\",\n",
    "    \"mk\": \"okay\",\n",
    "    \"mkay\": \"ok\",\n",
    "    \"mmk\": \"ok\",\n",
    "    \"mms\": \"multimedia messaging service\",\n",
    "    \"mng\": \"manage\",\n",
    "    \"mngr\": \"manager\",\n",
    "    \"mod\": \"modification\",\n",
    "    \"mofo\": \"mother fucking\",\n",
    "    \"mojo\": \"attractive talent\",\n",
    "    \"moss\": \"chill\",\n",
    "    \"ms\": \"miss\",\n",
    "    \"msg\": \"message\",\n",
    "    \"mtg\": \"meeting\",\n",
    "    \"mth\": \"month\",\n",
    "    \"mu\": \"miss you\",\n",
    "    \"mu@\": \"meet you at\",\n",
    "    \"muah\": \"kiss\",\n",
    "    \"mula\": \"money\",\n",
    "    \"mwa\": \"kiss\",\n",
    "    \"mwah\": \"kiss\",\n",
    "    \"n/m\": \"nevermind\",\n",
    "    \"n/m/h\": \"nothing much here\",\n",
    "    \"n/r\": \"no reserve\",\n",
    "    \"n00b\": \"newbie\",\n",
    "    \"n1\": \"nice one\",\n",
    "    \"n1c\": \"no one cares\",\n",
    "    \"n2m\": \"not too much\",\n",
    "    \"n2mh\": \"not too much here\",\n",
    "    \"n2w\": \"not to worry\",\n",
    "    \"n64\": \"nintendo 64\",\n",
    "    \"n8kd\": \"naked\",\n",
    "    \"nac\": \"not a chance\",\n",
    "    \"nah\": \"no\",\n",
    "    \"nal\": \"nationality\",\n",
    "    \"narc\": \"tattle tale\",\n",
    "    \"nark\": \"informant\",\n",
    "    \"naw\": \"no\",\n",
    "    \"nb\": \"not bad\",\n",
    "    \"nbd\": \"no big deal\",\n",
    "    \"nbjf\": \"no brag, just fact\",\n",
    "    \"nd\": \"and\",\n",
    "    \"ne\": \"any\",\n",
    "    \"ne1\": \"anyone\",\n",
    "    \"ne1er\": \"anyone here\",\n",
    "    \"neh\": \"no\",\n",
    "    \"nemore\": \"anymore\",\n",
    "    \"neva\": \"never\",\n",
    "    \"neway\": \"anyway\",\n",
    "    \"newaze\": \"anyways\",\n",
    "    \"newb\": \"newbie\",\n",
    "    \"nite\": \"night\",\n",
    "    \"nn2r\": \"no need to reply\",\n",
    "    \"nnito\": \"not necessarily in that order\",\n",
    "    \"nnto\": \"no need to open\",\n",
    "    \"nntr\": \"no need to reply\",\n",
    "    \"no1\": \"no one\",\n",
    "    \"noob\": \"newbie\",\n",
    "    \"nooblet\": \"young newbie\",\n",
    "    \"nooblord\": \"ultimate newbie\",\n",
    "    \"notch\": \"minecraft creator\",\n",
    "    \"nottie\": \"unattractive person\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"nub\": \"newbie\",\n",
    "    \"nuff\": \"enough\",\n",
    "    \"nufn\": \"nothing\",\n",
    "    \"num\": \"tasty\",\n",
    "    \"nvm\": \"nevermind\",\n",
    "    \"nvr\": \"never\",\n",
    "    \"nvrm\": \"nevermind\",\n",
    "    \"nw\": \"no way\",\n",
    "    \"nxt\": \"next\",\n",
    "    \"o4u\": \"only for you\",\n",
    "    \"obtw\": \"oh, by the way\",\n",
    "    \"obv\": \"obviously\",\n",
    "    \"obvi\": \"obviously\",\n",
    "    \"oc\": \"of course\",\n",
    "    \"ohemgee\": \"oh my gosh\",\n",
    "    \"oic\": \"oh, i see\",\n",
    "    \"oicn\": \"oh, i see now\",\n",
    "    \"oiy\": \"hi\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"onl\": \"online\",\n",
    "    \"onoz\": \"oh no\",\n",
    "    \"orly\": \"oh really\",\n",
    "    \"otay\": \"okay\",\n",
    "    \"otw\": \"on the way\",\n",
    "    \"outta\": \"out of\",\n",
    "    \"ovie\": \"overlord\",\n",
    "    \"ownage\": \"completely owned\",\n",
    "    \"p/d\": \"per day\",\n",
    "    \"p/m\": \"per month\",\n",
    "    \"p/y\": \"per year\",\n",
    "    \"p911\": \"parent alert!\",\n",
    "    \"p@h\": \"parents at home\",\n",
    "    \"pc\": \"personal computer\",\n",
    "    \"pda\": \"public display of affection\",\n",
    "    \"pic\": \"picture\",\n",
    "    \"pj\": \"poor joke\",\n",
    "    \"pl8\": \"plate\",\n",
    "    \"pld\": \"played\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"plzrd\": \"please read\",\n",
    "    \"pov\": \"point of view\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"ppp\": \"peace\",\n",
    "    \"prof\": \"professor\",\n",
    "    \"prolly\": \"probably\",\n",
    "    \"promo\": \"promotion\",\n",
    "    \"props\": \"recognition\",\n",
    "    \"prot\": \"protection\",\n",
    "    \"prvt\": \"private\",\n",
    "    \"ps\": \"postscript\",\n",
    "    \"ps2\": \"playstation 2\",\n",
    "    \"ps3\": \"playstation 3\",\n",
    "    \"psa\": \"public service announcement\",\n",
    "    \"psog\": \"pure stroke of genius\",\n",
    "    \"psp\": \"playstation portable\",\n",
    "    \"ptm\": \"please tell me\",\n",
    "    \"pwd\": \"password\",\n",
    "    \"psd\": \"password\",\n",
    "    \"pswd\": \"password\",\n",
    "    \"pwnd\": \"owned\",\n",
    "    \"pwned\": \"owned\",\n",
    "    \"pwnt\": \"owned\",\n",
    "    \"q4u\": \"question for you\",\n",
    "    \"qfe\": \"quoted for emphasis\",\n",
    "    \"qft\": \"quoted for truth\",\n",
    "    \"qq\": \"quick question\",\n",
    "    \"qqn\": \"looking\",\n",
    "    \"qrg\": \"quick reference guide\",\n",
    "    \"qt\": \"cutie\",\n",
    "    \"qtpi\": \"cutie pie\",\n",
    "    \"r\": \"are\",\n",
    "    \"r8\": \"rate\",\n",
    "    \"rdy\": \"ready\",\n",
    "    \"re\": \"replay\",\n",
    "    \"rehi\": \"hi again\",\n",
    "    \"rents\": \"parents\",\n",
    "    \"rep\": \"reputation\",\n",
    "    \"resq\": \"rescue\",\n",
    "    \"rgd\": \"regard\",\n",
    "    \"rgds\": \"regards\",\n",
    "    \"ridic\": \"ridiculous\",\n",
    "    \"rip\": \"rest in peace\",\n",
    "    \"rl\": \"real life\",\n",
    "    \"rlrt\": \"real life retweet\",\n",
    "    \"rly\": \"really\",\n",
    "    \"rm\": \"room\",\n",
    "    \"rn\": \"run\",\n",
    "    \"rnt\": \"aren't\",\n",
    "    \"rof\": \"laughing\",\n",
    "    \"rofl\": \"laughing\",\n",
    "    \"roflmao\": \"laughing\",\n",
    "    \"roflol\": \"laughing out loud\",\n",
    "    \"rolf\": \"laughing\",\n",
    "    \"ru\": \"are you\",\n",
    "    \"ruc\": \"are you coming?\",\n",
    "    \"rut\": \"are you there?\",\n",
    "    \"rx\": \"prescription\",\n",
    "    \"s/o\": \"sold out\",\n",
    "    \"s/u\": \"shut up\",\n",
    "    \"s/w\": \"software\",\n",
    "    \"s2r\": \"send to receive\",\n",
    "    \"s2s\": \"sorry to say\",\n",
    "    \"s2u\": \"same to you\",\n",
    "    \"samzd\": \"still amazed\",\n",
    "    \"sd\": \"sweet dreams\",\n",
    "    \"sec\": \"second\",\n",
    "    \"sho\": \"sure\",\n",
    "    \"sh^\": \"shut up\",\n",
    "    \"siul8r\": \"see you later\",\n",
    "    \"siv\": \"bad goaltender\",\n",
    "    \"sk8\": \"skate\",\n",
    "    \"sk8r\": \"skater\",\n",
    "    \"sly\": \"still love you\",\n",
    "    \"smf\": \"so much fun\",\n",
    "    \"smooch\": \"kiss\",\n",
    "    \"sorta\": \"sort of\",\n",
    "    \"spec\": \"specialization\",\n",
    "    \"spk\": \"speak\",\n",
    "    \"spkr\": \"speaker\",\n",
    "    \"srry\": \"sorry\",\n",
    "    \"srs\": \"serious\",\n",
    "    \"srsly\": \"seriously\",\n",
    "    \"sry\": \"sorry\",\n",
    "    \"stpd\": \"stupid\",\n",
    "    \"str\": \"strength\",\n",
    "    \"str8\": \"straight\",\n",
    "    \"sup\": \"what's up\",\n",
    "    \"syl\": \"see you later\",\n",
    "    \"sync\": \"synchronize\",\n",
    "    \"t2go\": \"time to go\",\n",
    "    \"t2m\": \"talk to me\",\n",
    "    \"t2u\": \"talk to you\",\n",
    "    \"t2ul\": \"talk to you later\",\n",
    "    \"t2ul8er\": \"talk to you later\",\n",
    "    \"t2ul8r\": \"talk to you later\",\n",
    "    \"t4lmk\": \"thanks for letting me know\",\n",
    "    \"t4p\": \"thanks for posting\",\n",
    "    \"t4t\": \"thanks for trade\",\n",
    "    \"tc\": \"take care\",\n",
    "    \"teh\": \"the\",\n",
    "    \"teme\": \"tell me\",\n",
    "    \"tg\": \"thank goodness\",\n",
    "    \"thnq\": \"thank you\",\n",
    "    \"tho\": \"though\",\n",
    "    \"thru\": \"through\",\n",
    "    \"tht\": \"that\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"tl\": \"tell\",\n",
    "    \"tlk\": \"talk\",\n",
    "    \"tlkin\": \"talking\",\n",
    "    \"tlking\": \"talking\",\n",
    "    \"tomoz\": \"tomorrow\",\n",
    "    \"tq\": \"thank you\",\n",
    "    \"tqvm\": \"thank you very much\",\n",
    "    \"tru\": \"true\",\n",
    "    \"ttl\": \"talk to you later\",\n",
    "    \"ttly\": \"totally\",\n",
    "    \"ttul\": \"talk to you later\",\n",
    "    \"tty\": \"talk to you\",\n",
    "    \"tu\": \"thank you\",\n",
    "    \"tude\": \"attitude\",\n",
    "    \"tx\": \"thanks\",\n",
    "    \"txt\": \"text\",\n",
    "    \"txtin\": \"texting\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"tyfa\": \"thank you for asking\",\n",
    "    \"tyl\": \"thank you lord\",\n",
    "    \"tym\": \"thank you much\",\n",
    "    \"tyt\": \"take your time\",\n",
    "    \"tyvm\": \"thank you very much\",\n",
    "    \"u\": \"you\",\n",
    "    \"u-ok\": \"you ok?\",\n",
    "    \"u/l\": \"upload\",\n",
    "    \"u2\": \"you too\",\n",
    "    \"u2u\": \"up to you\",\n",
    "    \"uok\": \"you ok?\",\n",
    "    \"ur\": \"your\",\n",
    "    \"ut\": \"you there?\",\n",
    "    \"veggies\": \"vegetables\",\n",
    "    \"vry\": \"very\",\n",
    "    \"vs\": \"versus\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/b\": \"welcome back\",\n",
    "    \"w/e\": \"whatever\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"w2f\": \"way too funny\",\n",
    "    \"w2g\": \"way to go\",\n",
    "    \"w2k\": \"windows 2000\",\n",
    "    \"w4u\": \"wait for you\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"w84m\": \"wait for me\",\n",
    "    \"w8am\": \"wait a minute\",\n",
    "    \"w8ing\": \"waiting\",\n",
    "    \"w8n\": \"waiting\",\n",
    "    \"wa\": \"what\",\n",
    "    \"waa\": \"crying\",\n",
    "    \"wack\": \"strange\",\n",
    "    \"wan2\": \"want to\",\n",
    "    \"wannabe\": \"want to be\",\n",
    "    \"wat\": \"what\",\n",
    "    \"watev\": \"whatever\",\n",
    "    \"watevs\": \"whatever\",\n",
    "    \"wlcm\": \"welcome\",\n",
    "    \"wha\": \"what\",\n",
    "    \"whipped\": \"tired\",\n",
    "    \"wht\": \"what\",\n",
    "    \"wk\": \"week\",\n",
    "    \"wknd\": \"weekend\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"wtg\": \"way to go\",\n",
    "    \"wup\": \"what's up?\",\n",
    "    \"ya\": \"yes\",\n",
    "    \"yeap\": \"yes\",\n",
    "    \"yep\": \"yes\",\n",
    "    \"yepperz\": \"yes\",\n",
    "    \"yesh\": \"yes\",\n",
    "    \"yo\": \"hi\",\n",
    "    \"yr\": \"your\",\n",
    "    \"yrs\": \"years\",\n",
    "    \"yt\": \"you there?\",\n",
    "    \"yt?\": \"you there?\",\n",
    "    \"yup\": \"yes\",\n",
    "    \"yupz\": \"ok\",\n",
    "    \"zzz\": \"sleeping\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import inflect\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import parsing\n",
    "#from gensim.parsing.preprocessing import split_alphanum 0==>99\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_stem = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "pt_stem = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "##Convert apostrophes word to original form\n",
    "def replace_numbers(word):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    word = word.split()\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isdigit():\n",
    "            word[i] = p.number_to_words(word[i])\n",
    "    word = \" \".join(word)\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"  Fixing Word Lengthening\n",
    "##https://rustyonrampa\"ge.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html\"\"\"\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def abbreviation_look_up(text):\n",
    "    \"\"\"\n",
    "    Replace abbreviation word in text to their original form\n",
    "    Example: hi, thanq so mch => hi, thank you so much\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        slanged (str): cleaned text with replaced slang\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in abbreviation_dict:\n",
    "            new_text.append(abbreviation_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    slanged = \" \".join(new_text)\n",
    "    return slanged\n",
    "\n",
    "def appos_look_up(text):\n",
    "    \"\"\"\n",
    "    Convert apostrophes word to original form\n",
    "    Example: I don't know what is going on?  => I do not know what is going on? \n",
    "    Args:\n",
    "        text (str): text \n",
    "    Returns:\n",
    "        apposed (str) : text with converted apostrophes\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in appos_dict:\n",
    "            new_text.append(appos_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    apposed = \" \".join(new_text)\n",
    "    return apposed\n",
    "\n",
    "\n",
    "def correct_word(text):\n",
    "    # Correct words\n",
    "    spell = SpellChecker()\n",
    "    misspelled = text.split()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(misspelled)):\n",
    "        # Get the one `most likely` answer\n",
    "        word = spell.correction(misspelled[i])\n",
    "        misspelled[i]=word\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"v\")\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"n\")\n",
    "        text = \" \".join(misspelled)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeated_characters(text):\n",
    "    \"\"\"\n",
    "    Remove repeated characters (>2) in words to max limit of 2\n",
    "    Example: I am verrry happpyyy today => I am verry happyy today\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed repeated chars\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'(.)\\1+')\n",
    "    clean_text = regex_pattern.sub(r'\\1\\1', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def separate_digit_text(text):\n",
    "    \"\"\"\n",
    "    Separate digit and words with space in text\n",
    "    Example: I will be booking tickets for 2adults => I will be booking tickets for 2 adults   \n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with separated digits and words\n",
    "    \"\"\"\n",
    "    regex_patter = re.compile(r'([\\d]+)([a-zA-Z]+)')\n",
    "    clean_text = regex_patter.sub(r'\\1 \\2', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stem_text(text, stemmer='snowball'):\n",
    "    \"\"\"\n",
    "    Convert words in text into their root form\n",
    "    Example: I am playing in ground => I am play in ground \n",
    "    Args:\n",
    "        text (str): text\n",
    "        \n",
    "    Returns:\n",
    "        text_stem (str): cleaned text with replaced stem words\n",
    "    \"\"\"\n",
    "    #text = remove_inside_braces(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    if stemmer == 'snowball':\n",
    "        text_stem = \" \".join([sb_stem.stem(w) for w in tokens])\n",
    "    else:\n",
    "        text_stem = \" \".join([pt_stem.stem(w) for w in tokens])\n",
    "    \n",
    "    return text_stem\n",
    "\n",
    "\n",
    "def remove_single_char_word(text):\n",
    "    \"\"\"\n",
    "    Remove single character word from text\n",
    "    Example: I am in a home for 2 years => am in home for years \n",
    "    Args:\n",
    "        text (str): text\n",
    "         \n",
    "    Returns:\n",
    "        (str): text with single char removed\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filter_words = [word for word in words if len(word) > 1]\n",
    "    return \" \".join(filter_words)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removed special characters from text\n",
    "    Example: he: I am going. are you coming? => he I am going. are you coming\n",
    "   \n",
    "    Args:\n",
    "        text (str): text\n",
    "   \n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed special characters\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[\\,+\\:\\?\\!\\\"\\(\\)!\\'\\.\\%\\[\\]]+')\n",
    "    clean_text = regex_pattern.sub(r' ', text)\n",
    "    clean_text = clean_text.replace('-', '')\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    \"\"\"\n",
    "    Remove extra white spaces space from text\n",
    "    Example: hey are   you coming. ? => he are you coming. ?\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): clean text with removed extra white spaces\n",
    "    \"\"\"\n",
    "    #text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    clean_text = ' '.join(text.strip().split())\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def replace_digits_with_char(text, replace_char='d'):\n",
    "    \"\"\"\n",
    "    Replace digits to `replace_char`\n",
    "    Example: I will be there on 22 april. => I will be there on dd april.\n",
    "    Args:\n",
    "        text (str): text\n",
    "        replace_char (str): character with which digit has to be replaced\n",
    "    Returns:\n",
    "        clean_text (str): clean text with replaced char for digits\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[0-9]')\n",
    "    clean_text = regex_pattern.sub(replace_char, text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    \"\"\"\n",
    "    Remove urls from text\n",
    "    Example: link to latest cricket score. https://xyz.com/a/b => link to latest cricket score.\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed urls\n",
    "    \"\"\"\n",
    "\n",
    "    urlfree = []\n",
    "    for word in text.split():\n",
    "        if not word.startswith(\"www\"):\n",
    "            urlfree.append(word)\n",
    "        elif not word.startswith(\"http\"):\n",
    "            urlfree.append(word)\n",
    "        elif not word.endswith(\".html\"):\n",
    "            urlfree.append(word)\n",
    "    urlfree = \" \".join(urlfree)\n",
    "\n",
    "    urls = re.finditer(r'http[\\w]*:\\/\\/[\\w]*\\.?[\\w-]+\\.+[\\w]+[\\/\\w]+', urlfree)\n",
    "    for i in urls:\n",
    "        urlfree = re.sub(i.group().strip(), '', urlfree)\n",
    "    return urlfree\n",
    "\n",
    "\n",
    "def remove_alphanumerics(text):\n",
    "    \"\"\"\n",
    "    Remove alphanumeric words from text\n",
    "    Example: hello man whatsup123 => hello man\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed alphanumeric words\n",
    "    \"\"\"\n",
    "    txt = []\n",
    "    for each in text.split():\n",
    "        if not any(x in each.lower() for x in \"0123456789\"):\n",
    "            txt.append(each)\n",
    "    txtsent = \" \".join(txt)\n",
    "    return txtsent \n",
    "\n",
    "\n",
    "def remove_words_start_with(text, starts_with_char):\n",
    "    \"\"\"\n",
    "    Remove words start with character `starts_with_char`\n",
    "    Example: dhoni rocks with last ball six #dhoni #six => dhoni rocks with last ball six (start_char_with='#')\n",
    "    Args:\n",
    "        text (str): text\n",
    "        starts_with_char (str): starting characters of word, which to be removed from text\n",
    "    Returns:\n",
    "        text (str): text with removed words start with given chars\n",
    "    \"\"\"\n",
    "    urls = re.finditer(starts_with_char + r'[A-Za-z0-9\\w]*', text)\n",
    "    for i in urls:\n",
    "        text = re.sub(i.group().strip(), '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stop_words(text, stop_words=stop_words_list):\n",
    "    \"\"\"\n",
    "    This function removes stop words from text\n",
    "    Example: I am very excited for today's football match => very excited today's football match\n",
    "    Params\n",
    "        text (str) :text on which processing needs to done\n",
    "        stop_words (list) : stop words which needs to be removed\n",
    "    Returns\n",
    "        text(str): text after stop words removal\n",
    "    \"\"\"\n",
    "    stop_words = set(stop_words)\n",
    "    split_list = text.split(\" \")\n",
    "    split_list = [word for word in split_list if word not in stop_words]\n",
    "    return \" \".join(split_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformText(text):\n",
    "    text = split_alphanum(text)\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    ## Replace abbreviation word in text to their original form\n",
    "    text = abbreviation_look_up(text)\n",
    "    ##Convert apostrophes word to original form \n",
    "    text = appos_look_up(text)\n",
    "    ##Replace all interger occurrences in list of tokenized words with textual representation\n",
    "    text = replace_numbers(text)\n",
    "    text = reduce_lengthening(text)\n",
    "    \n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    ##removes stop words from text\n",
    "    text = remove_stop_words(text, stop_words=stop_words_list)\n",
    "    \n",
    "    \n",
    "    ## Removed special characters from text\n",
    "    text = remove_punctuations(text)\n",
    "    \n",
    "    # remove html markup\n",
    "    text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "    text = correct_word(text)\n",
    "    \n",
    "    # # Correct words\n",
    "    text = remove_repeated_characters(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ## Replace digits to `replace_char for date :\n",
    "    #text = separate_digit_text(text)\n",
    "\n",
    "    ## Convert words in text into their root form\n",
    "    #text = stem_text(text, stemmer='snowball')\n",
    "    \n",
    "    #remove_single_char_word\n",
    "    text = remove_single_char_word(text)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #text = replace_digits_with_char(text, replace_char='d')\n",
    "    \n",
    "    ## Remove urls from text\n",
    "    text = remove_url(text)\n",
    "    \n",
    "    ##Remove alphanumeric words from text\n",
    "    text = remove_alphanumerics(text)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##Separate digit and words with space in text\n",
    "    #text = separate_digit_text(text)\n",
    "    \n",
    "    \n",
    "    ## Strip multiple whitespaces\n",
    "    text = remove_extra_space(text)\n",
    "    # Removing non ASCII chars    \n",
    "    #text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'service of group'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_word('servic of groop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she have twenty soo do not be service go group'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformText(\" she' I have so 20 soooooo don't i'm  can't servic going grooooooooop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(lambda x: transformText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts= df['text']\n",
    "tags= df['label']\n",
    "# dictionary of lists  \n",
    "dict = {'text': texts , 'label': tags } \n",
    "     \n",
    "df = pd.DataFrame(dict) \n",
    "  \n",
    "# saving the dataframe \n",
    "df.to_csv('Data/DATA_preprocessing_version2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
