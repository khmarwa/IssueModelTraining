{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  I had ordered a data cable, got a very well fi...  NOTISSUE\n",
      "1                                   Love This Phone.  NOTISSUE\n",
      "2                I get a very well finished product.  NOTISSUE\n",
      "3                             I couldn't be happier.  NOTISSUE\n",
      "4  I was looking for this headset for a long time...  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DATA_FILE = os.path.abspath('DATA/DATA_not_ameliorate.csv')\n",
    "df = pd.read_csv(DATA_FILE,delimiter=';',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOTISSUE    2028\n",
       "ISSUE       2027\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts() #balanced Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4055, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List stop words \n",
    "stop_words_list={\n",
    " 'a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'across',\n",
    " 'after',\n",
    " 'afterwards',\n",
    " 'against',\n",
    " 'all',\n",
    " 'almost',\n",
    " 'alone',\n",
    " 'along',\n",
    " 'already',\n",
    " 'also',\n",
    " 'although',\n",
    " 'always',\n",
    " #'am',\n",
    " 'among',\n",
    " 'amongst',\n",
    " 'amount',\n",
    " 'an',\n",
    " 'and',\n",
    " 'another',\n",
    " 'any',\n",
    " 'anyhow',\n",
    " 'anyone',\n",
    " 'anything',\n",
    " 'anyway',\n",
    " 'anywhere',\n",
    " 'are',\n",
    " 'around',\n",
    " 'as',\n",
    " 'at',\n",
    " #'became',\n",
    " 'because',\n",
    " #'become',\n",
    " #'becomes',\n",
    " #'becoming',\n",
    " #'been',\n",
    " 'before',\n",
    " 'beforehand',\n",
    " #'behind',\n",
    " #'being',\n",
    " 'below',\n",
    " 'beside',\n",
    " 'besides',\n",
    " 'between',\n",
    " 'beyond',\n",
    " 'both',\n",
    " 'bottom',\n",
    " 'but',\n",
    " 'by',\n",
    " 'ca',\n",
    " #'call',\n",
    " 'can',\n",
    " 'cannot',\n",
    " 'could',\n",
    " 'did',\n",
    " #'do',\n",
    " #'does',\n",
    " #'doing',\n",
    " 'done',\n",
    " 'down',\n",
    " 'due',\n",
    " 'during',\n",
    " 'each',\n",
    " 'either',\n",
    " 'eleven',\n",
    " 'else',\n",
    " 'elsewhere',\n",
    " 'empty',\n",
    " 'enough',\n",
    " 'even',\n",
    " 'ever',\n",
    " 'every',\n",
    " 'everyone',\n",
    " #'everything',\n",
    " 'everywhere',\n",
    " 'except',\n",
    " 'few',\n",
    " 'first',\n",
    " 'for',\n",
    " 'former',\n",
    " 'formerly',\n",
    " 'from',\n",
    " 'front',\n",
    " 'full',\n",
    " 'further',\n",
    " #'had',\n",
    " #'has',\n",
    " #'have',\n",
    " 'he',\n",
    " 'hence',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hereafter',\n",
    " 'hereby',\n",
    " 'herein',\n",
    " 'hereupon',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'however',\n",
    " 'hundred',\n",
    " 'i',\n",
    " 'if',\n",
    " 'in',\n",
    " 'indeed',\n",
    " 'into',\n",
    " #'is',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'just',\n",
    " 'last',\n",
    " 'latter',\n",
    " 'latterly',\n",
    " 'least',\n",
    " 'less',\n",
    " #'made',\n",
    " #'make',\n",
    " 'many',\n",
    " 'may',\n",
    " 'me',\n",
    " 'meanwhile',\n",
    " 'might',\n",
    " 'mine',\n",
    " #'more',\n",
    " 'moreover',\n",
    " 'most',\n",
    " 'mostly',\n",
    " 'move',\n",
    " 'much',\n",
    " 'must',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'name',\n",
    " 'namely',\n",
    " 'neither',\n",
    " 'nevertheless',\n",
    " 'next',\n",
    " 'noone',\n",
    " 'nor',\n",
    " 'now',\n",
    " 'nowhere',\n",
    " 'of',\n",
    " 'off',\n",
    " 'often',\n",
    " 'on',\n",
    " 'once',\n",
    " 'only',\n",
    " 'onto',\n",
    " 'or',\n",
    " 'other',\n",
    " 'others',\n",
    " 'otherwise',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 'part',\n",
    " 'per',\n",
    " 'perhaps',\n",
    " 'put',\n",
    " 'quite',\n",
    " 'rather',\n",
    " 're',\n",
    " 'really',\n",
    " 'regarding',\n",
    " 'same',\n",
    " #'say',\n",
    " #'see',\n",
    " #'seem',\n",
    " #'seemed',\n",
    " #'seeming',\n",
    " #'seems',\n",
    " 'serious',\n",
    " 'several',\n",
    " 'she',\n",
    " 'should',\n",
    " #'show',\n",
    " 'side',\n",
    " 'since',\n",
    " 'so',\n",
    " 'some',\n",
    " 'somehow',\n",
    " 'someone',\n",
    " 'something',\n",
    " 'sometime',\n",
    " 'sometimes',\n",
    " 'somewhere',\n",
    " 'still',\n",
    " 'such',\n",
    " #'take',\n",
    " 'than',\n",
    " 'that',\n",
    " 'the',\n",
    " 'their',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'thence',\n",
    " 'there',\n",
    " 'thereafter',\n",
    " 'thereby',\n",
    " 'therefore',\n",
    " 'therein',\n",
    " 'thereupon',\n",
    " 'these',\n",
    " 'they',\n",
    " 'third',\n",
    " 'this',\n",
    " 'those',\n",
    " 'though',\n",
    " 'three',\n",
    " 'through',\n",
    " 'throughout',\n",
    " 'thru',\n",
    " 'thus',\n",
    " 'to',\n",
    " 'together',\n",
    " 'too',\n",
    " 'top',\n",
    " #'toward',\n",
    " #'towards',\n",
    " 'twelve',\n",
    " 'under',\n",
    " 'unless',\n",
    " 'until',\n",
    " 'up',\n",
    " 'upon',\n",
    " 'us',\n",
    " #'used',\n",
    " 'using',\n",
    " 'various',\n",
    " 'very',\n",
    " 'via',\n",
    " #'was',\n",
    " 'we',\n",
    " 'were',\n",
    " 'what',\n",
    " 'whatever',\n",
    " 'when',\n",
    " 'whence',\n",
    " 'whenever',\n",
    " 'where',\n",
    " 'whereafter',\n",
    " 'whereas',\n",
    " 'whereby',\n",
    " 'wherein',\n",
    " 'whereupon',\n",
    " 'wherever',\n",
    " 'whether',\n",
    " 'which',\n",
    " 'while',\n",
    " 'whither',\n",
    " 'who',\n",
    " 'whoever',\n",
    " 'whole',\n",
    " 'whom',\n",
    " 'whose',\n",
    " 'why',\n",
    " 'will',\n",
    " 'with',\n",
    " 'within',\n",
    " 'without',\n",
    " 'would',\n",
    " 'yet',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "appos_dict = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"im\": \"I am\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"we'll\": \"will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"didnt\": \"did not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_dict= {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"im\": \"I am\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"we'll\": \"will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"didnt\": \"did not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import inflect\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import parsing\n",
    "from gensim.parsing.preprocessing import split_alphanum\n",
    "from spellchecker import SpellChecker\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert apostrophes word to original form\n",
    "def replace_word(word):\n",
    "    word = word.lower()\n",
    "    word = word.split()\n",
    "    for i in range(len(word)):\n",
    "        word[i] = abbreviation_dict.get(word[i], word[i])\n",
    "    word = \" \".join(word)\n",
    "    return word\n",
    "\n",
    "##Fixing Word Lengthening\n",
    "##https://rustyonrampa\"ge.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def replace_numbers(word):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    word = word.split()\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isdigit():\n",
    "            word[i] = p.number_to_words(word[i])\n",
    "    word = \" \".join(word)\n",
    "    return word\n",
    "def transformText(text):\n",
    "    text = split_alphanum(text)\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    text = replace_word(text)\n",
    "    text = replace_numbers(text)\n",
    "    text = reduce_lengthening(text)\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    # Removing all the stopwords\n",
    "    filtered_words = [word for word in text.split() if word not in STOP_WORDS]\n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    # remove html markup\n",
    "    text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "    # Correct words\n",
    "    spell = SpellChecker()\n",
    "    misspelled = text.split()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(misspelled)):\n",
    "        # Get the one `most likely` answer\n",
    "        word = spell.correction(misspelled[i])\n",
    "        misspelled[i]=word\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"v\")\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"n\")\n",
    "    text = \" \".join(misspelled)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    # Strip all the numerics\n",
    "    #text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have have twenty soo do not be service go group'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformText(\" she'd've I have so 20 soooooo don't i'm  can't servic going grooooooooop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(lambda x: transformText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts= df['text']\n",
    "tags= df['label']\n",
    "# dictionary of lists  \n",
    "dict = {'text': texts , 'label': tags } \n",
    "     \n",
    "df = pd.DataFrame(dict) \n",
    "  \n",
    "# saving the dataframe \n",
    "df.to_csv('Data/DATA_preprocessing_brute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>have order data cable get well finish work pro...</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love phone</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get well finish product</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not be happier</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be look headset long time have get</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  have order data cable get well finish work pro...  NOTISSUE\n",
       "1                                         love phone  NOTISSUE\n",
       "2                            get well finish product  NOTISSUE\n",
       "3                                     not be happier  NOTISSUE\n",
       "4                 be look headset long time have get  NOTISSUE"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"def emoticons_look_up(text):\n",
    "    \"\"\"\n",
    "    \"\"\"Remove emoticons from text and returns list of emotions present in text\n",
    "    #Example: Sure, you are welcome :) => Sure, you are welcome.\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed emoticons sign\n",
    "        emolist (list) : list of emotions from text\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"words = text.split()\n",
    "    emolist = []\n",
    "    for word in words:\n",
    "        if word in emo:\n",
    "            emolist.append(str(emo[word]))\n",
    "            text = text.replace(word,\" \")\n",
    "    return text, emolist\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "sb_stem = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "pt_stem = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "##Convert apostrophes word to original form\n",
    "def replace_numbers(word):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    word = word.split()\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isdigit():\n",
    "            word[i] = p.number_to_words(word[i])\n",
    "    word = \" \".join(word)\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"  Fixing Word Lengthening\n",
    "##https://rustyonrampa\"ge.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html\"\"\"\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def abbreviation_look_up(text):\n",
    "    \"\"\"\n",
    "    Replace abbreviation word in text to their original form\n",
    "    Example: hi, thanq so mch => hi, thank you so much\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        slanged (str): cleaned text with replaced slang\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in abbreviation_dict:\n",
    "            new_text.append(abbreviation_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    slanged = \" \".join(new_text)\n",
    "    return slanged\n",
    "\n",
    "def appos_look_up(text):\n",
    "    \"\"\"\n",
    "    Convert apostrophes word to original form\n",
    "    Example: I don't know what is going on?  => I do not know what is going on? \n",
    "    Args:\n",
    "        text (str): text \n",
    "    Returns:\n",
    "        apposed (str) : text with converted apostrophes\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in appos_dict:\n",
    "            new_text.append(appos_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    apposed = \" \".join(new_text)\n",
    "    return apposed\n",
    "\n",
    "\n",
    "def correct_word(text):\n",
    "    # Correct words\n",
    "    text=\"servic groop\"\n",
    "    spell = SpellChecker()\n",
    "    misspelled = text.split()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(misspelled)):\n",
    "        # Get the one `most likely` answer\n",
    "        word = spell.correction(misspelled[i])\n",
    "        misspelled[i]=word\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"v\")\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"n\")\n",
    "        text = \" \".join(misspelled)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeated_characters(text):\n",
    "    \"\"\"\n",
    "    Remove repeated characters (>2) in words to max limit of 2\n",
    "    Example: I am verrry happpyyy today => I am verry happyy today\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed repeated chars\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'(.)\\1+')\n",
    "    clean_text = regex_pattern.sub(r'\\1\\1', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def separate_digit_text(text):\n",
    "    \"\"\"\n",
    "    Separate digit and words with space in text\n",
    "    Example: I will be booking tickets for 2adults => I will be booking tickets for 2 adults   \n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with separated digits and words\n",
    "    \"\"\"\n",
    "    regex_patter = re.compile(r'([\\d]+)([a-zA-Z]+)')\n",
    "    clean_text = regex_patter.sub(r'\\1 \\2', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stem_text(text, stemmer='snowball'):\n",
    "    \"\"\"\n",
    "    Convert words in text into their root form\n",
    "    Example: I am playing in ground => I am play in ground \n",
    "    Args:\n",
    "        text (str): text\n",
    "        \n",
    "    Returns:\n",
    "        text_stem (str): cleaned text with replaced stem words\n",
    "    \"\"\"\n",
    "    #text = remove_inside_braces(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    if stemmer == 'snowball':\n",
    "        text_stem = \" \".join([sb_stem.stem(w) for w in tokens])\n",
    "    else:\n",
    "        text_stem = \" \".join([pt_stem.stem(w) for w in tokens])\n",
    "    \n",
    "    return text_stem\n",
    "\n",
    "\n",
    "def remove_single_char_word(text):\n",
    "    \"\"\"\n",
    "    Remove single character word from text\n",
    "    Example: I am in a home for 2 years => am in home for years \n",
    "    Args:\n",
    "        text (str): text\n",
    "         \n",
    "    Returns:\n",
    "        (str): text with single char removed\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filter_words = [word for word in words if len(word) > 1]\n",
    "    return \" \".join(filter_words)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removed special characters from text\n",
    "    Example: he: I am going. are you coming? => he I am going. are you coming\n",
    "   \n",
    "    Args:\n",
    "        text (str): text\n",
    "   \n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed special characters\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[\\,+\\:\\?\\!\\\"\\(\\)!\\'\\.\\%\\[\\]]+')\n",
    "    clean_text = regex_pattern.sub(r' ', text)\n",
    "    clean_text = clean_text.replace('-', '')\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    \"\"\"\n",
    "    Remove extra white spaces space from text\n",
    "    Example: hey are   you coming. ? => he are you coming. ?\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): clean text with removed extra white spaces\n",
    "    \"\"\"\n",
    "    #text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    clean_text = ' '.join(text.strip().split())\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def replace_digits_with_char(text, replace_char='d'):\n",
    "    \"\"\"\n",
    "    Replace digits to `replace_char`\n",
    "    Example: I will be there on 22 april. => I will be there on dd april.\n",
    "    Args:\n",
    "        text (str): text\n",
    "        replace_char (str): character with which digit has to be replaced\n",
    "    Returns:\n",
    "        clean_text (str): clean text with replaced char for digits\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[0-9]')\n",
    "    clean_text = regex_pattern.sub(replace_char, text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    \"\"\"\n",
    "    Remove urls from text\n",
    "    Example: link to latest cricket score. https://xyz.com/a/b => link to latest cricket score.\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed urls\n",
    "    \"\"\"\n",
    "\n",
    "    urlfree = []\n",
    "    for word in text.split():\n",
    "        if not word.startswith(\"www\"):\n",
    "            urlfree.append(word)\n",
    "        elif not word.startswith(\"http\"):\n",
    "            urlfree.append(word)\n",
    "        elif not word.endswith(\".html\"):\n",
    "            urlfree.append(word)\n",
    "    urlfree = \" \".join(urlfree)\n",
    "\n",
    "    urls = re.finditer(r'http[\\w]*:\\/\\/[\\w]*\\.?[\\w-]+\\.+[\\w]+[\\/\\w]+', urlfree)\n",
    "    for i in urls:\n",
    "        urlfree = re.sub(i.group().strip(), '', urlfree)\n",
    "    return urlfree\n",
    "\n",
    "\n",
    "def remove_alphanumerics(text):\n",
    "    \"\"\"\n",
    "    Remove alphanumeric words from text\n",
    "    Example: hello man whatsup123 => hello man\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed alphanumeric words\n",
    "    \"\"\"\n",
    "    txt = []\n",
    "    for each in text.split():\n",
    "        if not any(x in each.lower() for x in \"0123456789\"):\n",
    "            txt.append(each)\n",
    "    txtsent = \" \".join(txt)\n",
    "    return txtsent \n",
    "\n",
    "\n",
    "def remove_words_start_with(text, starts_with_char):\n",
    "    \"\"\"\n",
    "    Remove words start with character `starts_with_char`\n",
    "    Example: dhoni rocks with last ball six #dhoni #six => dhoni rocks with last ball six (start_char_with='#')\n",
    "    Args:\n",
    "        text (str): text\n",
    "        starts_with_char (str): starting characters of word, which to be removed from text\n",
    "    Returns:\n",
    "        text (str): text with removed words start with given chars\n",
    "    \"\"\"\n",
    "    urls = re.finditer(starts_with_char + r'[A-Za-z0-9\\w]*', text)\n",
    "    for i in urls:\n",
    "        text = re.sub(i.group().strip(), '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stop_words(text, stop_words=stop_words_list):\n",
    "    \"\"\"\n",
    "    This function removes stop words from text\n",
    "    Example: I am very excited for today's football match => very excited today's football match\n",
    "    Params\n",
    "        text (str) :text on which processing needs to done\n",
    "        stop_words (list) : stop words which needs to be removed\n",
    "    Returns\n",
    "        text(str): text after stop words removal\n",
    "    \"\"\"\n",
    "    stop_words = set(stop_words)\n",
    "    split_list = text.split(\" \")\n",
    "    split_list = [word for word in split_list if word not in stop_words]\n",
    "    return \" \".join(split_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformtext(text):\n",
    "    text = split_alphanum(text)\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    ##Replace all interger occurrences in list of tokenized words with textual representation\n",
    "    text = replace_numbers(text)\n",
    "    \n",
    "    #text = reduce_lengthening(text)\n",
    "\n",
    "    text = remove_repeated_characters(text)\n",
    "    \n",
    "    ## Replace digits to `replace_char for date :\n",
    "    #text = separate_digit_text(text)\n",
    "\n",
    "    ## Replace slang word in text to their original form\n",
    "    text = abbreviation_look_up(text)\n",
    "    \n",
    "    ##Convert apostrophes word to original form \n",
    "    text = appos_look_up(text)\n",
    "   \n",
    "    ## # Correct words\n",
    "    text = correct_word(text)\n",
    "   \n",
    "    ## Convert words in text into their root form\n",
    "    text = stem_text(text, stemmer='snowball')\n",
    "    \n",
    "    #remove_single_char_word\n",
    "    text = remove_single_char_word(text)\n",
    "    \n",
    "    ## Removed special characters from text\n",
    "    text = remove_punctuations(text)\n",
    "    \n",
    "    ## Strip multiple whitespaces\n",
    "    text = remove_extra_space(text)\n",
    "    \n",
    "    text = replace_digits_with_char(text, replace_char='d')\n",
    "    \n",
    "    ## Remove urls from text\n",
    "    text = remove_url(text)\n",
    "    \n",
    "    ##Remove alphanumeric words from text\n",
    "    text = remove_alphanumerics(text)\n",
    "\n",
    "    #text = remove_words_start_with(text, starts_with_char)\n",
    "    ##removes stop words from text\n",
    "    text = remove_stop_words(text, stop_words=stop_words_list)\n",
    "    \n",
    "    ##Separate digit and words with space in text\n",
    "    text = separate_digit_text(text)\n",
    "    \n",
    "    # remove html markup\n",
    "    text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'servic group'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformtext(\"goining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m_check_if_should_check\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# check if it is a number (int, float, etc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: \"ahandsf'ee\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-ed85a31d73b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtransformText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3380\u001b[0m         \"\"\"\n\u001b[0;32m   3381\u001b[0m         new_values = super(Series, self)._map_values(\n\u001b[1;32m-> 3382\u001b[1;33m             arg, na_action=na_action)\n\u001b[0m\u001b[0;32m   3383\u001b[0m         return self._constructor(new_values,\n\u001b[0;32m   3384\u001b[0m                                  index=self.index).__finalize__(self)\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-ed85a31d73b9>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtransformText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-18764e8af78f>\u001b[0m in \u001b[0;36mtransformText\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisspelled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Get the one `most likely` answer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisspelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mmisspelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mmisspelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet_lemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisspelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"v\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mcorrection\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                 str: The most likely candidate \"\"\"\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_probability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mcandidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distance\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__edit_distance_alt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mknown\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    159\u001b[0m         return set(\n\u001b[0;32m    160\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_frequency\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_frequency\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         )\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m_check_if_should_check\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].map(lambda x: transformText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
