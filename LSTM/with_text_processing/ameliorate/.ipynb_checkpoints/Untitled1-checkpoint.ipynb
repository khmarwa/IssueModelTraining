{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "from folder.appos import appos_dict\n",
    "from folder.slangs import slangs_dict\n",
    "from folder.stopwords import stop_words_list\n",
    "from folder.emo import emo\n",
    "\n",
    "\n",
    "sb_stem = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "pt_stem = PorterStemmer()\n",
    "\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appos_look_up(text):\n",
    "    \"\"\"\n",
    "    Convert apostrophes word to original form\n",
    "    Example: I don't know what is going on?  => I do not know what is going on? \n",
    "    Args:\n",
    "        text (str): text \n",
    "\n",
    "    Returns:\n",
    "        apposed (str) : text with converted apostrophes\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in appos_dict:\n",
    "            new_text.append(appos_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    apposed = \" \".join(new_text)\n",
    "    return apposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appos_look_up(text):\n",
    "    \"\"\"\n",
    "    Convert apostrophes word to original form\n",
    "    Example: I don't know what is going on?  => I do not know what is going on? \n",
    "    Args:\n",
    "        text (str): text \n",
    "\n",
    "    Returns:\n",
    "        apposed (str) : text with converted apostrophes\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in appos_dict:\n",
    "            new_text.append(appos_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    apposed = \" \".join(new_text)\n",
    "    return apposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeated_characters(text):\n",
    "    \"\"\"\n",
    "    Remove repeated characters (>2) in words to max limit of 2\n",
    "    Example: I am verrry happpyyy today => I am verry happyy today\n",
    "    Args:\n",
    "        text (str): text\n",
    "\n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed repeated chars\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'(.)\\1+')\n",
    "    clean_text = regex_pattern.sub(r'\\1\\1', text)\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_digit_text(text):\n",
    "    \"\"\"\n",
    "    Separate digit and words with space in text\n",
    "    Example: I will be booking tickets for 2adults => I will be booking tickets for 2 adults   \n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with separated digits and words\n",
    "    \"\"\"\n",
    "    regex_patter = re.compile(r'([\\d]+)([a-zA-Z]+)')\n",
    "    clean_text = regex_patter.sub(r'\\1 \\2', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slang_look_up(text):\n",
    "    \"\"\"\n",
    "    Replace slang word in text to their original form\n",
    "    Example: hi, thanq so mch => hi, thank you so much\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        slanged (str): cleaned text with replaced slang\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in slangs_dict:\n",
    "            new_text.append(slangs_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    slanged = \" \".join(new_text)\n",
    "    return slanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_text(text, stemmer='snowball'):\n",
    "    \"\"\"\n",
    "    Convert words in text into their root form\n",
    "    Example: I am playing in ground => I am play in ground \n",
    "    Args:\n",
    "        text (str): text\n",
    "        \n",
    "    Returns:\n",
    "        text_stem (str): cleaned text with replaced stem words\n",
    "    \"\"\"\n",
    "    text = remove_inside_braces(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    if stemmer == 'snowball':\n",
    "        text_stem = \" \".join([sb_stem.stem(w) for w in tokens])\n",
    "    else:\n",
    "        text_stem = \" \".join([pt_stem.stem(w) for w in tokens])\n",
    "    \n",
    "    return text_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_single_char_word(text):\n",
    "    \"\"\"\n",
    "    Remove single character word from text\n",
    "    Example: I am in a home for 2 years => am in home for years \n",
    "    Args:\n",
    "        text (str): text\n",
    "         \n",
    "    Returns:\n",
    "        (str): text with single char removed\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filter_words = [word for word in words if len(word) > 1]\n",
    "    return \" \".join(filter_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removed special characters from text\n",
    "    Example: he: I am going. are you coming? => he I am going. are you coming\n",
    "   \n",
    "    Args:\n",
    "        text (str): text\n",
    "   \n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed special characters\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[\\,+\\:\\?\\!\\\"\\(\\)!\\'\\.\\%\\[\\]]+')\n",
    "    clean_text = regex_pattern.sub(r' ', text)\n",
    "    clean_text = clean_text.replace('-', '')\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_space(text):\n",
    "    \"\"\"\n",
    "    Remove extra white spaces space from text\n",
    "    Example: hey are   you coming. ? => he are you coming. ?\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): clean text with removed extra white spaces\n",
    "    \"\"\"\n",
    "    clean_text = ' '.join(text.strip().split())\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def replace_digits_with_char(text, replace_char='d'):\n",
    "    \"\"\"\n",
    "    Replace digits to `replace_char`\n",
    "    Example: I will be there on 22 april. => I will be there on dd april.\n",
    "    Args:\n",
    "        text (str): text\n",
    "        replace_char (str): character with which digit has to be replaced\n",
    "    Returns:\n",
    "        clean_text (str): clean text with replaced char for digits\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[0-9]')\n",
    "    clean_text = regex_pattern.sub(replace_char, text)\n",
    "    return clean_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticons_look_up(text):\n",
    "    \"\"\"\n",
    "    Remove emoticons from text and returns list of emotions present in text\n",
    "    Example: Sure, you are welcome :) => Sure, you are welcome.\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed emoticons sign\n",
    "        emolist (list) : list of emotions from text\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    emolist = []\n",
    "    for word in words:\n",
    "        if word in emo:\n",
    "            emolist.append(str(emo[word]))\n",
    "            text = text.replace(word,\" \")\n",
    "    return text, emolist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    \"\"\"\n",
    "    Remove urls from text\n",
    "    Example: link to latest cricket score. https://xyz.com/a/b => link to latest cricket score.\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed urls\n",
    "    \"\"\"\n",
    "\n",
    "    urlfree = []\n",
    "    for word in text.split():\n",
    "        if not word.startswith(\"www\"):\n",
    "            urlfree.append(word)\n",
    "        elif not word.startswith(\"http\"):\n",
    "            urlfree.append(word)\n",
    "        elif not word.endswith(\".html\"):\n",
    "            urlfree.append(word)\n",
    "    urlfree = \" \".join(urlfree)\n",
    "\n",
    "    urls = re.finditer(r'http[\\w]*:\\/\\/[\\w]*\\.?[\\w-]+\\.+[\\w]+[\\/\\w]+', urlfree)\n",
    "    for i in urls:\n",
    "        urlfree = re.sub(i.group().strip(), '', urlfree)\n",
    "    return urlfree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_alphanumerics(text):\n",
    "    \"\"\"\n",
    "    Remove alphanumeric words from text\n",
    "    Example: hello man whatsup123 => hello man\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed alphanumeric words\n",
    "    \"\"\"\n",
    "    txt = []\n",
    "    for each in text.split():\n",
    "        if not any(x in each.lower() for x in \"0123456789\"):\n",
    "            txt.append(each)\n",
    "    txtsent = \" \".join(txt)\n",
    "    return txtsent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_start_with(text, starts_with_char):\n",
    "    \"\"\"\n",
    "    Remove words start with character `starts_with_char`\n",
    "    Example: dhoni rocks with last ball six #dhoni #six => dhoni rocks with last ball six (start_char_with='#')\n",
    "    Args:\n",
    "        text (str): text\n",
    "        starts_with_char (str): starting characters of word, which to be removed from text\n",
    "\n",
    "    Returns:\n",
    "        text (str): text with removed words start with given chars\n",
    "    \"\"\"\n",
    "    urls = re.finditer(starts_with_char + r'[A-Za-z0-9\\w]*', text)\n",
    "    for i in urls:\n",
    "        text = re.sub(i.group().strip(), '', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text, stop_words=stop_words_list):\n",
    "    \"\"\"\n",
    "    This function removes stop words from text\n",
    "    Example: I am very excited for today's football match => very excited today's football match\n",
    "    Params\n",
    "        text (str) :text on which processing needs to done\n",
    "        stop_words (list) : stop words which needs to be removed\n",
    "    Returns\n",
    "        text(str): text after stop words removal\n",
    "    \"\"\"\n",
    "    stop_words = set(stop_words)\n",
    "    split_list = text.split(\" \")\n",
    "    split_list = [word for word in split_list if word not in stop_words]\n",
    "    return \" \".join(split_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
