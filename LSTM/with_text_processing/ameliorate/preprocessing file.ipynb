{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "print(\"import done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text     label\n",
      "0  I had ordered a data cable, got a very well fi...  NOTISSUE\n",
      "1                                   Love This Phone.  NOTISSUE\n",
      "2                I get a very well finished product.  NOTISSUE\n",
      "3                             I couldn't be happier.  NOTISSUE\n",
      "4  I was looking for this headset for a long time...  NOTISSUE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DATA_FILE = os.path.abspath('DATA/DATA_not_ameliorate.csv')\n",
    "df = pd.read_csv(DATA_FILE,delimiter=';',encoding='UTF-8')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOTISSUE    2028\n",
       "ISSUE       2027\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts() #balanced Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4055, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List stop words \n",
    "stop_words_list={\n",
    " 'a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'across',\n",
    " 'after',\n",
    " 'afterwards',\n",
    " 'against',\n",
    " 'all',\n",
    " 'almost',\n",
    " 'alone',\n",
    " 'along',\n",
    " 'already',\n",
    " 'also',\n",
    " 'although',\n",
    " 'always',\n",
    " #'am',\n",
    " 'among',\n",
    " 'amongst',\n",
    " 'amount',\n",
    " 'an',\n",
    " 'and',\n",
    " 'another',\n",
    " 'any',\n",
    " 'anyhow',\n",
    " 'anyone',\n",
    " 'anything',\n",
    " 'anyway',\n",
    " 'anywhere',\n",
    " 'are',\n",
    " 'around',\n",
    " 'as',\n",
    " 'at',\n",
    " #'became',\n",
    " 'because',\n",
    " #'become',\n",
    " #'becomes',\n",
    " #'becoming',\n",
    " #'been',\n",
    " 'before',\n",
    " 'beforehand',\n",
    " #'behind',\n",
    " #'being',\n",
    " 'below',\n",
    " 'beside',\n",
    " 'besides',\n",
    " 'between',\n",
    " 'beyond',\n",
    " 'both',\n",
    " 'bottom',\n",
    " 'but',\n",
    " 'by',\n",
    " 'ca',\n",
    " #'call',\n",
    " 'can',\n",
    " 'cannot',\n",
    " 'could',\n",
    " 'did',\n",
    " #'do',\n",
    " #'does',\n",
    " #'doing',\n",
    " 'done',\n",
    " 'down',\n",
    " 'due',\n",
    " 'during',\n",
    " 'each',\n",
    " 'either',\n",
    " 'eleven',\n",
    " 'else',\n",
    " 'elsewhere',\n",
    " 'empty',\n",
    " 'enough',\n",
    " 'even',\n",
    " 'ever',\n",
    " 'every',\n",
    " 'everyone',\n",
    " #'everything',\n",
    " 'everywhere',\n",
    " 'except',\n",
    " 'few',\n",
    " 'first',\n",
    " 'for',\n",
    " 'former',\n",
    " 'formerly',\n",
    " 'from',\n",
    " 'front',\n",
    " 'full',\n",
    " 'further',\n",
    " #'had',\n",
    " #'has',\n",
    " #'have',\n",
    " 'he',\n",
    " 'hence',\n",
    " 'her',\n",
    " 'here',\n",
    " 'hereafter',\n",
    " 'hereby',\n",
    " 'herein',\n",
    " 'hereupon',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " 'however',\n",
    " 'hundred',\n",
    " 'i',\n",
    " 'if',\n",
    " 'in',\n",
    " 'indeed',\n",
    " 'into',\n",
    " #'is',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'just',\n",
    " 'last',\n",
    " 'latter',\n",
    " 'latterly',\n",
    " 'least',\n",
    " 'less',\n",
    " #'made',\n",
    " #'make',\n",
    " 'many',\n",
    " 'may',\n",
    " 'me',\n",
    " 'meanwhile',\n",
    " 'might',\n",
    " 'mine',\n",
    " #'more',\n",
    " 'moreover',\n",
    " 'most',\n",
    " 'mostly',\n",
    " 'move',\n",
    " 'much',\n",
    " 'must',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'name',\n",
    " 'namely',\n",
    " 'neither',\n",
    " 'nevertheless',\n",
    " 'next',\n",
    " 'noone',\n",
    " 'nor',\n",
    " 'now',\n",
    " 'nowhere',\n",
    " 'of',\n",
    " 'off',\n",
    " 'often',\n",
    " 'on',\n",
    " 'once',\n",
    " 'only',\n",
    " 'onto',\n",
    " 'or',\n",
    " 'other',\n",
    " 'others',\n",
    " 'otherwise',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 'part',\n",
    " 'per',\n",
    " 'perhaps',\n",
    " 'put',\n",
    " 'quite',\n",
    " 'rather',\n",
    " 're',\n",
    " 'really',\n",
    " 'regarding',\n",
    " 'same',\n",
    " #'say',\n",
    " #'see',\n",
    " #'seem',\n",
    " #'seemed',\n",
    " #'seeming',\n",
    " #'seems',\n",
    " 'serious',\n",
    " 'several',\n",
    " 'she',\n",
    " 'should',\n",
    " #'show',\n",
    " 'side',\n",
    " 'since',\n",
    " 'so',\n",
    " 'some',\n",
    " 'somehow',\n",
    " 'someone',\n",
    " 'something',\n",
    " 'sometime',\n",
    " 'sometimes',\n",
    " 'somewhere',\n",
    " 'still',\n",
    " 'such',\n",
    " #'take',\n",
    " 'than',\n",
    " 'that',\n",
    " 'the',\n",
    " 'their',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'thence',\n",
    " 'there',\n",
    " 'thereafter',\n",
    " 'thereby',\n",
    " 'therefore',\n",
    " 'therein',\n",
    " 'thereupon',\n",
    " 'these',\n",
    " 'they',\n",
    " 'third',\n",
    " 'this',\n",
    " 'those',\n",
    " 'though',\n",
    " 'three',\n",
    " 'through',\n",
    " 'throughout',\n",
    " 'thru',\n",
    " 'thus',\n",
    " 'to',\n",
    " 'together',\n",
    " 'too',\n",
    " 'top',\n",
    " #'toward',\n",
    " #'towards',\n",
    " 'twelve',\n",
    " 'under',\n",
    " 'unless',\n",
    " 'until',\n",
    " 'up',\n",
    " 'upon',\n",
    " 'us',\n",
    " #'used',\n",
    " 'using',\n",
    " 'various',\n",
    " 'very',\n",
    " 'via',\n",
    " #'was',\n",
    " 'we',\n",
    " 'were',\n",
    " 'what',\n",
    " 'whatever',\n",
    " 'when',\n",
    " 'whence',\n",
    " 'whenever',\n",
    " 'where',\n",
    " 'whereafter',\n",
    " 'whereas',\n",
    " 'whereby',\n",
    " 'wherein',\n",
    " 'whereupon',\n",
    " 'wherever',\n",
    " 'whether',\n",
    " 'which',\n",
    " 'while',\n",
    " 'whither',\n",
    " 'who',\n",
    " 'whoever',\n",
    " 'whole',\n",
    " 'whom',\n",
    " 'whose',\n",
    " 'why',\n",
    " 'will',\n",
    " 'with',\n",
    " 'within',\n",
    " 'without',\n",
    " 'would',\n",
    " 'yet',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "appos_dict = {\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"arent\": \"are not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"doesnt\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"havent\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"i'd\": \"I would\",\n",
    "    \"i'll\": \"I will\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"im\": \"I am\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightnt\": \"might not\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustnt\": \"must not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"thats\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"whats\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"wasnt\": \"was not\",\n",
    "    \"we'll\": \"will\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"didnt\": \"did not\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_dict= {\n",
    "    'awsm': 'awesome',\n",
    "    \"aamof\": \"as a matter of fact\",\n",
    "    \"abt\": \"about\",\n",
    "    \"abt2\": \"about to\",\n",
    "    \"ac\": \"air conditioning\",\n",
    "    \"ace\": \"solo winner\",\n",
    "    \"ack\": \"acknowledged\",\n",
    "    \"admin\": \"administrator\",\n",
    "    \"thr\": \"there\",\n",
    "    \"frm\": \"from\",\n",
    "    \"aggro\": \"aggression\",\n",
    "    \"agl\": \"angel\",\n",
    "    \"dob\": \"date of birth\",\n",
    "    \"ai\": \"artificial intelligence\",\n",
    "    \"aiic\": \"as if i care\",\n",
    "    \"aka\": \"also known as\",\n",
    "    \"alap\": \"as long as possible\",\n",
    "    \"alol\": \"actually laughing out loud\",\n",
    "    \"ama\": \"ask me anything\",\n",
    "    \"amap\": \"as much as possible\",\n",
    "    \"amazn\": \"amazing\",\n",
    "    \"ammo\": \"ammunition\",\n",
    "    \"ams\": \"ask me something\",\n",
    "    \"anon\": \"anonymous\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"asat\": \"as simple as that\",\n",
    "    \"awks\": \"awkward\",\n",
    "    \"awl\": \"always with love\",\n",
    "    \"ayk\": \"as you know\",\n",
    "    \"azm\": \"awesome\",\n",
    "    \"b\": \"be\",\n",
    "    \"b&w\": \"black and white\",\n",
    "    \"b-day\": \"birthday\",\n",
    "    \"bday\": \"birthday\",\n",
    "    \"bcoz\": \"because\",\n",
    "    \"bcos\": \"because\",\n",
    "    \"bcz\": \"because\",\n",
    "    \"bf\": \"boyfriend\",\n",
    "    \"btw\": \"between\",\n",
    "    \"b4\": \"before\",\n",
    "    \"bai\": \"bye\",\n",
    "    \"bb\": \"bye bye\",\n",
    "    \"bc\": \"abuse\",\n",
    "    \"mc\": \"abuse\",\n",
    "    \"bcc\": \"blind carbon copy\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"biz\": \"business\",\n",
    "    \"bk\": \"back\",\n",
    "    \"bo\": \"back off\",\n",
    "    \"bro\": \"brother\",\n",
    "    \"btwn\": \"between\",\n",
    "    \"c\": \"see\",\n",
    "    \"char\": \"character\",\n",
    "    \"combo\": \"combination\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cu2\": \"see you too\",\n",
    "    \"cu2mr\": \"see you tomorrow\",\n",
    "    \"cya\": \"see ya\",\n",
    "    \"cyal8r\": \"see you later\",\n",
    "    \"cyb\": \"call you back\",\n",
    "    \"cye\": \"check your e-mail\",\n",
    "    \"cyf\": \"check your facebook\",\n",
    "    \"cyfb\": \"check your facebook\",\n",
    "    \"cyl\": \"catch ya later\",\n",
    "    \"cym\": \"check your myspace\",\n",
    "    \"cyo\": \"see you online\",\n",
    "    \"d8\": \"date\",\n",
    "    \"da\": \"the\",\n",
    "    \"dece\": \"decent\",\n",
    "    \"ded\": \"dead\",\n",
    "    \"dept\": \"department\",\n",
    "    \"dis\": \"this\",\n",
    "    \"ditto\": \"same\",\n",
    "    \"diva\": \"rude woman\",\n",
    "    \"dk\": \"don't know\",\n",
    "    \"dlm\": \"don't leave me\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"dnd\": \"do not disturb\",\n",
    "    \"dno\": \"don't know\",\n",
    "    \"dnt\": \"don't\",\n",
    "    \"e1\": \"everyone\",\n",
    "    \"eg\": \"for example\",\n",
    "    \"emc2\": \"genius\",\n",
    "    \"emo\": \"emotional\",\n",
    "    \"enuf\": \"enough\",\n",
    "    \"eod\": \"end of discussion\",\n",
    "    \"eof\": \"end of file\",\n",
    "    \"eom\": \"end of message\",\n",
    "    \"eta\": \"estimated time of arrival\",\n",
    "    \"every1\": \"everyone\",\n",
    "    \"evs\": \"whatever\",\n",
    "    \"exp\": \"experience\",\n",
    "    \"f\": \"female\",\n",
    "    \"f2f\": \"face to face\",\n",
    "    \"f2p\": \"free to play\",\n",
    "    \"f2t\": \"free to talk\",\n",
    "    \"f9\": \"fine\",\n",
    "    \"fab\": \"fabulous\",\n",
    "    \"fail\": \"failure\",\n",
    "    \"faq\": \"frequently asked questions\",\n",
    "    \"fav\": \"favorite\",\n",
    "    \"fave\": \"favorite\",\n",
    "    \"favs\": \"favorites\",\n",
    "    \"fb\": \"facebook\",\n",
    "    \"fbc\": \"facebook chat\",\n",
    "    \"fbf\": \"facebook friend\",\n",
    "    \"fml\": \"family\",\n",
    "    \"fn\": \"fine\",\n",
    "    \"fo\": \"freaking out\",\n",
    "    \"fri\": \"friday\",\n",
    "    \"frnd\": \"friend\",\n",
    "    \"fu\": \"fuck you\",\n",
    "    \"fugly\": \"fucking ugly\",\n",
    "    \"gf\": \"girlfriend\",\n",
    "    \"g1\": \"good one\",\n",
    "    \"g2b\": \"going to bed\",\n",
    "    \"g2cu\": \"good to see you\",\n",
    "    \"g2g\": \"good to go\",\n",
    "    \"g4i\": \"go for it\",\n",
    "    \"g4n\": \"good for nothing\",\n",
    "    \"g4u\": \"good for you\",\n",
    "    \"g9\": \"goodnight\",\n",
    "    \"ga\": \"go ahead\",\n",
    "    \"ge\": \"good evening\",\n",
    "    \"gl\": \"good luck\",\n",
    "    \"gm\": \"good morning\",\n",
    "    \"gn\": \"goodnight\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"goon\": \"idiot\",\n",
    "    \"gorge\": \"gorgeous\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"grats\": \"congratulations\",\n",
    "    \"gratz\": \"congratulations\",\n",
    "    \"grl\": \"girl\",\n",
    "    \"gt2t\": \"got time to talk\",\n",
    "    \"gtg\": \"good to go\",\n",
    "    \"gud\": \"good\",\n",
    "    \"gv\": \"give\",\n",
    "    \"gvn\": \"given\",\n",
    "    \"gw\": \"good work\",\n",
    "    \"h/o\": \"hold on\",\n",
    "    \"h/p\": \"hold please\",\n",
    "    \"h/t\": \"hat tip\",\n",
    "    \"h/u\": \"hook up\",\n",
    "    \"h2cus\": \"hope to see you soon\",\n",
    "    \"h4u\": \"hot for you\",\n",
    "    \"h4x0r\": \"hacker\",\n",
    "    \"h4x0rz\": \"hackers\",\n",
    "    \"h8\": \"hate\",\n",
    "    \"h8r\": \"hater\",\n",
    "    \"h8t\": \"hate\",\n",
    "    \"ha\": \"hello again\",\n",
    "    \"haha\": \"laughing\",\n",
    "    \"hai\": \"hi\",\n",
    "    \"hak\": \"hugs and kisses\",\n",
    "    \"han\": \"how about now?\",\n",
    "    \"hav\": \"have\",\n",
    "    \"hax\": \"hacks\",\n",
    "    \"haxor\": \"hacker\",\n",
    "    \"hay\": \"how are you\",\n",
    "    \"hb2u\": \"happy birthday to you\",\n",
    "    \"hbbd\": \"happy belated birthday\",\n",
    "    \"hbd\": \"happy birthday\",\n",
    "    \"hc\": \"how cool\",\n",
    "    \"hcit\": \"how cool is that\",\n",
    "    \"hehe\": \"laughing\",\n",
    "    \"hf\": \"have fun\",\n",
    "    \"hi5\": \"high five\",\n",
    "    \"hig\": \"how's it going?\",\n",
    "    \"hih\": \"hope it helps\",\n",
    "    \"ho\": \"hold on\",\n",
    "    \"hoc\": \"house of cards\",\n",
    "    \"hof\": \"hall of fame\",\n",
    "    \"holla\": \"holler\",\n",
    "    \"hom\": \"hit or miss\",\n",
    "    \"hood\": \"neighborhood\",\n",
    "    \"hoops\": \"basketball\",\n",
    "    \"hottie\": \"attractive person\",\n",
    "    \"hr\": \"human resources\",\n",
    "    \"hru\": \"how are you\",\n",
    "    \"hry\": \"hurry\",\n",
    "    \"hubby\": \"husband\",\n",
    "    \"hwk\": \"homework\",\n",
    "    \"hwp\": \"height weight proportionate\",\n",
    "    \"hwu\": \"hey, what's up?\",\n",
    "    \"hxc\": \"hardcore\",\n",
    "    \"h^\": \"hook up\",\n",
    "    \"i8\": \"i ate\",\n",
    "    \"i8u\": \"i hate you\",\n",
    "    \"ia\": \"i agree\",\n",
    "    \"iab\": \"in a bit\",\n",
    "    \"iac\": \"in any case\",\n",
    "    \"iad\": \"it all depends\",\n",
    "    \"iae\": \"in any event\",\n",
    "    \"iag\": \"it's all good\",\n",
    "    \"iagw\": \"in a good way\",\n",
    "    \"iail\": \"i am in love\",\n",
    "    \"iam\": \"in a minute\",\n",
    "    \"ic\": \"i see\",\n",
    "    \"id10t\": \"idiot\",\n",
    "    \"idc\": \"i don't care\",\n",
    "    \"idd\": \"indeed\",\n",
    "    \"idi\": \"i doubt it\",\n",
    "    \"idk\": \"i don't know\",\n",
    "    \"idky\": \"i don't know why\",\n",
    "    \"idmb\": \"i'll do my best\",\n",
    "    \"idn\": \"i don't know\",\n",
    "    \"idnk\": \"i do not know\",\n",
    "    \"idr\": \"i don't remember\",\n",
    "    \"idt\": \"i don't think\",\n",
    "    \"idts\": \"i don't think so\",\n",
    "    \"idtt\": \"i'll drink to that\",\n",
    "    \"idu\": \"i don't understand\",\n",
    "    \"ie\": \"that is\",\n",
    "    \"ig2p\": \"i got to pee\",\n",
    "    \"iggy\": \"ignored\",\n",
    "    \"ight\": \"alright\",\n",
    "    \"igi\": \"i get it\",\n",
    "    \"ign\": \"in-game name\",\n",
    "    \"igtp\": \"i get the point\",\n",
    "    \"ih8u\": \"i hate you\",\n",
    "    \"ihu\": \"i hate you\",\n",
    "    \"ihy\": \"i hate you\",\n",
    "    \"ii\": \"i'm impressed\",\n",
    "    \"iiok\": \"if i only knew\",\n",
    "    \"iir\": \"if i remember\",\n",
    "    \"iirc\": \"if i remember correctly\",\n",
    "    \"iit\": \"i'm impressed too\",\n",
    "    \"iiuc\": \"if i understand correctly\",\n",
    "    \"ik\": \"i know\",\n",
    "    \"ikhyf\": \"i know how you feel\",\n",
    "    \"ikr\": \"i know, right?\",\n",
    "    \"ikwum\": \"i know what you mean\",\n",
    "    \"ikwym\": \"i know what you mean\",\n",
    "    \"ikyd\": \"i know you did\",\n",
    "    \"ilu\": \"i like you\",\n",
    "    \"ilu2\": \"i love you too\",\n",
    "    \"ilub\": \"i love you baby\",\n",
    "    \"ilyk\": \"i'll let you know\",\n",
    "    \"ilyl\": \"i love you lots\",\n",
    "    \"ilysm\": \"i love you so much\",\n",
    "    \"ima\": \"i'm\",\n",
    "    \"imma\": \"i'm gonna\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imy\": \"i miss you\",\n",
    "    \"inb4\": \"in before\",\n",
    "    \"inc\": \"incoming\",\n",
    "    \"indie\": \"independent\",\n",
    "    \"info\": \"information\",\n",
    "    \"init\": \"initialize\",\n",
    "    \"ipo\": \"initial public offering\",\n",
    "    \"ir\": \"in room\",\n",
    "    \"ir8\": \"irate\",\n",
    "    \"irdk\": \"i really don't know\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"iyo\": \"in your opinion\",\n",
    "    \"iyq\": \"i like you\",\n",
    "    \"j/k\": \"just kidding\",\n",
    "    \"j/p\": \"just playing\",\n",
    "    \"j/w\": \"just wondering\",\n",
    "    \"j2lyk\": \"just to let you know\",\n",
    "    \"j4f\": \"just for fun\",\n",
    "    \"j4g\": \"just for grins\",\n",
    "    \"jas\": \"just a second\",\n",
    "    \"jb/c\": \"just because\",\n",
    "    \"joshing\": \"joking\",\n",
    "    \"k\": \"ok\",\n",
    "    \"k3u\": \"i love you\",\n",
    "    \"kappa\": \"sarcasm\",\n",
    "    \"kek\": \"korean laugh\",\n",
    "    \"keke\": \"korean laugh\",\n",
    "    \"kewl\": \"cool\",\n",
    "    \"kewt\": \"cute\",\n",
    "    \"kfc\": \"kentucky fried chicken\",\n",
    "    \"kgo\": \"ok, go\",\n",
    "    \"kik\": \"laughing out loud\",\n",
    "    \"kinda\": \"kind of\",\n",
    "    \"kk\": \"ok\",\n",
    "    \"kl\": \"kool\",\n",
    "    \"km\": \"kiss me\",\n",
    "    \"kma\": \"kiss my ass\",\n",
    "    \"knp\": \"ok, no problem\",\n",
    "    \"kw\": \"know\",\n",
    "    \"kwl\": \"cool\",\n",
    "    \"l2m\": \"listening to music\",\n",
    "    \"l2p\": \"learn to play\",\n",
    "    \"l33t\": \"leet\",\n",
    "    \"l8\": \"late\",\n",
    "    \"l8er\": \"later\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"la\": \"laughing a lot\",\n",
    "    \"laf\": \"laugh\",\n",
    "    \"laffing\": \"laughing\",\n",
    "    \"lafs\": \"love at first sight\",\n",
    "    \"lam\": \"leave a message\",\n",
    "    \"lamer\": \"lame person\",\n",
    "    \"legit\": \"legitimate\",\n",
    "    \"lemeno\": \"let me know\",\n",
    "    \"lil\": \"little\",\n",
    "    \"lk\": \"like\",\n",
    "    \"llol\": \"literally laughing out loud\",\n",
    "    \"lmho\": \"laughing my head off\",\n",
    "    \"loi\": \"laughing on the inside\",\n",
    "    \"lola\": \"love often, laugh a lot\",\n",
    "    \"lolol\": \"lots of laugh out louds\",\n",
    "    \"lolz\": \"laugh out louds\",\n",
    "    \"ltr\": \"later\",\n",
    "    \"lulz\": \"lol\",\n",
    "    \"luv\": \"love\",\n",
    "    \"luzr\": \"loser\",\n",
    "    \"lv\": \"love\",\n",
    "    \"ly\": \"love ya\",\n",
    "    \"lya\": \"love you always\",\n",
    "    \"lyk\": \"let you know\",\n",
    "    \"lyn\": \"lying\",\n",
    "    \"lysm\": \"love you so much\",\n",
    "    \"m\": \"male\",\n",
    "    \"mcd\": \"mcdonald's\",\n",
    "    \"mcds\": \"mcdonald's\",\n",
    "    \"md@u\": \"mad at you\",\n",
    "    \"me2\": \"me too\",\n",
    "    \"meh\": \"whatever\",\n",
    "    \"mf\": \"mother fucker\",\n",
    "    \"mfb\": \"mother fucking bitch\",\n",
    "    \"mgmt\": \"management\",\n",
    "    \"mid\": \"middle\",\n",
    "    \"mil\": \"mother-in-law\",\n",
    "    \"min\": \"minute\",\n",
    "    \"mins\": \"minutes\",\n",
    "    \"mk\": \"okay\",\n",
    "    \"mkay\": \"ok\",\n",
    "    \"mmk\": \"ok\",\n",
    "    \"mms\": \"multimedia messaging service\",\n",
    "    \"mng\": \"manage\",\n",
    "    \"mngr\": \"manager\",\n",
    "    \"mod\": \"modification\",\n",
    "    \"mofo\": \"mother fucking\",\n",
    "    \"mojo\": \"attractive talent\",\n",
    "    \"moss\": \"chill\",\n",
    "    \"ms\": \"miss\",\n",
    "    \"msg\": \"message\",\n",
    "    \"mtg\": \"meeting\",\n",
    "    \"mth\": \"month\",\n",
    "    \"mu\": \"miss you\",\n",
    "    \"mu@\": \"meet you at\",\n",
    "    \"muah\": \"kiss\",\n",
    "    \"mula\": \"money\",\n",
    "    \"mwa\": \"kiss\",\n",
    "    \"mwah\": \"kiss\",\n",
    "    \"n/m\": \"nevermind\",\n",
    "    \"n/m/h\": \"nothing much here\",\n",
    "    \"n/r\": \"no reserve\",\n",
    "    \"n00b\": \"newbie\",\n",
    "    \"n1\": \"nice one\",\n",
    "    \"n1c\": \"no one cares\",\n",
    "    \"n2m\": \"not too much\",\n",
    "    \"n2mh\": \"not too much here\",\n",
    "    \"n2w\": \"not to worry\",\n",
    "    \"n64\": \"nintendo 64\",\n",
    "    \"n8kd\": \"naked\",\n",
    "    \"nac\": \"not a chance\",\n",
    "    \"nah\": \"no\",\n",
    "    \"nal\": \"nationality\",\n",
    "    \"narc\": \"tattle tale\",\n",
    "    \"nark\": \"informant\",\n",
    "    \"naw\": \"no\",\n",
    "    \"nb\": \"not bad\",\n",
    "    \"nbd\": \"no big deal\",\n",
    "    \"nbjf\": \"no brag, just fact\",\n",
    "    \"nd\": \"and\",\n",
    "    \"ne\": \"any\",\n",
    "    \"ne1\": \"anyone\",\n",
    "    \"ne1er\": \"anyone here\",\n",
    "    \"neh\": \"no\",\n",
    "    \"nemore\": \"anymore\",\n",
    "    \"neva\": \"never\",\n",
    "    \"neway\": \"anyway\",\n",
    "    \"newaze\": \"anyways\",\n",
    "    \"newb\": \"newbie\",\n",
    "    \"nite\": \"night\",\n",
    "    \"nn2r\": \"no need to reply\",\n",
    "    \"nnito\": \"not necessarily in that order\",\n",
    "    \"nnto\": \"no need to open\",\n",
    "    \"nntr\": \"no need to reply\",\n",
    "    \"no1\": \"no one\",\n",
    "    \"noob\": \"newbie\",\n",
    "    \"nooblet\": \"young newbie\",\n",
    "    \"nooblord\": \"ultimate newbie\",\n",
    "    \"notch\": \"minecraft creator\",\n",
    "    \"nottie\": \"unattractive person\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"nub\": \"newbie\",\n",
    "    \"nuff\": \"enough\",\n",
    "    \"nufn\": \"nothing\",\n",
    "    \"num\": \"tasty\",\n",
    "    \"nvm\": \"nevermind\",\n",
    "    \"nvr\": \"never\",\n",
    "    \"nvrm\": \"nevermind\",\n",
    "    \"nw\": \"no way\",\n",
    "    \"nxt\": \"next\",\n",
    "    \"o4u\": \"only for you\",\n",
    "    \"obtw\": \"oh, by the way\",\n",
    "    \"obv\": \"obviously\",\n",
    "    \"obvi\": \"obviously\",\n",
    "    \"oc\": \"of course\",\n",
    "    \"ohemgee\": \"oh my gosh\",\n",
    "    \"oic\": \"oh, i see\",\n",
    "    \"oicn\": \"oh, i see now\",\n",
    "    \"oiy\": \"hi\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"onl\": \"online\",\n",
    "    \"onoz\": \"oh no\",\n",
    "    \"orly\": \"oh really\",\n",
    "    \"otay\": \"okay\",\n",
    "    \"otw\": \"on the way\",\n",
    "    \"outta\": \"out of\",\n",
    "    \"ovie\": \"overlord\",\n",
    "    \"ownage\": \"completely owned\",\n",
    "    \"p/d\": \"per day\",\n",
    "    \"p/m\": \"per month\",\n",
    "    \"p/y\": \"per year\",\n",
    "    \"p911\": \"parent alert!\",\n",
    "    \"p@h\": \"parents at home\",\n",
    "    \"pc\": \"personal computer\",\n",
    "    \"pda\": \"public display of affection\",\n",
    "    \"pic\": \"picture\",\n",
    "    \"pj\": \"poor joke\",\n",
    "    \"pl8\": \"plate\",\n",
    "    \"pld\": \"played\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"plzrd\": \"please read\",\n",
    "    \"pov\": \"point of view\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"ppp\": \"peace\",\n",
    "    \"prof\": \"professor\",\n",
    "    \"prolly\": \"probably\",\n",
    "    \"promo\": \"promotion\",\n",
    "    \"props\": \"recognition\",\n",
    "    \"prot\": \"protection\",\n",
    "    \"prvt\": \"private\",\n",
    "    \"ps\": \"postscript\",\n",
    "    \"ps2\": \"playstation 2\",\n",
    "    \"ps3\": \"playstation 3\",\n",
    "    \"psa\": \"public service announcement\",\n",
    "    \"psog\": \"pure stroke of genius\",\n",
    "    \"psp\": \"playstation portable\",\n",
    "    \"ptm\": \"please tell me\",\n",
    "    \"pwd\": \"password\",\n",
    "    \"psd\": \"password\",\n",
    "    \"pswd\": \"password\",\n",
    "    \"pwnd\": \"owned\",\n",
    "    \"pwned\": \"owned\",\n",
    "    \"pwnt\": \"owned\",\n",
    "    \"q4u\": \"question for you\",\n",
    "    \"qfe\": \"quoted for emphasis\",\n",
    "    \"qft\": \"quoted for truth\",\n",
    "    \"qq\": \"quick question\",\n",
    "    \"qqn\": \"looking\",\n",
    "    \"qrg\": \"quick reference guide\",\n",
    "    \"qt\": \"cutie\",\n",
    "    \"qtpi\": \"cutie pie\",\n",
    "    \"r\": \"are\",\n",
    "    \"r8\": \"rate\",\n",
    "    \"rdy\": \"ready\",\n",
    "    \"re\": \"replay\",\n",
    "    \"rehi\": \"hi again\",\n",
    "    \"rents\": \"parents\",\n",
    "    \"rep\": \"reputation\",\n",
    "    \"resq\": \"rescue\",\n",
    "    \"rgd\": \"regard\",\n",
    "    \"rgds\": \"regards\",\n",
    "    \"ridic\": \"ridiculous\",\n",
    "    \"rip\": \"rest in peace\",\n",
    "    \"rl\": \"real life\",\n",
    "    \"rlrt\": \"real life retweet\",\n",
    "    \"rly\": \"really\",\n",
    "    \"rm\": \"room\",\n",
    "    \"rn\": \"run\",\n",
    "    \"rnt\": \"aren't\",\n",
    "    \"rof\": \"laughing\",\n",
    "    \"rofl\": \"laughing\",\n",
    "    \"roflmao\": \"laughing\",\n",
    "    \"roflol\": \"laughing out loud\",\n",
    "    \"rolf\": \"laughing\",\n",
    "    \"ru\": \"are you\",\n",
    "    \"ruc\": \"are you coming?\",\n",
    "    \"rut\": \"are you there?\",\n",
    "    \"rx\": \"prescription\",\n",
    "    \"s/o\": \"sold out\",\n",
    "    \"s/u\": \"shut up\",\n",
    "    \"s/w\": \"software\",\n",
    "    \"s2r\": \"send to receive\",\n",
    "    \"s2s\": \"sorry to say\",\n",
    "    \"s2u\": \"same to you\",\n",
    "    \"samzd\": \"still amazed\",\n",
    "    \"sd\": \"sweet dreams\",\n",
    "    \"sec\": \"second\",\n",
    "    \"sho\": \"sure\",\n",
    "    \"sh^\": \"shut up\",\n",
    "    \"siul8r\": \"see you later\",\n",
    "    \"siv\": \"bad goaltender\",\n",
    "    \"sk8\": \"skate\",\n",
    "    \"sk8r\": \"skater\",\n",
    "    \"sly\": \"still love you\",\n",
    "    \"smf\": \"so much fun\",\n",
    "    \"smooch\": \"kiss\",\n",
    "    \"sorta\": \"sort of\",\n",
    "    \"spec\": \"specialization\",\n",
    "    \"spk\": \"speak\",\n",
    "    \"spkr\": \"speaker\",\n",
    "    \"srry\": \"sorry\",\n",
    "    \"srs\": \"serious\",\n",
    "    \"srsly\": \"seriously\",\n",
    "    \"sry\": \"sorry\",\n",
    "    \"stpd\": \"stupid\",\n",
    "    \"str\": \"strength\",\n",
    "    \"str8\": \"straight\",\n",
    "    \"sup\": \"what's up\",\n",
    "    \"syl\": \"see you later\",\n",
    "    \"sync\": \"synchronize\",\n",
    "    \"t2go\": \"time to go\",\n",
    "    \"t2m\": \"talk to me\",\n",
    "    \"t2u\": \"talk to you\",\n",
    "    \"t2ul\": \"talk to you later\",\n",
    "    \"t2ul8er\": \"talk to you later\",\n",
    "    \"t2ul8r\": \"talk to you later\",\n",
    "    \"t4lmk\": \"thanks for letting me know\",\n",
    "    \"t4p\": \"thanks for posting\",\n",
    "    \"t4t\": \"thanks for trade\",\n",
    "    \"tc\": \"take care\",\n",
    "    \"teh\": \"the\",\n",
    "    \"teme\": \"tell me\",\n",
    "    \"tg\": \"thank goodness\",\n",
    "    \"thnq\": \"thank you\",\n",
    "    \"tho\": \"though\",\n",
    "    \"thru\": \"through\",\n",
    "    \"tht\": \"that\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"tl\": \"tell\",\n",
    "    \"tlk\": \"talk\",\n",
    "    \"tlkin\": \"talking\",\n",
    "    \"tlking\": \"talking\",\n",
    "    \"tomoz\": \"tomorrow\",\n",
    "    \"tq\": \"thank you\",\n",
    "    \"tqvm\": \"thank you very much\",\n",
    "    \"tru\": \"true\",\n",
    "    \"ttl\": \"talk to you later\",\n",
    "    \"ttly\": \"totally\",\n",
    "    \"ttul\": \"talk to you later\",\n",
    "    \"tty\": \"talk to you\",\n",
    "    \"tu\": \"thank you\",\n",
    "    \"tude\": \"attitude\",\n",
    "    \"tx\": \"thanks\",\n",
    "    \"txt\": \"text\",\n",
    "    \"txtin\": \"texting\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"tyfa\": \"thank you for asking\",\n",
    "    \"tyl\": \"thank you lord\",\n",
    "    \"tym\": \"thank you much\",\n",
    "    \"tyt\": \"take your time\",\n",
    "    \"tyvm\": \"thank you very much\",\n",
    "    \"u\": \"you\",\n",
    "    \"u-ok\": \"you ok?\",\n",
    "    \"u/l\": \"upload\",\n",
    "    \"u2\": \"you too\",\n",
    "    \"u2u\": \"up to you\",\n",
    "    \"uok\": \"you ok?\",\n",
    "    \"ur\": \"your\",\n",
    "    \"ut\": \"you there?\",\n",
    "    \"veggies\": \"vegetables\",\n",
    "    \"vry\": \"very\",\n",
    "    \"vs\": \"versus\",\n",
    "    \"w/\": \"with\",\n",
    "    \"w/b\": \"welcome back\",\n",
    "    \"w/e\": \"whatever\",\n",
    "    \"w/o\": \"without\",\n",
    "    \"w2f\": \"way too funny\",\n",
    "    \"w2g\": \"way to go\",\n",
    "    \"w2k\": \"windows 2000\",\n",
    "    \"w4u\": \"wait for you\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"w84m\": \"wait for me\",\n",
    "    \"w8am\": \"wait a minute\",\n",
    "    \"w8ing\": \"waiting\",\n",
    "    \"w8n\": \"waiting\",\n",
    "    \"wa\": \"what\",\n",
    "    \"waa\": \"crying\",\n",
    "    \"wack\": \"strange\",\n",
    "    \"wan2\": \"want to\",\n",
    "    \"wannabe\": \"want to be\",\n",
    "    \"wat\": \"what\",\n",
    "    \"watev\": \"whatever\",\n",
    "    \"watevs\": \"whatever\",\n",
    "    \"wlcm\": \"welcome\",\n",
    "    \"wha\": \"what\",\n",
    "    \"whipped\": \"tired\",\n",
    "    \"wht\": \"what\",\n",
    "    \"wk\": \"week\",\n",
    "    \"wknd\": \"weekend\",\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"wtg\": \"way to go\",\n",
    "    \"wup\": \"what's up?\",\n",
    "    \"ya\": \"yes\",\n",
    "    \"yeap\": \"yes\",\n",
    "    \"yep\": \"yes\",\n",
    "    \"yepperz\": \"yes\",\n",
    "    \"yesh\": \"yes\",\n",
    "    \"yo\": \"hi\",\n",
    "    \"yr\": \"your\",\n",
    "    \"yrs\": \"years\",\n",
    "    \"yt\": \"you there?\",\n",
    "    \"yt?\": \"you there?\",\n",
    "    \"yup\": \"yes\",\n",
    "    \"yupz\": \"ok\",\n",
    "    \"zzz\": \"sleeping\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import inflect\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import parsing\n",
    "from gensim.parsing.preprocessing import split_alphanum\n",
    "from spellchecker import SpellChecker\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert apostrophes word to original form\n",
    "def replace_word(word):\n",
    "    word = word.lower()\n",
    "    word = word.split()\n",
    "    for i in range(len(word)):\n",
    "        word[i] = abbreviation_dict.get(word[i], word[i])\n",
    "    word = \" \".join(word)\n",
    "    return word\n",
    "\n",
    "##Fixing Word Lengthening\n",
    "##https://rustyonrampa\"ge.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def replace_numbers(word):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    word = word.split()\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isdigit():\n",
    "            word[i] = p.number_to_words(word[i])\n",
    "    word = \" \".join(word)\n",
    "    return word\n",
    "def transformText(text):\n",
    "    text = split_alphanum(text)\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    text = replace_word(text)\n",
    "    text = replace_numbers(text)\n",
    "    text = reduce_lengthening(text)\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    # Removing all the stopwords\n",
    "    filtered_words = [word for word in text.split() if word not in STOP_WORDS]\n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    # remove html markup\n",
    "    text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "    # Correct words\n",
    "    spell = SpellChecker()\n",
    "    misspelled = text.split()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(misspelled)):\n",
    "        # Get the one `most likely` answer\n",
    "        word = spell.correction(misspelled[i])\n",
    "        misspelled[i]=word\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"v\")\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"n\")\n",
    "    text = \" \".join(misspelled)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    # Strip all the numerics\n",
    "    #text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have have twenty soo do not be service go group'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformText(\" she'd've I have so 20 soooooo don't i'm  can't servic going grooooooooop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].map(lambda x: transformText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts= df['text']\n",
    "tags= df['label']\n",
    "# dictionary of lists  \n",
    "dict = {'text': texts , 'label': tags } \n",
    "     \n",
    "df = pd.DataFrame(dict) \n",
    "  \n",
    "# saving the dataframe \n",
    "df.to_csv('Data/DATA_preprocessing_brute.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>have order data cable get well finish work pro...</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>love phone</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>get well finish product</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>not be happier</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be look headset long time have get</td>\n",
       "      <td>NOTISSUE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0  have order data cable get well finish work pro...  NOTISSUE\n",
       "1                                         love phone  NOTISSUE\n",
       "2                            get well finish product  NOTISSUE\n",
       "3                                     not be happier  NOTISSUE\n",
       "4                 be look headset long time have get  NOTISSUE"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"def emoticons_look_up(text):\n",
    "    \"\"\"\n",
    "    \"\"\"Remove emoticons from text and returns list of emotions present in text\n",
    "    #Example: Sure, you are welcome :) => Sure, you are welcome.\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed emoticons sign\n",
    "        emolist (list) : list of emotions from text\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"words = text.split()\n",
    "    emolist = []\n",
    "    for word in words:\n",
    "        if word in emo:\n",
    "            emolist.append(str(emo[word]))\n",
    "            text = text.replace(word,\" \")\n",
    "    return text, emolist\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "sb_stem = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "pt_stem = PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "##Convert apostrophes word to original form\n",
    "def replace_numbers(word):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    word = word.split()\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isdigit():\n",
    "            word[i] = p.number_to_words(word[i])\n",
    "    word = \" \".join(word)\n",
    "    return word\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"  Fixing Word Lengthening\n",
    "##https://rustyonrampa\"ge.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html\"\"\"\n",
    "def reduce_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "def abbreviation_look_up(text):\n",
    "    \"\"\"\n",
    "    Replace abbreviation word in text to their original form\n",
    "    Example: hi, thanq so mch => hi, thank you so much\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        slanged (str): cleaned text with replaced slang\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in abbreviation_dict:\n",
    "            new_text.append(abbreviation_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    slanged = \" \".join(new_text)\n",
    "    return slanged\n",
    "\n",
    "def appos_look_up(text):\n",
    "    \"\"\"\n",
    "    Convert apostrophes word to original form\n",
    "    Example: I don't know what is going on?  => I do not know what is going on? \n",
    "    Args:\n",
    "        text (str): text \n",
    "    Returns:\n",
    "        apposed (str) : text with converted apostrophes\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "    for word in words:\n",
    "        word_s = word.lower()\n",
    "        if word_s in appos_dict:\n",
    "            new_text.append(appos_dict[word_s])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    apposed = \" \".join(new_text)\n",
    "    return apposed\n",
    "\n",
    "\n",
    "def correct_word(text):\n",
    "    # Correct words\n",
    "    text=\"servic groop\"\n",
    "    spell = SpellChecker()\n",
    "    misspelled = text.split()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for i in range(len(misspelled)):\n",
    "        # Get the one `most likely` answer\n",
    "        word = spell.correction(misspelled[i])\n",
    "        misspelled[i]=word\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"v\")\n",
    "        misspelled[i] = wordnet_lemmatizer.lemmatize(misspelled[i], pos=\"n\")\n",
    "        text = \" \".join(misspelled)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeated_characters(text):\n",
    "    \"\"\"\n",
    "    Remove repeated characters (>2) in words to max limit of 2\n",
    "    Example: I am verrry happpyyy today => I am verry happyy today\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed repeated chars\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'(.)\\1+')\n",
    "    clean_text = regex_pattern.sub(r'\\1\\1', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def separate_digit_text(text):\n",
    "    \"\"\"\n",
    "    Separate digit and words with space in text\n",
    "    Example: I will be booking tickets for 2adults => I will be booking tickets for 2 adults   \n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with separated digits and words\n",
    "    \"\"\"\n",
    "    regex_patter = re.compile(r'([\\d]+)([a-zA-Z]+)')\n",
    "    clean_text = regex_patter.sub(r'\\1 \\2', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stem_text(text, stemmer='snowball'):\n",
    "    \"\"\"\n",
    "    Convert words in text into their root form\n",
    "    Example: I am playing in ground => I am play in ground \n",
    "    Args:\n",
    "        text (str): text\n",
    "        \n",
    "    Returns:\n",
    "        text_stem (str): cleaned text with replaced stem words\n",
    "    \"\"\"\n",
    "    #text = remove_inside_braces(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    if stemmer == 'snowball':\n",
    "        text_stem = \" \".join([sb_stem.stem(w) for w in tokens])\n",
    "    else:\n",
    "        text_stem = \" \".join([pt_stem.stem(w) for w in tokens])\n",
    "    \n",
    "    return text_stem\n",
    "\n",
    "\n",
    "def remove_single_char_word(text):\n",
    "    \"\"\"\n",
    "    Remove single character word from text\n",
    "    Example: I am in a home for 2 years => am in home for years \n",
    "    Args:\n",
    "        text (str): text\n",
    "         \n",
    "    Returns:\n",
    "        (str): text with single char removed\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filter_words = [word for word in words if len(word) > 1]\n",
    "    return \" \".join(filter_words)\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removed special characters from text\n",
    "    Example: he: I am going. are you coming? => he I am going. are you coming\n",
    "   \n",
    "    Args:\n",
    "        text (str): text\n",
    "   \n",
    "    Returns:\n",
    "        clean_text (str): cleaned text with removed special characters\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[\\,+\\:\\?\\!\\\"\\(\\)!\\'\\.\\%\\[\\]]+')\n",
    "    clean_text = regex_pattern.sub(r' ', text)\n",
    "    clean_text = clean_text.replace('-', '')\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_extra_space(text):\n",
    "    \"\"\"\n",
    "    Remove extra white spaces space from text\n",
    "    Example: hey are   you coming. ? => he are you coming. ?\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        clean_text (str): clean text with removed extra white spaces\n",
    "    \"\"\"\n",
    "    #text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    clean_text = ' '.join(text.strip().split())\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def replace_digits_with_char(text, replace_char='d'):\n",
    "    \"\"\"\n",
    "    Replace digits to `replace_char`\n",
    "    Example: I will be there on 22 april. => I will be there on dd april.\n",
    "    Args:\n",
    "        text (str): text\n",
    "        replace_char (str): character with which digit has to be replaced\n",
    "    Returns:\n",
    "        clean_text (str): clean text with replaced char for digits\n",
    "    \"\"\"\n",
    "    regex_pattern = re.compile(r'[0-9]')\n",
    "    clean_text = regex_pattern.sub(replace_char, text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def remove_url(text):\n",
    "    \"\"\"\n",
    "    Remove urls from text\n",
    "    Example: link to latest cricket score. https://xyz.com/a/b => link to latest cricket score.\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed urls\n",
    "    \"\"\"\n",
    "\n",
    "    urlfree = []\n",
    "    for word in text.split():\n",
    "        if not word.startswith(\"www\"):\n",
    "            urlfree.append(word)\n",
    "        elif not word.startswith(\"http\"):\n",
    "            urlfree.append(word)\n",
    "        elif not word.endswith(\".html\"):\n",
    "            urlfree.append(word)\n",
    "    urlfree = \" \".join(urlfree)\n",
    "\n",
    "    urls = re.finditer(r'http[\\w]*:\\/\\/[\\w]*\\.?[\\w-]+\\.+[\\w]+[\\/\\w]+', urlfree)\n",
    "    for i in urls:\n",
    "        urlfree = re.sub(i.group().strip(), '', urlfree)\n",
    "    return urlfree\n",
    "\n",
    "\n",
    "def remove_alphanumerics(text):\n",
    "    \"\"\"\n",
    "    Remove alphanumeric words from text\n",
    "    Example: hello man whatsup123 => hello man\n",
    "    Args:\n",
    "        text (str): text\n",
    "    Returns:\n",
    "        text (str): text with removed alphanumeric words\n",
    "    \"\"\"\n",
    "    txt = []\n",
    "    for each in text.split():\n",
    "        if not any(x in each.lower() for x in \"0123456789\"):\n",
    "            txt.append(each)\n",
    "    txtsent = \" \".join(txt)\n",
    "    return txtsent \n",
    "\n",
    "\n",
    "def remove_words_start_with(text, starts_with_char):\n",
    "    \"\"\"\n",
    "    Remove words start with character `starts_with_char`\n",
    "    Example: dhoni rocks with last ball six #dhoni #six => dhoni rocks with last ball six (start_char_with='#')\n",
    "    Args:\n",
    "        text (str): text\n",
    "        starts_with_char (str): starting characters of word, which to be removed from text\n",
    "    Returns:\n",
    "        text (str): text with removed words start with given chars\n",
    "    \"\"\"\n",
    "    urls = re.finditer(starts_with_char + r'[A-Za-z0-9\\w]*', text)\n",
    "    for i in urls:\n",
    "        text = re.sub(i.group().strip(), '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_stop_words(text, stop_words=stop_words_list):\n",
    "    \"\"\"\n",
    "    This function removes stop words from text\n",
    "    Example: I am very excited for today's football match => very excited today's football match\n",
    "    Params\n",
    "        text (str) :text on which processing needs to done\n",
    "        stop_words (list) : stop words which needs to be removed\n",
    "    Returns\n",
    "        text(str): text after stop words removal\n",
    "    \"\"\"\n",
    "    stop_words = set(stop_words)\n",
    "    split_list = text.split(\" \")\n",
    "    split_list = [word for word in split_list if word not in stop_words]\n",
    "    return \" \".join(split_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformtext(text):\n",
    "    text = split_alphanum(text)\n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    \n",
    "    ##Replace all interger occurrences in list of tokenized words with textual representation\n",
    "    text = replace_numbers(text)\n",
    "    \n",
    "    #text = reduce_lengthening(text)\n",
    "\n",
    "    text = remove_repeated_characters(text)\n",
    "    \n",
    "    ## Replace digits to `replace_char for date :\n",
    "    #text = separate_digit_text(text)\n",
    "\n",
    "    ## Replace slang word in text to their original form\n",
    "    text = abbreviation_look_up(text)\n",
    "    \n",
    "    ##Convert apostrophes word to original form \n",
    "    text = appos_look_up(text)\n",
    "   \n",
    "    ## # Correct words\n",
    "    text = correct_word(text)\n",
    "   \n",
    "    ## Convert words in text into their root form\n",
    "    text = stem_text(text, stemmer='snowball')\n",
    "    \n",
    "    #remove_single_char_word\n",
    "    text = remove_single_char_word(text)\n",
    "    \n",
    "    ## Removed special characters from text\n",
    "    text = remove_punctuations(text)\n",
    "    \n",
    "    ## Strip multiple whitespaces\n",
    "    text = remove_extra_space(text)\n",
    "    \n",
    "    text = replace_digits_with_char(text, replace_char='d')\n",
    "    \n",
    "    ## Remove urls from text\n",
    "    text = remove_url(text)\n",
    "    \n",
    "    ##Remove alphanumeric words from text\n",
    "    text = remove_alphanumerics(text)\n",
    "\n",
    "    #text = remove_words_start_with(text, starts_with_char)\n",
    "    ##removes stop words from text\n",
    "    text = remove_stop_words(text, stop_words=stop_words_list)\n",
    "    \n",
    "    ##Separate digit and words with space in text\n",
    "    text = separate_digit_text(text)\n",
    "    \n",
    "    # remove html markup\n",
    "    text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'servic group'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformtext(\"goining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m_check_if_should_check\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# check if it is a number (int, float, etc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: \"ahandsf'ee\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-ed85a31d73b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtransformText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3380\u001b[0m         \"\"\"\n\u001b[0;32m   3381\u001b[0m         new_values = super(Series, self)._map_values(\n\u001b[1;32m-> 3382\u001b[1;33m             arg, na_action=na_action)\n\u001b[0m\u001b[0;32m   3383\u001b[0m         return self._constructor(new_values,\n\u001b[0;32m   3384\u001b[0m                                  index=self.index).__finalize__(self)\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1218\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-ed85a31d73b9>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtransformText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-18764e8af78f>\u001b[0m in \u001b[0;36mtransformText\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisspelled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Get the one `most likely` answer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorrection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisspelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mmisspelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mmisspelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet_lemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisspelled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"v\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mcorrection\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mReturns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                 str: The most likely candidate \"\"\"\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_probability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mcandidates\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# if still not found, use the edit distance 1 to calc edit distance 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distance\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m             \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__edit_distance_alt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36mknown\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m    159\u001b[0m         return set(\n\u001b[0;32m    160\u001b[0m             \u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_frequency\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_frequency\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_if_should_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         )\n\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\ana\\lib\\site-packages\\spellchecker\\spellchecker.py\u001b[0m in \u001b[0;36m_check_if_should_check\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['text'] = df['text'].map(lambda x: transformText(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
